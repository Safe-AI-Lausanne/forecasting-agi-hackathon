benchmark_name,benchmark_paper,title,authors,publication,year,url,pdf_url,abstract,cited_by_count,paper_type
TruthfulQA,TruthfulQA: Measuring How Models Mimic Human Falsehoods,A survey on evaluation of large language models,"Y Chang,X Wang,J Wang,Y Wu,L Yang‚Ä¶¬†- ACM transactions on¬†‚Ä¶, 2024",dl.acm.org,,https://dl.acm.org/doi/abs/10.1145/3641289,,"Large language models (LLMs) are gaining increasing popularity in both academia andindustry, owing to their unprecedented performance in various applications. As LLMs¬†‚Ä¶",4405.0,Survey
TruthfulQA,TruthfulQA: Measuring How Models Mimic Human Falsehoods,"A survey on large language model (llm) security and privacy: The good, the bad, and the ugly","Y Yao,J Duan,K Xu, Y Cai,Z Sun,Y Zhang- High-Confidence Computing, 2024",Elsevier,,https://www.sciencedirect.com/science/article/pii/S266729522400014X,,"Abstract Large Language Models (LLMs), such as ChatGPT and Bard, have revolutionizednatural language understanding and generation. They possess deep language¬†‚Ä¶",1335.0,Survey
TruthfulQA,TruthfulQA: Measuring How Models Mimic Human Falsehoods,"A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions","L Huang,W Yu,W Ma,W Zhong,Z Feng‚Ä¶¬†- ACM Transactions on¬†‚Ä¶, 2025",dl.acm.org,,https://dl.acm.org/doi/abs/10.1145/3703155,,"The emergence of large language models (LLMs) has marked a significant breakthrough innatural language processing (NLP), fueling a paradigm shift in information acquisition¬†‚Ä¶",3037.0,Survey
TruthfulQA,TruthfulQA: Measuring How Models Mimic Human Falsehoods,üßú Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models,"Y Zhang,Y Li,L Cui,D Cai,L Liu,T Fu‚Ä¶¬†- Computational¬†‚Ä¶, 2025",direct.mit.edu,,https://direct.mit.edu/coli/article/doi/10.1162/coli.a.16/131631,,"While large language models (LLMs) have demonstrated remarkable capabilities across arange of downstream tasks, a significant concern revolves around their propensity to exhibit¬†‚Ä¶",1549.0,Survey
TruthfulQA,TruthfulQA: Measuring How Models Mimic Human Falsehoods,A comprehensive overview of large language models,"H Naveed,AU Khan,S Qiu,M Saqib,S Anwar‚Ä¶¬†- ACM Transactions on¬†‚Ä¶, 2025",dl.acm.org,,https://dl.acm.org/doi/abs/10.1145/3744746,,Large Language Models (LLMs) have recently demonstrated remarkable capabilities innatural language processing tasks and beyond. This success of LLMs has led to a large¬†‚Ä¶,1815.0,Survey
TruthfulQA,TruthfulQA: Measuring How Models Mimic Human Falsehoods,Large language models: A survey,"S Minaee,T Mikolov,N Nikzad,M Chenaghlu‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2024",arxiv.org,,https://arxiv.org/abs/2402.06196,,"Large Language Models (LLMs) have drawn a lot of attention due to their strongperformance on a wide range of natural language tasks, since the release of ChatGPT in¬†‚Ä¶",1504.0,Survey
TruthfulQA,TruthfulQA: Measuring How Models Mimic Human Falsehoods,A survey of large language models,"WX Zhao,K Zhou,J Li,T Tang,X Wang‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2023",researchgate.net,,https://www.researchgate.net/profile/Tang-Tianyi-3/publication/369740832_A_Survey_of_Large_Language_Models/links/665fd2e3637e4448a37dd281/A-Survey-of-Large-Language-Models.pdf,,"Ever since the Turing Test was proposed in the 1950s, humans have explored the masteringof language intelligence by machine. Language is essentially a complex, intricate system of¬†‚Ä¶",6302.0,Survey
HaluEval,HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models,A survey of GPT-3 family large language models including ChatGPT and GPT-4,"KS Kalyan- Natural Language Processing Journal, 2024",Elsevier,,https://www.sciencedirect.com/science/article/pii/S2949719123000456,,"Large language models (LLMs) are a special class of pretrained language models (PLMs)obtained by scaling model size, pretraining corpus and computation. LLMs, because of their¬†‚Ä¶",512.0,Survey
HaluEval,HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models,From google gemini to openai q*(q-star): A survey on reshaping the generative artificial intelligence (ai) research landscape,"TR McIntosh,T Susnjak,T Liu,P Watters,D Xu,D Liu‚Ä¶¬†- Technologies, 2025",mdpi.com,,https://www.mdpi.com/2227-7080/13/2/51,,"This comprehensive survey explored the evolving landscape of generative ArtificialIntelligence (AI), with a specific focus on the recent technological breakthroughs and the¬†‚Ä¶",250.0,Survey
HaluEval,HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models,"A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions","L Huang,W Yu,W Ma,W Zhong,Z Feng‚Ä¶¬†- ACM Transactions on¬†‚Ä¶, 2025",dl.acm.org,,https://dl.acm.org/doi/abs/10.1145/3703155,,"The emergence of large language models (LLMs) has marked a significant breakthrough innatural language processing (NLP), fueling a paradigm shift in information acquisition¬†‚Ä¶",3037.0,Survey
HaluEval,HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models,üßú Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models,"Y Zhang,Y Li,L Cui,D Cai,L Liu,T Fu‚Ä¶¬†- Computational¬†‚Ä¶, 2025",direct.mit.edu,,https://direct.mit.edu/coli/article/doi/10.1162/coli.a.16/131631,,"While large language models (LLMs) have demonstrated remarkable capabilities across arange of downstream tasks, a significant concern revolves around their propensity to exhibit¬†‚Ä¶",1549.0,Survey
HaluEval,HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models,Large language models: A survey,"S Minaee,T Mikolov,N Nikzad,M Chenaghlu‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2024",arxiv.org,,https://arxiv.org/abs/2402.06196,,"Large Language Models (LLMs) have drawn a lot of attention due to their strongperformance on a wide range of natural language tasks, since the release of ChatGPT in¬†‚Ä¶",1504.0,Survey
HaluEval,HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models,A survey of large language models,"WX Zhao,K Zhou,J Li,T Tang,X Wang‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2023",researchgate.net,,https://www.researchgate.net/profile/Tang-Tianyi-3/publication/369740832_A_Survey_of_Large_Language_Models/links/665fd2e3637e4448a37dd281/A-Survey-of-Large-Language-Models.pdf,,"Ever since the Turing Test was proposed in the 1950s, humans have explored the masteringof language intelligence by machine. Language is essentially a complex, intricate system of¬†‚Ä¶",6302.0,Survey
HaluEval,HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models,Large language models for information retrieval: A survey,"Y Zhu,H Yuan,S Wang,J Liu,W Liu,C Deng‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2023",arxiv.org,,https://arxiv.org/abs/2308.07107,,"As a primary means of information acquisition, information retrieval (IR) systems, such assearch engines, have integrated themselves into our daily lives. These systems also serve¬†‚Ä¶",585.0,Survey
TRUE,TRUE: Re-evaluating Factual Consistency Evaluation,A survey on evaluation of large language models,"Y Chang,X Wang,J Wang,Y Wu,L Yang‚Ä¶¬†- ACM transactions on¬†‚Ä¶, 2024",dl.acm.org,,https://dl.acm.org/doi/abs/10.1145/3641289,,"Large language models (LLMs) are gaining increasing popularity in both academia andindustry, owing to their unprecedented performance in various applications. As LLMs¬†‚Ä¶",4405.0,Survey
TRUE,TRUE: Re-evaluating Factual Consistency Evaluation,Survey of hallucination in natural language generation,"Z Ji,N Lee,R Frieske,T Yu,D Su,Y Xu,E Ishii‚Ä¶¬†- ACM computing¬†‚Ä¶, 2023",dl.acm.org,,https://dl.acm.org/doi/abs/10.1145/3571730,,Natural Language Generation (NLG) has improved exponentially in recent years thanks tothe development of sequence-to-sequence deep learning technologies such as Transformer¬†‚Ä¶,5570.0,Survey
TRUE,TRUE: Re-evaluating Factual Consistency Evaluation,Large language model alignment: A survey,"T Shen,R Jin,Y Huang,C Liu,W Dong, Z Guo‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2023",arxiv.org,,https://arxiv.org/abs/2309.15025,,"Recent years have witnessed remarkable progress made in large language models (LLMs).Such advancements, while garnering significant attention, have concurrently elicited various¬†‚Ä¶",289.0,Survey
FIB,Evaluating the Factual Consistency of Large Language Models Through News Summarization,"Survey on factuality in large language models: Knowledge, retrieval and domain-specificity","C Wang,X Liu,Y Yue,X Tang,T Zhang‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2023",arxiv.org,,https://arxiv.org/abs/2310.07521,,"This survey addresses the crucial issue of factuality in Large Language Models (LLMs). AsLLMs find applications across diverse domains, the reliability and accuracy of their outputs¬†‚Ä¶",281.0,Survey
FIB,Evaluating the Factual Consistency of Large Language Models Through News Summarization,Evaluating large language models: A comprehensive survey,"Z Guo,R Jin,C Liu,Y Huang,D Shi, L Yu, Y Liu‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2023",arxiv.org,,https://arxiv.org/abs/2310.19736,,Large language models (LLMs) have demonstrated remarkable capabilities across a broadspectrum of tasks. They have attracted significant attention and been deployed in numerous¬†‚Ä¶,237.0,Survey
FIB,Evaluating the Factual Consistency of Large Language Models Through News Summarization,Cognitive mirage: A review of hallucinations in large language models,"H Ye,T Liu,A Zhang, W Hua,W Jia- arXiv preprint arXiv:2309.06794, 2023",arxiv.org,,https://arxiv.org/abs/2309.06794,,"As large language models continue to develop in the field of AI, text generation systems aresusceptible to a worrisome phenomenon known as hallucination. In this study, we¬†‚Ä¶",218.0,Survey
FIB,Evaluating the Factual Consistency of Large Language Models Through News Summarization,A comprehensive survey on process-oriented automatic text summarization with exploration of llm-based methods,"Y Zhang, H Jin, D Meng, J Wang, J Tan¬†- arXiv preprint arXiv:2403.02901, 2024",arxiv.org,,https://arxiv.org/abs/2403.02901,,"Automatic Text Summarization (ATS), utilizing Natural Language Processing (NLP)algorithms, aims to create concise and accurate summaries, thereby significantly reducing¬†‚Ä¶",223.0,Survey
FIB,Evaluating the Factual Consistency of Large Language Models Through News Summarization,A systematic survey of text summarization: From statistical methods to large language models,"H Zhang,PS Yu,J Zhang- ACM Computing Surveys, 2025",dl.acm.org,,https://dl.acm.org/doi/abs/10.1145/3731445,,"Text summarization research has undergone several significant transformations with theadvent of deep neural networks, pre-trained language models (PLMs), and recent large¬†‚Ä¶",113.0,Survey
FEVER,FEVER: a Large-scale Dataset for Fact Extraction and VERification,A comprehensive overview of large language models,"H Naveed,AU Khan,S Qiu,M Saqib,S Anwar‚Ä¶¬†- ACM Transactions on¬†‚Ä¶, 2025",dl.acm.org,,https://dl.acm.org/doi/abs/10.1145/3744746,,Large Language Models (LLMs) have recently demonstrated remarkable capabilities innatural language processing tasks and beyond. This success of LLMs has led to a large¬†‚Ä¶,1815.0,Survey
FEVER,FEVER: a Large-scale Dataset for Fact Extraction and VERification,Survey of hallucination in natural language generation,"Z Ji,N Lee,R Frieske,T Yu,D Su,Y Xu,E Ishii‚Ä¶¬†- ACM computing¬†‚Ä¶, 2023",dl.acm.org,,https://dl.acm.org/doi/abs/10.1145/3571730,,Natural Language Generation (NLG) has improved exponentially in recent years thanks tothe development of sequence-to-sequence deep learning technologies such as Transformer¬†‚Ä¶,5570.0,Survey
FEVER,FEVER: a Large-scale Dataset for Fact Extraction and VERification,Retrieval-augmented generation for large language models: A survey,"Y Gao, Y Xiong, X Gao, K Jia, J Pan,Y Bi‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2023",simg.baai.ac.cn,,https://simg.baai.ac.cn/paperfile/25a43194-c74c-4cd3-b60f-0a1f27f8b8af.pdf,,"Large language models (LLMs) demonstrate powerful capabilities, but they still facechallenges in practical applications, such as hallucinations, slow knowledge updates, and¬†‚Ä¶",3542.0,Survey
FEVER,FEVER: a Large-scale Dataset for Fact Extraction and VERification,Unleashing the potential of prompt engineering in large language models: a comprehensive review,"B Chen,Z Zhang,N Langren√©,S Zhu- arXiv preprint arXiv:2310.14735, 2023",arxiv.org,,https://arxiv.org/abs/2310.14735,,This comprehensive review delves into the pivotal role of prompt engineering in unleashingthe capabilities of Large Language Models (LLMs). The development of Artificial Intelligence¬†‚Ä¶,568.0,Survey
HalluLens,HalluLens: LLM Hallucination Benchmark,Towards holistic evaluation of large audio-language models: A comprehensive survey,"CK Yang, NS Ho,H Lee- arXiv preprint arXiv:2505.15957, 2025",arxiv.org,,https://arxiv.org/abs/2505.15957,,"With advancements in large audio-language models (LALMs), which enhance largelanguage models (LLMs) with auditory capabilities, these models are expected to¬†‚Ä¶",19.0,Survey
HalluLens,HalluLens: LLM Hallucination Benchmark,Never compromise to vulnerabilities: A comprehensive survey on ai governance,"Y Jiang,J Zhao, Y Yuan,T Zhang,Y Huang‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2025",arxiv.org,,https://arxiv.org/abs/2508.08789,,"The rapid advancement of AI has expanded its capabilities across domains, yet introducedcritical technical vulnerabilities, such as algorithmic bias and adversarial sensitivity, that¬†‚Ä¶",2.0,Survey
HalluLens,HalluLens: LLM Hallucination Benchmark,A comprehensive taxonomy of hallucinations in large language models,"M Cossio- arXiv preprint arXiv:2508.01781, 2025",arxiv.org,,https://arxiv.org/abs/2508.01781,,"Large language models (LLMs) have revolutionized natural language processing, yet theirpropensity for hallucination, generating plausible but factually incorrect or fabricated content¬†‚Ä¶",9.0,Survey
FreshQA,FreshLLMs: Refreshing Large Language Models with Search Engine Augmentation,A survey on evaluation of large language models,"Y Chang,X Wang,J Wang,Y Wu,L Yang‚Ä¶¬†- ACM transactions on¬†‚Ä¶, 2024",dl.acm.org,,https://dl.acm.org/doi/abs/10.1145/3641289,,"Large language models (LLMs) are gaining increasing popularity in both academia andindustry, owing to their unprecedented performance in various applications. As LLMs¬†‚Ä¶",4405.0,Survey
FreshQA,FreshLLMs: Refreshing Large Language Models with Search Engine Augmentation,"A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions","L Huang,W Yu,W Ma,W Zhong,Z Feng‚Ä¶¬†- ACM Transactions on¬†‚Ä¶, 2025",dl.acm.org,,https://dl.acm.org/doi/abs/10.1145/3703155,,"The emergence of large language models (LLMs) has marked a significant breakthrough innatural language processing (NLP), fueling a paradigm shift in information acquisition¬†‚Ä¶",3037.0,Survey
FreshQA,FreshLLMs: Refreshing Large Language Models with Search Engine Augmentation,Large language models for information retrieval: A survey,"Y Zhu,H Yuan,S Wang,J Liu,W Liu,C Deng‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2023",arxiv.org,,https://arxiv.org/abs/2308.07107,,"As a primary means of information acquisition, information retrieval (IR) systems, such assearch engines, have integrated themselves into our daily lives. These systems also serve¬†‚Ä¶",585.0,Survey
FreshQA,FreshLLMs: Refreshing Large Language Models with Search Engine Augmentation,"Survey on factuality in large language models: Knowledge, retrieval and domain-specificity","C Wang,X Liu,Y Yue,X Tang,T Zhang‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2023",arxiv.org,,https://arxiv.org/abs/2310.07521,,"This survey addresses the crucial issue of factuality in Large Language Models (LLMs). AsLLMs find applications across diverse domains, the reliability and accuracy of their outputs¬†‚Ä¶",281.0,Survey
FreshQA,FreshLLMs: Refreshing Large Language Models with Search Engine Augmentation,Knowledge conflicts for llms: A survey,"R Xu,Z Qi,Z Guo,C Wang,H Wang,Y Zhang‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2024",arxiv.org,,https://arxiv.org/abs/2403.08319,,"This survey provides an in-depth analysis of knowledge conflicts for large language models(LLMs), highlighting the complex challenges they encounter when blending contextual and¬†‚Ä¶",188.0,Survey
ETHICS,Aligning AI With Shared Human Values,A survey on evaluation of large language models,"Y Chang,X Wang,J Wang,Y Wu,L Yang‚Ä¶¬†- ACM transactions on¬†‚Ä¶, 2024",dl.acm.org,,https://dl.acm.org/doi/abs/10.1145/3641289,,"Large language models (LLMs) are gaining increasing popularity in both academia andindustry, owing to their unprecedented performance in various applications. As LLMs¬†‚Ä¶",4405.0,Survey
ETHICS,Aligning AI With Shared Human Values,Challenges and applications of large language models,"J Kaddour,J Harris,M Mozes,H Bradley‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2023",arxiv.org,,https://arxiv.org/abs/2307.10169,,"Large Language Models (LLMs) went from non-existent to ubiquitous in the machinelearning discourse within a few years. Due to the fast pace of the field, it is difficult to identify¬†‚Ä¶",768.0,Survey
ETHICS,Aligning AI With Shared Human Values,"Connecting the dots in trustworthy Artificial Intelligence: From AI principles, ethics, and key requirements to responsible AI systems and regulation","N D√≠az-Rodr√≠guez,J Del Ser,M Coeckelbergh‚Ä¶¬†- Information¬†‚Ä¶, 2023",Elsevier,,https://www.sciencedirect.com/science/article/pii/S1566253523002129,,Abstract Trustworthy Artificial Intelligence (AI) is based on seven technical requirementssustained over three main pillars that should be met throughout the system's entire life cycle¬†‚Ä¶,851.0,Survey
ETHICS,Aligning AI With Shared Human Values,DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models.,"B Wang,W Chen,H Pei,C Xie,M Kang,C Zhang,C Xu‚Ä¶¬†- NeurIPS, 2023",blogs.qub.ac.uk,,https://blogs.qub.ac.uk/wp-content/uploads/sites/7/2024/01/A-comprehensive-Assessment-of-Trustworthiness-in-GPT-Models.pdf,,"Abstract Generative Pre-trained Transformer (GPT) models have exhibited exciting progressin their capabilities, capturing the interest of practitioners and the public alike. Yet, while the¬†‚Ä¶",625.0,Survey
Moral Stories,Moral Stories: Situated Reasoning about Norms Intents Actions and their Consequences,Ai alignment: A comprehensive survey,"J Ji,T Qiu,B Chen, B Zhang,H Lou, K Wang‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2023",arxiv.org,,https://arxiv.org/abs/2310.19852,,"AI alignment aims to make AI systems behave in line with human intentions and values. AsAI systems grow more capable, so do risks from misalignment. To provide a comprehensive¬†‚Ä¶",459.0,Survey
Moral Stories,Moral Stories: Situated Reasoning about Norms Intents Actions and their Consequences,"Natural language reasoning, a survey","F Yu,H Zhang,P Tiwari,B Wang- ACM Computing Surveys, 2024",dl.acm.org,,https://dl.acm.org/doi/abs/10.1145/3664194,,"This survey article proposes a clearer view of Natural Language Reasoning (NLR) in thefield of Natural Language Processing (NLP), both conceptually and practically¬†‚Ä¶",161.0,Survey
Moral Stories,Moral Stories: Situated Reasoning about Norms Intents Actions and their Consequences,DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models.,"B Wang,W Chen,H Pei,C Xie,M Kang,C Zhang,C Xu‚Ä¶¬†- NeurIPS, 2023",blogs.qub.ac.uk,,https://blogs.qub.ac.uk/wp-content/uploads/sites/7/2024/01/A-comprehensive-Assessment-of-Trustworthiness-in-GPT-Models.pdf,,"Abstract Generative Pre-trained Transformer (GPT) models have exhibited exciting progressin their capabilities, capturing the interest of practitioners and the public alike. Yet, while the¬†‚Ä¶",625.0,Survey
Moral Stories,Moral Stories: Situated Reasoning about Norms Intents Actions and their Consequences,From pretraining data to language models to downstream tasks: Tracking the trails of political biases leading to unfair NLP models,"S Feng,CY Park,Y Liu,Y Tsvetkov- arXiv preprint arXiv:2305.08283, 2023",arxiv.org,,https://arxiv.org/abs/2305.08283,,"Language models (LMs) are pretrained on diverse data sources, including news, discussionforums, books, and online encyclopedias. A significant portion of this data includes opinions¬†‚Ä¶",373.0,Survey
Moral Stories,Moral Stories: Situated Reasoning about Norms Intents Actions and their Consequences,Evaluating the moral beliefs encoded in llms,"N Scherrer,C Shi,A Feder‚Ä¶¬†- Advances in Neural¬†‚Ä¶, 2023",proceedings.neurips.cc,,https://proceedings.neurips.cc/paper_files/paper/2023/hash/a2cf225ba392627529efef14dc857e22-Abstract-Conference.html,,"This paper presents a case study on the design, administration, post-processing, andevaluation of surveys on large language models (LLMs). It comprises two components:(1) A¬†‚Ä¶",217.0,Survey
Moral Stories,Moral Stories: Situated Reasoning about Norms Intents Actions and their Consequences,Culturally aware and adapted nlp: A taxonomy and a survey of the state of the art,"CC Liu,I Gurevych,A Korhonen- Transactions of the Association for¬†‚Ä¶, 2025",direct.mit.edu,,https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00760/131587,,"The surge of interest in culture in NLP has inspired much recent research, but a sharedunderstanding of ‚Äúculture‚Äù remains unclear, making it difficult to evaluate progress in this¬†‚Ä¶",48.0,Survey
Jiminy Cricket,What Would Jiminy Cricket Do? Towards Agents That Behave Morally,Safetyprompts: a systematic review of open datasets for evaluating and improving large language model safety,"P R√∂ttger,F Pernisi,B Vidgen,D Hovy- Proceedings of the AAAI¬†‚Ä¶, 2025",ojs.aaai.org,,https://ojs.aaai.org/index.php/AAAI/article/view/34975,,The last two years have seen a rapid growth in concerns around the safety of largelanguage models (LLMs). Researchers and practitioners have met these concerns by¬†‚Ä¶,52.0,Survey
Jiminy Cricket,What Would Jiminy Cricket Do? Towards Agents That Behave Morally,DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models.,"B Wang,W Chen,H Pei,C Xie,M Kang,C Zhang,C Xu‚Ä¶¬†- NeurIPS, 2023",blogs.qub.ac.uk,,https://blogs.qub.ac.uk/wp-content/uploads/sites/7/2024/01/A-comprehensive-Assessment-of-Trustworthiness-in-GPT-Models.pdf,,"Abstract Generative Pre-trained Transformer (GPT) models have exhibited exciting progressin their capabilities, capturing the interest of practitioners and the public alike. Yet, while the¬†‚Ä¶",625.0,Survey
Jiminy Cricket,What Would Jiminy Cricket Do? Towards Agents That Behave Morally,Representation engineering: A top-down approach to ai transparency,"A Zou,L Phan,S Chen,J Campbell,P Guo‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2023",arxiv.org,,https://arxiv.org/abs/2310.01405,,"In this paper, we identify and characterize the emerging area of representation engineering(RepE), an approach to enhancing the transparency of AI systems that draws on insights¬†‚Ä¶",567.0,Survey
Jiminy Cricket,What Would Jiminy Cricket Do? Towards Agents That Behave Morally,Unsolved problems in ml safety,"D Hendrycks,N Carlini,J Schulman‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2021",arxiv.org,,https://arxiv.org/abs/2109.13916,,"Machine learning (ML) systems are rapidly increasing in size, are acquiring newcapabilities, and are increasingly deployed in high-stakes settings. As with other powerful¬†‚Ä¶",459.0,Survey
Jiminy Cricket,What Would Jiminy Cricket Do? Towards Agents That Behave Morally,Open-Ethical AI: Advancements in Open-Source Human-Centric Neural Language Models,"S Sicari,JF Cevallos M,A Rizzardi‚Ä¶¬†- ACM Computing¬†‚Ä¶, 2024",dl.acm.org,,https://dl.acm.org/doi/abs/10.1145/3703454,,"This survey summarises the most recent methods for building and assessing helpful, honest,and harmless neural language models, considering small, medium, and large-size models¬†‚Ä¶",9.0,Survey
SCRUPLES,Scruples: A Corpus of Community Ethical Judgments on 32000 Real-Life Anecdotes,Evaluating large language models: A comprehensive survey,"Z Guo,R Jin,C Liu,Y Huang,D Shi, L Yu, Y Liu‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2023",arxiv.org,,https://arxiv.org/abs/2310.19736,,Large language models (LLMs) have demonstrated remarkable capabilities across a broadspectrum of tasks. They have attracted significant attention and been deployed in numerous¬†‚Ä¶,237.0,Survey
SCRUPLES,Scruples: A Corpus of Community Ethical Judgments on 32000 Real-Life Anecdotes,"Navigating llm ethics: Advancements, challenges, and future directions","J Jiao,S Afroogh,Y Xu,C Phillips- AI and Ethics, 2025",Springer,,https://link.springer.com/article/10.1007/s43681-025-00814-5,,This study addresses ethical issues surrounding Large Language Models (LLMs) within thefield of artificial intelligence. It explores the common ethical challenges posed by both LLMs¬†‚Ä¶,91.0,Survey
SCRUPLES,Scruples: A Corpus of Community Ethical Judgments on 32000 Real-Life Anecdotes,Evaluating the moral beliefs encoded in llms,"N Scherrer,C Shi,A Feder‚Ä¶¬†- Advances in Neural¬†‚Ä¶, 2023",proceedings.neurips.cc,,https://proceedings.neurips.cc/paper_files/paper/2023/hash/a2cf225ba392627529efef14dc857e22-Abstract-Conference.html,,"This paper presents a case study on the design, administration, post-processing, andevaluation of surveys on large language models (LLMs). It comprises two components:(1) A¬†‚Ä¶",217.0,Survey
SCRUPLES,Scruples: A Corpus of Community Ethical Judgments on 32000 Real-Life Anecdotes,Large language model alignment: A survey,"T Shen,R Jin,Y Huang,C Liu,W Dong, Z Guo‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2023",arxiv.org,,https://arxiv.org/abs/2309.15025,,"Recent years have witnessed remarkable progress made in large language models (LLMs).Such advancements, while garnering significant attention, have concurrently elicited various¬†‚Ä¶",289.0,Survey
SCRUPLES,Scruples: A Corpus of Community Ethical Judgments on 32000 Real-Life Anecdotes,Knowledge of cultural moral norms in large language models,"A Ramezani,Y Xu- arXiv preprint arXiv:2306.01857, 2023",arxiv.org,,https://arxiv.org/abs/2306.01857,,"Moral norms vary across cultures. A recent line of work suggests that English large languagemodels contain human-like moral biases, but these studies typically do not examine moral¬†‚Ä¶",121.0,Survey
SCRUPLES,Scruples: A Corpus of Community Ethical Judgments on 32000 Real-Life Anecdotes,Safetyprompts: a systematic review of open datasets for evaluating and improving large language model safety,"P R√∂ttger,F Pernisi,B Vidgen,D Hovy- Proceedings of the AAAI¬†‚Ä¶, 2025",ojs.aaai.org,,https://ojs.aaai.org/index.php/AAAI/article/view/34975,,The last two years have seen a rapid growth in concerns around the safety of largelanguage models (LLMs). Researchers and practitioners have met these concerns by¬†‚Ä¶,52.0,Survey
MoralBench,MoralBench: Moral Evaluation of LLMs,"Large language model psychometrics: A systematic review of evaluation, validation, and enhancement","H Ye, J Jin,Y Xie, X Zhang,G Song- arXiv preprint arXiv:2505.08245, 2025",arxiv.org,,https://arxiv.org/abs/2505.08245,,"The rapid advancement of large language models (LLMs) has outpaced traditionalevaluation methodologies. It presents novel challenges, such as measuring human-like¬†‚Ä¶",12.0,Survey
MoralBench,MoralBench: Moral Evaluation of LLMs,Recommender systems meet large language model agents: A survey,"X Zhu,Y Wang,H Gao,W Xu,C Wang‚Ä¶¬†- ‚Ä¶¬†and Trends¬Æ in¬†‚Ä¶, 2025",nowpublishers.com,,https://www.nowpublishers.com/article/Details/SEC-050,,"In recent years, the integration of Large Language Models (LLMs) and RecommenderSystems (RS) has revolutionized the way personalized and intelligent user experiences are¬†‚Ä¶",15.0,Survey
MoralBench,MoralBench: Moral Evaluation of LLMs,Normative evaluation of large language models with everyday moral dilemmas,"P Sachdeva, T van Nuenen¬†- Proceedings of the 2025 ACM Conference¬†‚Ä¶, 2025",dl.acm.org,,https://dl.acm.org/doi/abs/10.1145/3715275.3732044,,The rapid adoption of large language models (LLMs) has spurred extensive research intotheir encoded moral norms and decision-making processes. While prior work often¬†‚Ä¶,6.0,Survey
MoralBench,MoralBench: Moral Evaluation of LLMs,Whose morality do they speak? Unraveling cultural bias in multilingual language models,"M Aksoy- Natural Language Processing Journal, 2025",Elsevier,,https://www.sciencedirect.com/science/article/pii/S2949719125000482,,"Large language models (LLMs) have become integral tools in diverse domains, yet theirmoral reasoning capabilities across cultural and linguistic contexts remain underexplored¬†‚Ä¶",9.0,Survey
MoralBench,MoralBench: Moral Evaluation of LLMs,Large Language Models meet moral values: A comprehensive assessment of moral abilities,"L Bulla,S De Giorgis,M Mongiov√¨‚Ä¶¬†- Computers in Human¬†‚Ä¶, 2025",Elsevier,,https://www.sciencedirect.com/science/article/pii/S2451958825000247,,"Automatic moral classification in textual data is crucial for various fields including NaturalLanguage Processing (NLP), social sciences, and ethical AI development. Despite¬†‚Ä¶",10.0,Survey
Social Chemistry 101,Social Chemistry 101: Learning to Reason about Social and Moral Norms,Ai alignment: A comprehensive survey,"J Ji,T Qiu,B Chen, B Zhang,H Lou, K Wang‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2023",arxiv.org,,https://arxiv.org/abs/2310.19852,,"AI alignment aims to make AI systems behave in line with human intentions and values. AsAI systems grow more capable, so do risks from misalignment. To provide a comprehensive¬†‚Ä¶",459.0,Survey
Social Chemistry 101,Social Chemistry 101: Learning to Reason about Social and Moral Norms,Evaluating large language models: A comprehensive survey,"Z Guo,R Jin,C Liu,Y Huang,D Shi, L Yu, Y Liu‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2023",arxiv.org,,https://arxiv.org/abs/2310.19736,,Large language models (LLMs) have demonstrated remarkable capabilities across a broadspectrum of tasks. They have attracted significant attention and been deployed in numerous¬†‚Ä¶,237.0,Survey
Social Chemistry 101,Social Chemistry 101: Learning to Reason about Social and Moral Norms,DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models.,"B Wang,W Chen,H Pei,C Xie,M Kang,C Zhang,C Xu‚Ä¶¬†- NeurIPS, 2023",blogs.qub.ac.uk,,https://blogs.qub.ac.uk/wp-content/uploads/sites/7/2024/01/A-comprehensive-Assessment-of-Trustworthiness-in-GPT-Models.pdf,,"Abstract Generative Pre-trained Transformer (GPT) models have exhibited exciting progressin their capabilities, capturing the interest of practitioners and the public alike. Yet, while the¬†‚Ä¶",625.0,Survey
Social Chemistry 101,Social Chemistry 101: Learning to Reason about Social and Moral Norms,Evaluating the moral beliefs encoded in llms,"N Scherrer,C Shi,A Feder‚Ä¶¬†- Advances in Neural¬†‚Ä¶, 2023",proceedings.neurips.cc,,https://proceedings.neurips.cc/paper_files/paper/2023/hash/a2cf225ba392627529efef14dc857e22-Abstract-Conference.html,,"This paper presents a case study on the design, administration, post-processing, andevaluation of surveys on large language models (LLMs). It comprises two components:(1) A¬†‚Ä¶",217.0,Survey
Social Chemistry 101,Social Chemistry 101: Learning to Reason about Social and Moral Norms,Large language model alignment: A survey,"T Shen,R Jin,Y Huang,C Liu,W Dong, Z Guo‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2023",arxiv.org,,https://arxiv.org/abs/2309.15025,,"Recent years have witnessed remarkable progress made in large language models (LLMs).Such advancements, while garnering significant attention, have concurrently elicited various¬†‚Ä¶",289.0,Survey
Social Chemistry 101,Social Chemistry 101: Learning to Reason about Social and Moral Norms,"Natural language reasoning, a survey","F Yu,H Zhang,P Tiwari,B Wang- ACM Computing Surveys, 2024",dl.acm.org,,https://dl.acm.org/doi/abs/10.1145/3664194,,"This survey article proposes a clearer view of Natural Language Reasoning (NLR) in thefield of Natural Language Processing (NLP), both conceptually and practically¬†‚Ä¶",161.0,Survey
Social Chemistry 101,Social Chemistry 101: Learning to Reason about Social and Moral Norms,A survey on fairness in large language models,"Y Li,M Du,R Song,X Wang,Y Wang- arXiv preprint arXiv:2308.10149, 2023",arxiv.org,,https://arxiv.org/abs/2308.10149,,"Large Language Models (LLMs) have shown powerful performance and developmentprospects and are widely deployed in the real world. However, LLMs can capture social¬†‚Ä¶",166.0,Survey
Delphi,Delphi: Towards Machine Ethics and Norms,Evaluating the moral beliefs encoded in llms,"N Scherrer,C Shi,A Feder‚Ä¶¬†- Advances in Neural¬†‚Ä¶, 2023",proceedings.neurips.cc,,https://proceedings.neurips.cc/paper_files/paper/2023/hash/a2cf225ba392627529efef14dc857e22-Abstract-Conference.html,,"This paper presents a case study on the design, administration, post-processing, andevaluation of surveys on large language models (LLMs). It comprises two components:(1) A¬†‚Ä¶",217.0,Survey
Delphi,Delphi: Towards Machine Ethics and Norms,Open-Ethical AI: Advancements in Open-Source Human-Centric Neural Language Models,"S Sicari,JF Cevallos M,A Rizzardi‚Ä¶¬†- ACM Computing¬†‚Ä¶, 2024",dl.acm.org,,https://dl.acm.org/doi/abs/10.1145/3703454,,"This survey summarises the most recent methods for building and assessing helpful, honest,and harmless neural language models, considering small, medium, and large-size models¬†‚Ä¶",9.0,Survey
Delphi,Delphi: Towards Machine Ethics and Norms,Bridging the gap: A survey on integrating (human) feedback for natural language generation,"P Fernandes,A Madaan,E Liu,A Farinhas‚Ä¶¬†- Transactions of the¬†‚Ä¶, 2023",direct.mit.edu,,https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00626/118795,,"Natural language generation has witnessed significant advancements due to the training oflarge language models on vast internet-scale datasets. Despite these advancements, there¬†‚Ä¶",99.0,Survey
STORAL,A Corpus for Understanding and Generating Moral Stories,Open-world story generation with structured knowledge enhancement: A comprehensive survey,"Y Wang,J Lin,Z Yu,W Hu,BF Karlsson- Neurocomputing, 2023",Elsevier,,https://www.sciencedirect.com/science/article/pii/S0925231223009153,,"Storytelling and narrative are fundamental to human experience, intertwined with our socialand cultural engagement. As such, researchers have long attempted to create systems that¬†‚Ä¶",42.0,Survey
STORAL,A Corpus for Understanding and Generating Moral Stories,"Values, ethics, morals? on the use of moral concepts in NLP research","K Vida, J Simon,A Lauscher- arXiv preprint arXiv:2310.13915, 2023",arxiv.org,,https://arxiv.org/abs/2310.13915,,"With language technology increasingly affecting individuals' lives, many recent works haveinvestigated the ethical aspects of NLP. Among other topics, researchers focused on the¬†‚Ä¶",24.0,Survey
STORAL,A Corpus for Understanding and Generating Moral Stories,Bridging cultural nuances in dialogue agents through cultural value surveys,"Y Cao,M Chen,D Hershcovich- arXiv preprint arXiv:2401.10352, 2024",arxiv.org,,https://arxiv.org/abs/2401.10352,,The cultural landscape of interactions with dialogue agents is a compelling yet relativelyunexplored territory. It's clear that various sociocultural aspects--from communication styles¬†‚Ä¶,10.0,Survey
STORAL,A Corpus for Understanding and Generating Moral Stories,A survey on modelling morality for text analysis,"I Reinig,M Becker, I Rehbein,SP Ponzetto- 2024",madoc.bib.uni-mannheim.de,,https://madoc.bib.uni-mannheim.de/67689/,,"In this survey, we provide a systematic review of recent work on modelling morality in text, anarea of research that has garnered increasing attention in recent years. Our survey is¬†‚Ä¶",9.0,Survey
Moral Integrity Corpus,The Moral Integrity Corpus: A Benchmark for Ethical Dialogue Systems,Ai alignment: A comprehensive survey,"J Ji,T Qiu,B Chen, B Zhang,H Lou, K Wang‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2023",arxiv.org,,https://arxiv.org/abs/2310.19852,,"AI alignment aims to make AI systems behave in line with human intentions and values. AsAI systems grow more capable, so do risks from misalignment. To provide a comprehensive¬†‚Ä¶",459.0,Survey
Moral Integrity Corpus,The Moral Integrity Corpus: A Benchmark for Ethical Dialogue Systems,Evaluating large language models: A comprehensive survey,"Z Guo,R Jin,C Liu,Y Huang,D Shi, L Yu, Y Liu‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2023",arxiv.org,,https://arxiv.org/abs/2310.19736,,Large language models (LLMs) have demonstrated remarkable capabilities across a broadspectrum of tasks. They have attracted significant attention and been deployed in numerous¬†‚Ä¶,237.0,Survey
Moral Integrity Corpus,The Moral Integrity Corpus: A Benchmark for Ethical Dialogue Systems,Large language models in education: Vision and opportunities,"W Gan, Z Qi, J Wu, JCW Lin¬†- 2023 IEEE international¬†‚Ä¶, 2023",ieeexplore.ieee.org,,https://ieeexplore.ieee.org/abstract/document/10386291/,,"With the rapid development of artificial intelligence technology, large language models(LLMs) have become a hot research topic. Education plays an important role in human¬†‚Ä¶",202.0,Survey
Moral Integrity Corpus,The Moral Integrity Corpus: A Benchmark for Ethical Dialogue Systems,Culturally aware and adapted nlp: A taxonomy and a survey of the state of the art,"CC Liu,I Gurevych,A Korhonen- Transactions of the Association for¬†‚Ä¶, 2025",direct.mit.edu,,https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00760/131587,,"The surge of interest in culture in NLP has inspired much recent research, but a sharedunderstanding of ‚Äúculture‚Äù remains unclear, making it difficult to evaluate progress in this¬†‚Ä¶",48.0,Survey
Moral Integrity Corpus,The Moral Integrity Corpus: A Benchmark for Ethical Dialogue Systems,Survey of cultural awareness in language models: Text and beyond,"S Pawar,J Park,J Jin,A Arora,J Myung‚Ä¶¬†- Computational¬†‚Ä¶, 2025",direct.mit.edu,,https://direct.mit.edu/coli/article/doi/10.1162/COLI.a.14/130804,,"Large-scale deployment of large language models (LLMs) in various applications, such aschatbots and virtual assistants, requires LLMs to be culturally sensitive to the user to ensure¬†‚Ä¶",61.0,Survey
Moral Integrity Corpus,The Moral Integrity Corpus: A Benchmark for Ethical Dialogue Systems,Safetyprompts: a systematic review of open datasets for evaluating and improving large language model safety,"P R√∂ttger,F Pernisi,B Vidgen,D Hovy- Proceedings of the AAAI¬†‚Ä¶, 2025",ojs.aaai.org,,https://ojs.aaai.org/index.php/AAAI/article/view/34975,,The last two years have seen a rapid growth in concerns around the safety of largelanguage models (LLMs). Researchers and practitioners have met these concerns by¬†‚Ä¶,52.0,Survey
RobustBench,RobustBench: a standardized adversarial robustness benchmark,"Interpretable deep learning: Interpretation, interpretability, trustworthiness, and beyond","X Li,H Xiong,X Li, X Wu,X Zhang,J Liu,J Bian‚Ä¶¬†- ‚Ä¶¬†and Information Systems, 2022",Springer,,https://link.springer.com/article/10.1007/s10115-022-01756-8,,"Deep neural networks have been well-known for their superb handling of various machinelearning and artificial intelligence tasks. However, due to their over-parameterized black-box¬†‚Ä¶",573.0,Survey
RobustBench,RobustBench: a standardized adversarial robustness benchmark,Foundational challenges in assuring alignment and safety of large language models,"U Anwar,A Saparov,J Rando,D Paleka‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2024",arxiv.org,,https://arxiv.org/abs/2404.09932,,This work identifies 18 foundational challenges in assuring the alignment and safety of largelanguage models (LLMs). These challenges are organized into three different categories¬†‚Ä¶,254.0,Survey
AutoAttack,Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks,"A survey of attacks on large vision‚Äìlanguage models: Resources, advances, and future trends","D Liu, M Yang,X Qu,P Zhou‚Ä¶¬†- IEEE Transactions on¬†‚Ä¶, 2025",ieeexplore.ieee.org,,https://ieeexplore.ieee.org/abstract/document/11127221/,,"With the significant development of large models in recent years, large vision‚Äìlanguagemodels (LVLMs) have demonstrated remarkable capabilities across a wide range of¬†‚Ä¶",86.0,Survey
AutoAttack,Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks,How deep learning sees the world: A survey on adversarial attacks & defenses,"JC Costa,T Roxo,H Proen√ßa,PRM Inacio- IEEE Access, 2024",ieeexplore.ieee.org,,https://ieeexplore.ieee.org/abstract/document/10510296/,,"Deep Learning is currently used to perform multiple tasks, such as object recognition, facerecognition, and natural language processing. However, Deep Neural Networks (DNNs) are¬†‚Ä¶",143.0,Survey
ImageNet-C / ImageNet-P,Benchmarking Neural Network Robustness to Common Corruptions and Perturbations,The rise and potential of large language model based agents: A survey,"Z Xi,W Chen,X Guo,W He,Y Ding, B Hong‚Ä¶¬†- Science China¬†‚Ä¶, 2025",Springer,,https://link.springer.com/article/10.1007/s11432-024-4222-0,,"For a long time, researchers have sought artificial intelligence (AI) that matches or exceedshuman intelligence. AI agents, which are artificial entities capable of sensing the¬†‚Ä¶",1558.0,Survey
ImageNet-C / ImageNet-P,Benchmarking Neural Network Robustness to Common Corruptions and Perturbations,Ai alignment: A comprehensive survey,"J Ji,T Qiu,B Chen, B Zhang,H Lou, K Wang‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2023",arxiv.org,,https://arxiv.org/abs/2310.19852,,"AI alignment aims to make AI systems behave in line with human intentions and values. AsAI systems grow more capable, so do risks from misalignment. To provide a comprehensive¬†‚Ä¶",459.0,Survey
WILDS,WILDS: A Benchmark of in-the-Wild Distribution Shifts,The current and future state of AI interpretation of medical images,"P Rajpurkar,MP Lungren- New England Journal of Medicine, 2023",Mass Medical Soc,,https://www.nejm.org/doi/full/10.1056/NEJMra2301725,,The Current and Future State of AI Interpretation of Medical Images | New England Journal ofMedicine Skip to main content The New England Journal of Medicine homepage Advanced¬†‚Ä¶,396.0,Survey
WILDS,WILDS: A Benchmark of in-the-Wild Distribution Shifts,Holistic evaluation of language models,"P Liang,R Bommasani,T Lee, D Tsipras‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2022",arxiv.org,,https://arxiv.org/abs/2211.09110,,"Language models (LMs) are becoming the foundation for almost all major languagetechnologies, but their capabilities, limitations, and risks are not well understood. We present¬†‚Ä¶",1905.0,Survey
WILDS,WILDS: A Benchmark of in-the-Wild Distribution Shifts,DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models.,"B Wang,W Chen,H Pei,C Xie,M Kang,C Zhang,C Xu‚Ä¶¬†- NeurIPS, 2023",blogs.qub.ac.uk,,https://blogs.qub.ac.uk/wp-content/uploads/sites/7/2024/01/A-comprehensive-Assessment-of-Trustworthiness-in-GPT-Models.pdf,,"Abstract Generative Pre-trained Transformer (GPT) models have exhibited exciting progressin their capabilities, capturing the interest of practitioners and the public alike. Yet, while the¬†‚Ä¶",625.0,Survey
WILDS,WILDS: A Benchmark of in-the-Wild Distribution Shifts,Generalized out-of-distribution detection: A survey,"J Yang,K Zhou,Y Li,Z Liu- International Journal of Computer Vision, 2024",Springer,,https://link.springer.com/article/10.1007/s11263-024-02117-4,,"Abstract Out-of-distribution (OOD) detection is critical to ensuring the reliability and safety ofmachine learning systems. For instance, in autonomous driving, we would like the driving¬†‚Ä¶",1451.0,Survey
WILDS,WILDS: A Benchmark of in-the-Wild Distribution Shifts,Trustworthy llms: a survey and guideline for evaluating large language models' alignment,"Y Liu,Y Yao,JF Ton,X Zhang,R Guo,H Cheng‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2023",arxiv.org,,https://arxiv.org/abs/2308.05374,,"Ensuring alignment, which refers to making models behave in accordance with humanintentions [1, 2], has become a critical task before deploying large language models (LLMs)¬†‚Ä¶",516.0,Survey
WILDS,WILDS: A Benchmark of in-the-Wild Distribution Shifts,Data-centric artificial intelligence: A survey,"D Zha, ZP Bhat,KH Lai,F Yang,Z Jiang‚Ä¶¬†- ACM Computing¬†‚Ä¶, 2025",dl.acm.org,,https://dl.acm.org/doi/abs/10.1145/3711118,,Artificial Intelligence (AI) is making a profound impact in almost every domain. A vital enablerof its great success is the availability of abundant and high-quality data for building machine¬†‚Ä¶,441.0,Survey
OoD-Bench,OoD-Bench: Quantifying and Understanding Two Dimensions of Out-of-Distribution Generalization,AUC maximization in the era of big data and AI: A survey,"T Yang,Y Ying- ACM computing surveys, 2022",dl.acm.org,,https://dl.acm.org/doi/abs/10.1145/3554729,,"Area under the ROC curve, aka AUC, is a measure of choice for assessing the performanceof a classifier for imbalanced data. AUC maximization refers to a learning paradigm that¬†‚Ä¶",146.0,Survey
OoD-Bench,OoD-Bench: Quantifying and Understanding Two Dimensions of Out-of-Distribution Generalization,"Physics-informed machine learning: A survey on problems, methods and applications","Z Hao,S Liu,Y Zhang,C Ying,Y Feng,H Su‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2022",arxiv.org,,https://arxiv.org/abs/2211.08064,,"Recent advances of data-driven machine learning have revolutionized fields like computervision, reinforcement learning, and many scientific and engineering domains. In many real¬†‚Ä¶",263.0,Survey
OoD-Bench,OoD-Bench: Quantifying and Understanding Two Dimensions of Out-of-Distribution Generalization,A survey on evaluation of out-of-distribution generalization,"H Yu,J Liu,X Zhang,J Wu,P Cui- arXiv preprint arXiv:2403.01874, 2024",arxiv.org,,https://arxiv.org/abs/2403.01874,,"Machine learning models, while progressively advanced, rely heavily on the IID assumption,which is often unfulfilled in practice due to inevitable distribution shifts. This renders them¬†‚Ä¶",35.0,Survey
SoK: Certified Robustness,SoK: Certified Robustness for Deep Neural Networks,Security and privacy challenges of large language models: A survey,"BC Das,MH Amini,Y Wu- ACM Computing Surveys, 2025",dl.acm.org,,https://dl.acm.org/doi/abs/10.1145/3712001,,"Large language models (LLMs) have demonstrated extraordinary capabilities andcontributed to multiple fields, such as generating and summarizing text, language¬†‚Ä¶",388.0,Survey
SoK: Certified Robustness,SoK: Certified Robustness for Deep Neural Networks,"Adversarial machine learning: a review of methods, tools, and critical industry sectors.","S Pelekis,T Koutroubas,A Blika‚Ä¶¬†- Artificial Intelligence¬†‚Ä¶, 2025",drive.google.com,,https://drive.google.com/file/d/1N1s5ndgZkIXhlJeo4kisJ1ey2Fw6_zHY/view,,"The rapid advancement of Artificial Intelligence (AI), particularly Machine Learning (ML) andDeep Learning (DL), has produced high-performance models widely used in various¬†‚Ä¶",14.0,Survey
SoK: Certified Robustness,SoK: Certified Robustness for Deep Neural Networks,Adversarial robustness of deep neural networks: A survey from a formal verification perspective,"MH Meng,G Bai,SG Teo, Z Hou,Y Xiao‚Ä¶¬†- ‚Ä¶¬†on Dependable and¬†‚Ä¶, 2022",ieeexplore.ieee.org,,https://ieeexplore.ieee.org/abstract/document/9785704/,,"Neural networks have been widely applied in security applications such as spam andphishing detection, intrusion prevention, and malware detection. This black-box method¬†‚Ä¶",109.0,Survey
SoK: Certified Robustness,SoK: Certified Robustness for Deep Neural Networks,A comprehensive survey of robust deep learning in computer vision,"J Liu,Y Jin- Journal of Automation and Intelligence, 2023",Elsevier,,https://www.sciencedirect.com/science/article/pii/S294985542300045X,,"Deep learning has presented remarkable progress in various tasks. Despite the excellentperformance, deep learning models remain not robust, especially to well-designed¬†‚Ä¶",42.0,Survey
Adversarial Robustness Benchmark,Benchmarking Adversarial Robustness on Image Classification,Data-centric artificial intelligence: A survey,"D Zha, ZP Bhat,KH Lai,F Yang,Z Jiang‚Ä¶¬†- ACM Computing¬†‚Ä¶, 2025",dl.acm.org,,https://dl.acm.org/doi/abs/10.1145/3711118,,Artificial Intelligence (AI) is making a profound impact in almost every domain. A vital enablerof its great success is the availability of abundant and high-quality data for building machine¬†‚Ä¶,441.0,Survey
Adversarial Robustness Benchmark,Benchmarking Adversarial Robustness on Image Classification,Toward the third generation artificial intelligence,"B Zhang,J Zhu,H Su- Science China Information Sciences, 2023",Springer,,https://link.springer.com/article/10.1007/s11432-021-3449-x,,"There have been two competing paradigms in artificial intelligence (AI) development eversince its birth in 1956, ie, symbolism and connectionism (or sub-symbolism). While¬†‚Ä¶",346.0,Survey
Adversarial Robustness Benchmark,Benchmarking Adversarial Robustness on Image Classification,Interpreting adversarial examples in deep learning: A review,"S Han,C Lin,C Shen,Q Wang, X Guan¬†- ACM Computing Surveys, 2023",dl.acm.org,,https://dl.acm.org/doi/abs/10.1145/3594869,,Deep learning technology is increasingly being applied in safety-critical scenarios but hasrecently been found to be susceptible to imperceptible adversarial perturbations. This raises¬†‚Ä¶,102.0,Survey
GRB,Graph Robustness Benchmark: Benchmarking the Adversarial Robustness of Graph Machine Learning,"Trustworthy graph neural networks: Aspects, methods, and trends","H Zhang,B Wu,X Yuan,S Pan,H Tong‚Ä¶¬†- Proceedings of the¬†‚Ä¶, 2024",ieeexplore.ieee.org,,https://ieeexplore.ieee.org/abstract/document/10477407/,,"Graph neural networks (GNNs) have emerged as a series of competent graph learningmethods for diverse real-world scenarios, ranging from daily applications such as¬†‚Ä¶",189.0,Survey
GRB,Graph Robustness Benchmark: Benchmarking the Adversarial Robustness of Graph Machine Learning,Adversarial attack and defense on graph data: A survey,"L Sun,Y Dou,C Yang,K Zhang,J Wang‚Ä¶¬†- ‚Ä¶¬†on Knowledge and¬†‚Ä¶, 2022",ieeexplore.ieee.org,,https://ieeexplore.ieee.org/abstract/document/9878092/,,"Deep neural networks (DNNs) have been widely applied to various applications, includingimage classification, text generation, audio recognition, and graph data analysis. However¬†‚Ä¶",467.0,Survey
GRB,Graph Robustness Benchmark: Benchmarking the Adversarial Robustness of Graph Machine Learning,Are defenses for graph neural networks robust?,"F Mujkanovic,S Geisler‚Ä¶¬†- Advances in Neural¬†‚Ä¶, 2022",proceedings.neurips.cc,,https://proceedings.neurips.cc/paper_files/paper/2022/hash/3ac904a31f9141444009777abef2ed8e-Abstract-Conference.html,,"A cursory reading of the literature suggests that we have made a lot of progress in designingeffective adversarial defenses for Graph Neural Networks (GNNs). Yet, the standard¬†‚Ä¶",99.0,Survey
GRB,Graph Robustness Benchmark: Benchmarking the Adversarial Robustness of Graph Machine Learning,"Adversarial training for graph neural networks: Pitfalls, solutions, and new directions","L Gosch,S Geisler, D Sturm‚Ä¶¬†- Advances in neural¬†‚Ä¶, 2023",proceedings.neurips.cc,,https://proceedings.neurips.cc/paper_files/paper/2023/hash/b5a801e6bc4f4ffa3e6786518a324488-Abstract-Conference.html,,"Despite its success in the image domain, adversarial training did not (yet) stand out as aneffective defense for Graph Neural Networks (GNNs) against graph structure perturbations¬†‚Ä¶",55.0,Survey
RealToxicityPrompts,RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models,A survey on evaluation of large language models,"Y Chang,X Wang,J Wang,Y Wu,L Yang‚Ä¶¬†- ACM transactions on¬†‚Ä¶, 2024",dl.acm.org,,https://dl.acm.org/doi/abs/10.1145/3641289,,"Large language models (LLMs) are gaining increasing popularity in both academia andindustry, owing to their unprecedented performance in various applications. As LLMs¬†‚Ä¶",4405.0,Survey
RealToxicityPrompts,RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models,"A survey on large language model (llm) security and privacy: The good, the bad, and the ugly","Y Yao,J Duan,K Xu, Y Cai,Z Sun,Y Zhang- High-Confidence Computing, 2024",Elsevier,,https://www.sciencedirect.com/science/article/pii/S266729522400014X,,"Abstract Large Language Models (LLMs), such as ChatGPT and Bard, have revolutionizednatural language understanding and generation. They possess deep language¬†‚Ä¶",1335.0,Survey
RealToxicityPrompts,RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models,A comprehensive overview of large language models,"H Naveed,AU Khan,S Qiu,M Saqib,S Anwar‚Ä¶¬†- ACM Transactions on¬†‚Ä¶, 2025",dl.acm.org,,https://dl.acm.org/doi/abs/10.1145/3744746,,Large Language Models (LLMs) have recently demonstrated remarkable capabilities innatural language processing tasks and beyond. This success of LLMs has led to a large¬†‚Ä¶,1815.0,Survey
RealToxicityPrompts,RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models,A survey of large language models,"WX Zhao,K Zhou,J Li,T Tang,X Wang‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2023",researchgate.net,,https://www.researchgate.net/profile/Tang-Tianyi-3/publication/369740832_A_Survey_of_Large_Language_Models/links/665fd2e3637e4448a37dd281/A-Survey-of-Large-Language-Models.pdf,,"Ever since the Turing Test was proposed in the 1950s, humans have explored the masteringof language intelligence by machine. Language is essentially a complex, intricate system of¬†‚Ä¶",6302.0,Survey
RealToxicityPrompts,RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models,Holistic evaluation of language models,"P Liang,R Bommasani,T Lee, D Tsipras‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2022",arxiv.org,,https://arxiv.org/abs/2211.09110,,"Language models (LMs) are becoming the foundation for almost all major languagetechnologies, but their capabilities, limitations, and risks are not well understood. We present¬†‚Ä¶",1905.0,Survey
SafetyBench,SafetyBench: Evaluating the Safety of Large Language Models with Multiple Choice Questions,"A comprehensive survey in llm (-agent) full stack safety: Data, training and deployment","K Wang,G Zhang,Z Zhou,J Wu,M Yu,S Zhao‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2025",arxiv.org,,https://arxiv.org/abs/2504.15585,,The remarkable success of Large Language Models (LLMs) has illuminated a promisingpathway toward achieving Artificial General Intelligence for both academic and industrial¬†‚Ä¶,61.0,Survey
SafetyBench,SafetyBench: Evaluating the Safety of Large Language Models with Multiple Choice Questions,Safetyprompts: a systematic review of open datasets for evaluating and improving large language model safety,"P R√∂ttger,F Pernisi,B Vidgen,D Hovy- Proceedings of the AAAI¬†‚Ä¶, 2025",ojs.aaai.org,,https://ojs.aaai.org/index.php/AAAI/article/view/34975,,The last two years have seen a rapid growth in concerns around the safety of largelanguage models (LLMs). Researchers and practitioners have met these concerns by¬†‚Ä¶,52.0,Survey
SafetyBench,SafetyBench: Evaluating the Safety of Large Language Models with Multiple Choice Questions,"On the trustworthiness of generative foundation models: Guideline, assessment, and perspective","Y Huang,C Gao, S Wu,H Wang,X Wang‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2025",arxiv.org,,https://arxiv.org/abs/2502.14296,,"Generative Foundation Models (GenFMs) have emerged as transformative tools. However,their widespread adoption raises critical concerns regarding trustworthiness across¬†‚Ä¶",32.0,Survey
SafetyBench,SafetyBench: Evaluating the Safety of Large Language Models with Multiple Choice Questions,Safety at scale: A comprehensive survey of large model safety,"X Ma, Y Gao,Y Wang,R Wang,X Wang,Y Sun‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2025",arxiv.org,,https://arxiv.org/abs/2502.05206,,"The rapid advancement of large models, driven by their exceptional abilities in learning andgeneralization through large-scale pre-training, has reshaped the landscape of Artificial¬†‚Ä¶",37.0,Survey
SafetyBench,SafetyBench: Evaluating the Safety of Large Language Models with Multiple Choice Questions,Toward generalizable evaluation in the llm era: A survey beyond benchmarks,"Y Cao, S Hong,X Li,J Ying,Y Ma, H Liang‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2025",arxiv.org,,https://arxiv.org/abs/2504.18838,,"Large Language Models (LLMs) are advancing at an amazing speed and have becomeindispensable across academia, industry, and daily applications. To keep pace with the¬†‚Ä¶",19.0,Survey
ToxiGen,ToxiGen: A Large-Scale Machine-Generated Dataset for Adversarial and Implicit Hate Speech Detection,A survey of GPT-3 family large language models including ChatGPT and GPT-4,"KS Kalyan- Natural Language Processing Journal, 2024",Elsevier,,https://www.sciencedirect.com/science/article/pii/S2949719123000456,,"Large language models (LLMs) are a special class of pretrained language models (PLMs)obtained by scaling model size, pretraining corpus and computation. LLMs, because of their¬†‚Ä¶",512.0,Survey
ToxiGen,ToxiGen: A Large-Scale Machine-Generated Dataset for Adversarial and Implicit Hate Speech Detection,Ai alignment: A comprehensive survey,"J Ji,T Qiu,B Chen, B Zhang,H Lou, K Wang‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2023",arxiv.org,,https://arxiv.org/abs/2310.19852,,"AI alignment aims to make AI systems behave in line with human intentions and values. AsAI systems grow more capable, so do risks from misalignment. To provide a comprehensive¬†‚Ä¶",459.0,Survey
BBQ,BBQ: A Hand-Built Bias Benchmark for Question Answering,A survey on evaluation of large language models,"Y Chang,X Wang,J Wang,Y Wu,L Yang‚Ä¶¬†- ACM transactions on¬†‚Ä¶, 2024",dl.acm.org,,https://dl.acm.org/doi/abs/10.1145/3641289,,"Large language models (LLMs) are gaining increasing popularity in both academia andindustry, owing to their unprecedented performance in various applications. As LLMs¬†‚Ä¶",4405.0,Survey
BBQ,BBQ: A Hand-Built Bias Benchmark for Question Answering,A comprehensive overview of large language models,"H Naveed,AU Khan,S Qiu,M Saqib,S Anwar‚Ä¶¬†- ACM Transactions on¬†‚Ä¶, 2025",dl.acm.org,,https://dl.acm.org/doi/abs/10.1145/3744746,,Large Language Models (LLMs) have recently demonstrated remarkable capabilities innatural language processing tasks and beyond. This success of LLMs has led to a large¬†‚Ä¶,1815.0,Survey
BBQ,BBQ: A Hand-Built Bias Benchmark for Question Answering,Holistic evaluation of language models,"P Liang,R Bommasani,T Lee, D Tsipras‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2022",arxiv.org,,https://arxiv.org/abs/2211.09110,,"Language models (LMs) are becoming the foundation for almost all major languagetechnologies, but their capabilities, limitations, and risks are not well understood. We present¬†‚Ä¶",1905.0,Survey
BOLD,BOLD: Dataset and Metrics for Measuring Biases in Open-Ended Language Generation,A survey on evaluation of large language models,"Y Chang,X Wang,J Wang,Y Wu,L Yang‚Ä¶¬†- ACM transactions on¬†‚Ä¶, 2024",dl.acm.org,,https://dl.acm.org/doi/abs/10.1145/3641289,,"Large language models (LLMs) are gaining increasing popularity in both academia andindustry, owing to their unprecedented performance in various applications. As LLMs¬†‚Ä¶",4405.0,Survey
BOLD,BOLD: Dataset and Metrics for Measuring Biases in Open-Ended Language Generation,A comprehensive survey of ai-generated content (aigc): A history of generative ai from gan to chatgpt,"Y Cao,S Li,Y Liu,Z Yan,Y Dai,PS Yu‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2023",arxiv.org,,https://arxiv.org/abs/2303.04226,,"Recently, ChatGPT, along with DALL-E-2 and Codex, has been gaining significant attentionfrom society. As a result, many individuals have become interested in related resources and¬†‚Ä¶",1247.0,Survey
BOLD,BOLD: Dataset and Metrics for Measuring Biases in Open-Ended Language Generation,Holistic evaluation of language models,"P Liang,R Bommasani,T Lee, D Tsipras‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2022",arxiv.org,,https://arxiv.org/abs/2211.09110,,"Language models (LMs) are becoming the foundation for almost all major languagetechnologies, but their capabilities, limitations, and risks are not well understood. We present¬†‚Ä¶",1905.0,Survey
BOLD,BOLD: Dataset and Metrics for Measuring Biases in Open-Ended Language Generation,DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models.,"B Wang,W Chen,H Pei,C Xie,M Kang,C Zhang,C Xu‚Ä¶¬†- NeurIPS, 2023",blogs.qub.ac.uk,,https://blogs.qub.ac.uk/wp-content/uploads/sites/7/2024/01/A-comprehensive-Assessment-of-Trustworthiness-in-GPT-Models.pdf,,"Abstract Generative Pre-trained Transformer (GPT) models have exhibited exciting progressin their capabilities, capturing the interest of practitioners and the public alike. Yet, while the¬†‚Ä¶",625.0,Survey
JailbreakBench,JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models,"A comprehensive survey of small language models in the era of large language models: Techniques, enhancements, applications, collaboration with llms, and¬†‚Ä¶","F Wang,Z Zhang,X Zhang,Z Wu, T Mo,Q Lu‚Ä¶¬†- ACM Transactions on¬†‚Ä¶, 2024",dl.acm.org,,https://dl.acm.org/doi/abs/10.1145/3768165,,"Large language models (LLMs) have demonstrated emergent abilities in text generation,question answering, and reasoning, facilitating various tasks and domains. Despite their¬†‚Ä¶",111.0,Survey
JailbreakBench,JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models,"A comprehensive survey in llm (-agent) full stack safety: Data, training and deployment","K Wang,G Zhang,Z Zhou,J Wu,M Yu,S Zhao‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2025",arxiv.org,,https://arxiv.org/abs/2504.15585,,The remarkable success of Large Language Models (LLMs) has illuminated a promisingpathway toward achieving Artificial General Intelligence for both academic and industrial¬†‚Ä¶,61.0,Survey
JailbreakBench,JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models,Jailbreak attacks and defenses against large language models: A survey,"S Yi,Y Liu,Z Sun,T Cong,X He, J Song,K Xu‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2024",arxiv.org,,https://arxiv.org/abs/2407.04295,,"Large Language Models (LLMs) have performed exceptionally in various text-generativetasks, including question answering, translation, code completion, etc. However, the over¬†‚Ä¶",200.0,Survey
HarmBench,HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal,"A comprehensive survey in llm (-agent) full stack safety: Data, training and deployment","K Wang,G Zhang,Z Zhou,J Wu,M Yu,S Zhao‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2025",arxiv.org,,https://arxiv.org/abs/2504.15585,,The remarkable success of Large Language Models (LLMs) has illuminated a promisingpathway toward achieving Artificial General Intelligence for both academic and industrial¬†‚Ä¶,61.0,Survey
HarmBench,HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal,Adversarial attacks of vision tasks in the past 10 years: A survey,"C Zhang, L Zhou,X Xu, J Wu,Z Liu- ACM Computing Surveys, 2025",dl.acm.org,,https://dl.acm.org/doi/abs/10.1145/3743126,,"With the advent of Large Vision-Language Models (LVLMs), new attack vectors, such ascognitive bias, prompt injection, and jailbreaking, have emerged. Understanding these¬†‚Ä¶",17.0,Survey
DecodingTrust,DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models,A survey on evaluation of large language models,"Y Chang,X Wang,J Wang,Y Wu,L Yang‚Ä¶¬†- ACM transactions on¬†‚Ä¶, 2024",dl.acm.org,,https://dl.acm.org/doi/abs/10.1145/3641289,,"Large language models (LLMs) are gaining increasing popularity in both academia andindustry, owing to their unprecedented performance in various applications. As LLMs¬†‚Ä¶",4405.0,Survey
DecodingTrust,DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models,Mm-llms: Recent advances in multimodal large language models,"D Zhang,Y Yu,J Dong,C Li,D Su,C Chu‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2024",arxiv.org,,https://arxiv.org/abs/2401.13601,,"In the past year, MultiModal Large Language Models (MM-LLMs) have undergonesubstantial advancements, augmenting off-the-shelf LLMs to support MM inputs or outputs¬†‚Ä¶",493.0,Survey
DecodingTrust,DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models,Trustworthy llms: a survey and guideline for evaluating large language models' alignment,"Y Liu,Y Yao,JF Ton,X Zhang,R Guo,H Cheng‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2023",arxiv.org,,https://arxiv.org/abs/2308.05374,,"Ensuring alignment, which refers to making models behave in accordance with humanintentions [1, 2], has become a critical task before deploying large language models (LLMs)¬†‚Ä¶",516.0,Survey
AdvBench,Universal and Transferable Adversarial Attacks on Aligned Language Models,"A survey on large language model (llm) security and privacy: The good, the bad, and the ugly","Y Yao,J Duan,K Xu, Y Cai,Z Sun,Y Zhang- High-Confidence Computing, 2024",Elsevier,,https://www.sciencedirect.com/science/article/pii/S266729522400014X,,"Abstract Large Language Models (LLMs), such as ChatGPT and Bard, have revolutionizednatural language understanding and generation. They possess deep language¬†‚Ä¶",1335.0,Survey
AdvBench,Universal and Transferable Adversarial Attacks on Aligned Language Models,The rise and potential of large language model based agents: A survey,"Z Xi,W Chen,X Guo,W He,Y Ding, B Hong‚Ä¶¬†- Science China¬†‚Ä¶, 2025",Springer,,https://link.springer.com/article/10.1007/s11432-024-4222-0,,"For a long time, researchers have sought artificial intelligence (AI) that matches or exceedshuman intelligence. AI agents, which are artificial entities capable of sensing the¬†‚Ä¶",1558.0,Survey
AdvBench,Universal and Transferable Adversarial Attacks on Aligned Language Models,üßú Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models,"Y Zhang,Y Li,L Cui,D Cai,L Liu,T Fu‚Ä¶¬†- Computational¬†‚Ä¶, 2025",direct.mit.edu,,https://direct.mit.edu/coli/article/doi/10.1162/coli.a.16/131631,,"While large language models (LLMs) have demonstrated remarkable capabilities across arange of downstream tasks, a significant concern revolves around their propensity to exhibit¬†‚Ä¶",1549.0,Survey
HarmfulQA,Red-Teaming Large Language Models using Chain of Utterances for Safety-Alignment,A survey of GPT-3 family large language models including ChatGPT and GPT-4,"KS Kalyan- Natural Language Processing Journal, 2024",Elsevier,,https://www.sciencedirect.com/science/article/pii/S2949719123000456,,"Large language models (LLMs) are a special class of pretrained language models (PLMs)obtained by scaling model size, pretraining corpus and computation. LLMs, because of their¬†‚Ä¶",512.0,Survey
HarmfulQA,Red-Teaming Large Language Models using Chain of Utterances for Safety-Alignment,Survey of vulnerabilities in large language models revealed by adversarial attacks,"E Shayegani,MAA Mamun,Y Fu,P Zaree‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2023",arxiv.org,,https://arxiv.org/abs/2310.10844,,"Large Language Models (LLMs) are swiftly advancing in architecture and capability, and asthey integrate more deeply into complex systems, the urgency to scrutinize their security¬†‚Ä¶",234.0,Survey
HarmfulQA,Red-Teaming Large Language Models using Chain of Utterances for Safety-Alignment,Jailbreak attacks and defenses against large language models: A survey,"S Yi,Y Liu,Z Sun,T Cong,X He, J Song,K Xu‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2024",arxiv.org,,https://arxiv.org/abs/2407.04295,,"Large Language Models (LLMs) have performed exceptionally in various text-generativetasks, including question answering, translation, code completion, etc. However, the over¬†‚Ä¶",200.0,Survey
DoNotAnswer,Do-Not-Answer: A Dataset for Evaluating Safeguards in LLMs,A survey of confidence estimation and calibration in large language models,"J Geng,F Cai,Y Wang,H Koeppl,P Nakov‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2023",arxiv.org,,https://arxiv.org/abs/2311.08298,,"Large language models (LLMs) have demonstrated remarkable capabilities across a widerange of tasks in various domains. Despite their impressive performance, they can be¬†‚Ä¶",149.0,Survey
DoNotAnswer,Do-Not-Answer: A Dataset for Evaluating Safeguards in LLMs,Jailbreak attacks and defenses against large language models: A survey,"S Yi,Y Liu,Z Sun,T Cong,X He, J Song,K Xu‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2024",arxiv.org,,https://arxiv.org/abs/2407.04295,,"Large Language Models (LLMs) have performed exceptionally in various text-generativetasks, including question answering, translation, code completion, etc. However, the over¬†‚Ä¶",200.0,Survey
SALAD-Bench,SALAD-Bench: A Hierarchical and Comprehensive Safety Benchmark for Large Language Models,"A comprehensive survey in llm (-agent) full stack safety: Data, training and deployment","K Wang,G Zhang,Z Zhou,J Wu,M Yu,S Zhao‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2025",arxiv.org,,https://arxiv.org/abs/2504.15585,,The remarkable success of Large Language Models (LLMs) has illuminated a promisingpathway toward achieving Artificial General Intelligence for both academic and industrial¬†‚Ä¶,61.0,Survey
SALAD-Bench,SALAD-Bench: A Hierarchical and Comprehensive Safety Benchmark for Large Language Models,Adversarial attacks of vision tasks in the past 10 years: A survey,"C Zhang, L Zhou,X Xu, J Wu,Z Liu- ACM Computing Surveys, 2025",dl.acm.org,,https://dl.acm.org/doi/abs/10.1145/3743126,,"With the advent of Large Vision-Language Models (LVLMs), new attack vectors, such ascognitive bias, prompt injection, and jailbreaking, have emerged. Understanding these¬†‚Ä¶",17.0,Survey
SALAD-Bench,SALAD-Bench: A Hierarchical and Comprehensive Safety Benchmark for Large Language Models,A survey on llm-as-a-judge,"J Gu,X Jiang,Z Shi,H Tan,X Zhai,C Xu,W Li‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2024",arxiv.org,,https://arxiv.org/abs/2411.15594,,"Accurate and consistent evaluation is crucial for decision-making across numerous fields,yet it remains a challenging task due to inherent subjectivity, variability, and scale. Large¬†‚Ä¶",714.0,Survey
MACHIAVELLI,Do the Rewards Justify the Means? Measuring Trade-Offs Between Rewards and Ethical Behavior in the MACHIAVELLI Benchmark,Ai alignment: A comprehensive survey,"J Ji,T Qiu,B Chen, B Zhang,H Lou, K Wang‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2023",arxiv.org,,https://arxiv.org/abs/2310.19852,,"AI alignment aims to make AI systems behave in line with human intentions and values. AsAI systems grow more capable, so do risks from misalignment. To provide a comprehensive¬†‚Ä¶",459.0,Survey
MACHIAVELLI,Do the Rewards Justify the Means? Measuring Trade-Offs Between Rewards and Ethical Behavior in the MACHIAVELLI Benchmark,"AI deception: A survey of examples, risks, and potential solutions","PS Park,S Goldstein,A O'Gara,M Chen,D Hendrycks- Patterns, 2024",cell.com,,https://www.cell.com/patterns/fulltext/S2666-3899(24)00103-X?ref=aiexec.whitegloveai.com,,This paper argues that a range of current AI systems have learned how to deceive humans.We define deception as the systematic inducement of false beliefs in the pursuit of some¬†‚Ä¶,341.0,Survey
MACHIAVELLI,Do the Rewards Justify the Means? Measuring Trade-Offs Between Rewards and Ethical Behavior in the MACHIAVELLI Benchmark,DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models.,"B Wang,W Chen,H Pei,C Xie,M Kang,C Zhang,C Xu‚Ä¶¬†- NeurIPS, 2023",blogs.qub.ac.uk,,https://blogs.qub.ac.uk/wp-content/uploads/sites/7/2024/01/A-comprehensive-Assessment-of-Trustworthiness-in-GPT-Models.pdf,,"Abstract Generative Pre-trained Transformer (GPT) models have exhibited exciting progressin their capabilities, capturing the interest of practitioners and the public alike. Yet, while the¬†‚Ä¶",625.0,Survey
MACHIAVELLI,Do the Rewards Justify the Means? Measuring Trade-Offs Between Rewards and Ethical Behavior in the MACHIAVELLI Benchmark,Representation engineering: A top-down approach to ai transparency,"A Zou,L Phan,S Chen,J Campbell,P Guo‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2023",arxiv.org,,https://arxiv.org/abs/2310.01405,,"In this paper, we identify and characterize the emerging area of representation engineering(RepE), an approach to enhancing the transparency of AI systems that draws on insights¬†‚Ä¶",567.0,Survey
MACHIAVELLI,Do the Rewards Justify the Means? Measuring Trade-Offs Between Rewards and Ethical Behavior in the MACHIAVELLI Benchmark,Foundational challenges in assuring alignment and safety of large language models,"U Anwar,A Saparov,J Rando,D Paleka‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2024",arxiv.org,,https://arxiv.org/abs/2404.09932,,This work identifies 18 foundational challenges in assuring the alignment and safety of largelanguage models (LLMs). These challenges are organized into three different categories¬†‚Ä¶,254.0,Survey
MACHIAVELLI,Do the Rewards Justify the Means? Measuring Trade-Offs Between Rewards and Ethical Behavior in the MACHIAVELLI Benchmark,An overview of catastrophic AI risks,"D Hendrycks,M Mazeika,T Woodside- arXiv preprint arXiv:2306.12001, 2023",arxiv.org,,https://arxiv.org/abs/2306.12001,,"Rapid advancements in artificial intelligence (AI) have sparked growing concerns amongexperts, policymakers, and world leaders regarding the potential for increasingly advanced¬†‚Ä¶",316.0,Survey
MACHIAVELLI,Do the Rewards Justify the Means? Measuring Trade-Offs Between Rewards and Ethical Behavior in the MACHIAVELLI Benchmark,Review of large vision models and visual prompt engineering,"J Wang,Z Liu,L Zhao,Z Wu,C Ma, S Yu,H Dai‚Ä¶¬†- Meta-Radiology, 2023",Elsevier,,https://www.sciencedirect.com/science/article/pii/S2950162823000474,,"Visual prompt engineering is a fundamental methodology in the field of visual and imageartificial general intelligence. As the development of large vision models progresses, the¬†‚Ä¶",244.0,Survey
OpenDeception,OpenDeception: Benchmarking and Investigating AI Deceptive Behaviors via Open-ended Interaction Simulation,"The hidden complexities of android TPL detection: An empirical analysis of techniques, challenges, and effectiveness","L Zhan,J Ming, J Fu, G Peng, L Sha, L Lan¬†- Computers & Security, 2025",Elsevier,,https://www.sciencedirect.com/science/article/pii/S016740482500361X,,"Third-party libraries (TPLs) play a crucial role in Android application (app) development andhave become an indispensable part of the Android ecosystem. However, TPLs also¬†‚Ä¶",,Survey
Goal Misgeneralization (Procgen),Goal Misgeneralization in Deep Reinforcement Learning,Ai alignment: A comprehensive survey,"J Ji,T Qiu,B Chen, B Zhang,H Lou, K Wang‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2023",arxiv.org,,https://arxiv.org/abs/2310.19852,,"AI alignment aims to make AI systems behave in line with human intentions and values. AsAI systems grow more capable, so do risks from misalignment. To provide a comprehensive¬†‚Ä¶",459.0,Survey
Goal Misgeneralization (Procgen),Goal Misgeneralization in Deep Reinforcement Learning,"AI deception: A survey of examples, risks, and potential solutions","PS Park,S Goldstein,A O'Gara,M Chen,D Hendrycks- Patterns, 2024",cell.com,,https://www.cell.com/patterns/fulltext/S2666-3899(24)00103-X?ref=aiexec.whitegloveai.com,,This paper argues that a range of current AI systems have learned how to deceive humans.We define deception as the systematic inducement of false beliefs in the pursuit of some¬†‚Ä¶,341.0,Survey
Goal Misgeneralization (Procgen),Goal Misgeneralization in Deep Reinforcement Learning,Open problems and fundamental limitations of reinforcement learning from human feedback,"S Casper,X Davies,C Shi,TK Gilbert‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2023",arxiv.org,,https://arxiv.org/abs/2307.15217,,Reinforcement learning from human feedback (RLHF) is a technique for training AI systemsto align with human goals. RLHF has emerged as the central method used to finetune state¬†‚Ä¶,777.0,Survey
Goal Misgeneralization (Procgen),Goal Misgeneralization in Deep Reinforcement Learning,Foundational challenges in assuring alignment and safety of large language models,"U Anwar,A Saparov,J Rando,D Paleka‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2024",arxiv.org,,https://arxiv.org/abs/2404.09932,,This work identifies 18 foundational challenges in assuring the alignment and safety of largelanguage models (LLMs). These challenges are organized into three different categories¬†‚Ä¶,254.0,Survey
Goal Misgeneralization (Procgen),Goal Misgeneralization in Deep Reinforcement Learning,Towards reasoning era: A survey of long chain-of-thought for reasoning large language models,"Q Chen,L Qin, J Liu,D Peng, J Guan,P Wang‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2025",arxiv.org,,https://arxiv.org/abs/2503.09567,,"Recent advancements in reasoning with large language models (RLLMs), such as OpenAI-O1 and DeepSeek-R1, have demonstrated their impressive capabilities in complex domains¬†‚Ä¶",228.0,Survey
Goal Misgeneralization (Procgen),Goal Misgeneralization in Deep Reinforcement Learning,"Survey on large language model-enhanced reinforcement learning: Concept, taxonomy, and methods","Y Cao,H Zhao,Y Cheng,T Shu,Y Chen‚Ä¶¬†- ‚Ä¶¬†on Neural Networks¬†‚Ä¶, 2024",ieeexplore.ieee.org,,https://ieeexplore.ieee.org/abstract/document/10766898/,,"With extensive pretrained knowledge and high-level general capabilities, large languagemodels (LLMs) emerge as a promising avenue to augment reinforcement learning (RL) in¬†‚Ä¶",175.0,Survey
Goal Misgeneralization (Procgen),Goal Misgeneralization in Deep Reinforcement Learning,Eight things to know about large language models,"SR Bowman- Critical AI, 2024",read.dukeupress.edu,,https://read.dukeupress.edu/critical-ai/article-abstract/doi/10.1215/2834703X-11556011/400182,,"The widespread public deployment of large language models (LLMs) in recent months hasprompted a wave of new attention and engagement from advocates, policymakers, and¬†‚Ä¶",243.0,Survey
Goal Misgeneralization (Analysis),Goal Misgeneralization: Why Correct Specifications Aren't Enough For Correct Goals,"AI deception: A survey of examples, risks, and potential solutions","PS Park,S Goldstein,A O'Gara,M Chen,D Hendrycks- Patterns, 2024",cell.com,,https://www.cell.com/patterns/fulltext/S2666-3899(24)00103-X?ref=aiexec.whitegloveai.com,,This paper argues that a range of current AI systems have learned how to deceive humans.We define deception as the systematic inducement of false beliefs in the pursuit of some¬†‚Ä¶,341.0,Survey
Goal Misgeneralization (Analysis),Goal Misgeneralization: Why Correct Specifications Aren't Enough For Correct Goals,Open problems and fundamental limitations of reinforcement learning from human feedback,"S Casper,X Davies,C Shi,TK Gilbert‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2023",arxiv.org,,https://arxiv.org/abs/2307.15217,,Reinforcement learning from human feedback (RLHF) is a technique for training AI systemsto align with human goals. RLHF has emerged as the central method used to finetune state¬†‚Ä¶,777.0,Survey
Goal Misgeneralization (Analysis),Goal Misgeneralization: Why Correct Specifications Aren't Enough For Correct Goals,Large language model alignment: A survey,"T Shen,R Jin,Y Huang,C Liu,W Dong, Z Guo‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2023",arxiv.org,,https://arxiv.org/abs/2309.15025,,"Recent years have witnessed remarkable progress made in large language models (LLMs).Such advancements, while garnering significant attention, have concurrently elicited various¬†‚Ä¶",289.0,Survey
Goal Misgeneralization (Analysis),Goal Misgeneralization: Why Correct Specifications Aren't Enough For Correct Goals,Managing ai risks in an era of rapid progress,"Y Bengio,G Hinton,A Yao,D Song‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2023",blog.biocomm.ai,,https://blog.biocomm.ai/wp-content/uploads/2023/11/Managing-AI-Risks-in-an-Era-of-Rapid-Progress.pdf,,"In this short consensus paper, we outline risks from upcoming, advanced AI systems. Weexamine large-scale social harms and malicious uses, as well as an irreversible loss of¬†‚Ä¶",121.0,Survey
Goal Misgeneralization (Analysis),Goal Misgeneralization: Why Correct Specifications Aren't Enough For Correct Goals,Harms from increasingly agentic algorithmic systems,"A Chan, R Salganik,A Markelius, C Pang‚Ä¶¬†- Proceedings of the¬†‚Ä¶, 2023",dl.acm.org,,https://dl.acm.org/doi/abs/10.1145/3593013.3594033,,"Research in Fairness, Accountability, Transparency, and Ethics (FATE) 1 has establishedmany sources and forms of algorithmic harm, in domains as diverse as health care, finance¬†‚Ä¶",181.0,Survey
Goal Misgeneralization (Analysis),Goal Misgeneralization: Why Correct Specifications Aren't Enough For Correct Goals,Characterizing manipulation from AI systems,"M Carroll,A Chan, H Ashton,D Krueger- ‚Ä¶¬†of the 3rd ACM Conference on¬†‚Ä¶, 2023",dl.acm.org,,https://dl.acm.org/doi/abs/10.1145/3617694.3623226,,"Manipulation is a concern in many domains, such as social media, advertising, andchatbots. As AI systems mediate more of our digital interactions, it is important to understand¬†‚Ä¶",124.0,Survey
IPS Index,Instrumental Power-Seeking Index: A Novel Benchmark for Detecting Power-Seeking Behavior in LLMs,Ai alignment: A comprehensive survey,"J Ji,T Qiu,B Chen, B Zhang,H Lou, K Wang‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2023",arxiv.org,,https://arxiv.org/abs/2310.19852,,"AI alignment aims to make AI systems behave in line with human intentions and values. AsAI systems grow more capable, so do risks from misalignment. To provide a comprehensive¬†‚Ä¶",459.0,Survey
IPS Index,Instrumental Power-Seeking Index: A Novel Benchmark for Detecting Power-Seeking Behavior in LLMs,"AI deception: A survey of examples, risks, and potential solutions","PS Park,S Goldstein,A O'Gara,M Chen,D Hendrycks- Patterns, 2024",cell.com,,https://www.cell.com/patterns/fulltext/S2666-3899(24)00103-X?ref=aiexec.whitegloveai.com,,This paper argues that a range of current AI systems have learned how to deceive humans.We define deception as the systematic inducement of false beliefs in the pursuit of some¬†‚Ä¶,341.0,Survey
IPS Index,Instrumental Power-Seeking Index: A Novel Benchmark for Detecting Power-Seeking Behavior in LLMs,DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models.,"B Wang,W Chen,H Pei,C Xie,M Kang,C Zhang,C Xu‚Ä¶¬†- NeurIPS, 2023",blogs.qub.ac.uk,,https://blogs.qub.ac.uk/wp-content/uploads/sites/7/2024/01/A-comprehensive-Assessment-of-Trustworthiness-in-GPT-Models.pdf,,"Abstract Generative Pre-trained Transformer (GPT) models have exhibited exciting progressin their capabilities, capturing the interest of practitioners and the public alike. Yet, while the¬†‚Ä¶",625.0,Survey
IPS Index,Instrumental Power-Seeking Index: A Novel Benchmark for Detecting Power-Seeking Behavior in LLMs,Representation engineering: A top-down approach to ai transparency,"A Zou,L Phan,S Chen,J Campbell,P Guo‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2023",arxiv.org,,https://arxiv.org/abs/2310.01405,,"In this paper, we identify and characterize the emerging area of representation engineering(RepE), an approach to enhancing the transparency of AI systems that draws on insights¬†‚Ä¶",567.0,Survey
IPS Index,Instrumental Power-Seeking Index: A Novel Benchmark for Detecting Power-Seeking Behavior in LLMs,Foundational challenges in assuring alignment and safety of large language models,"U Anwar,A Saparov,J Rando,D Paleka‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2024",arxiv.org,,https://arxiv.org/abs/2404.09932,,This work identifies 18 foundational challenges in assuring the alignment and safety of largelanguage models (LLMs). These challenges are organized into three different categories¬†‚Ä¶,254.0,Survey
IPS Index,Instrumental Power-Seeking Index: A Novel Benchmark for Detecting Power-Seeking Behavior in LLMs,An overview of catastrophic AI risks,"D Hendrycks,M Mazeika,T Woodside- arXiv preprint arXiv:2306.12001, 2023",arxiv.org,,https://arxiv.org/abs/2306.12001,,"Rapid advancements in artificial intelligence (AI) have sparked growing concerns amongexperts, policymakers, and world leaders regarding the potential for increasingly advanced¬†‚Ä¶",316.0,Survey
IPS Index,Instrumental Power-Seeking Index: A Novel Benchmark for Detecting Power-Seeking Behavior in LLMs,Review of large vision models and visual prompt engineering,"J Wang,Z Liu,L Zhao,Z Wu,C Ma, S Yu,H Dai‚Ä¶¬†- Meta-Radiology, 2023",Elsevier,,https://www.sciencedirect.com/science/article/pii/S2950162823000474,,"Visual prompt engineering is a fundamental methodology in the field of visual and imageartificial general intelligence. As the development of large vision models progresses, the¬†‚Ä¶",244.0,Survey
SycEval,SycEval: Evaluating LLM Sycophancy,Explainability of Large Language Models: Opportunities and Challenges toward Generating Trustworthy Explanations,"S Atakishiyev,HKB Babiker, J Dai,N Farruque‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2025",arxiv.org,,https://arxiv.org/abs/2510.17256,,"Large language models have exhibited impressive performance across a broad range ofdownstream tasks in natural language processing. However, how a language model¬†‚Ä¶",,Survey
HELM Safety,HELM Safety: Towards Standardized Safety Evaluations of Language Models,Safetyprompts: a systematic review of open datasets for evaluating and improving large language model safety,"P R√∂ttger,F Pernisi,B Vidgen,D Hovy- Proceedings of the AAAI¬†‚Ä¶, 2025",ojs.aaai.org,,https://ojs.aaai.org/index.php/AAAI/article/view/34975,,The last two years have seen a rapid growth in concerns around the safety of largelanguage models (LLMs). Researchers and practitioners have met these concerns by¬†‚Ä¶,52.0,Survey
AIR-Bench 2024,AIR-Bench 2024: A Safety Benchmark Based on Risk Categories from Regulations and Policies,A Comprehensive Survey on Trustworthiness in Reasoning with Large Language Models,"Y Wang,Y Yu,J Liang, R He¬†- arXiv preprint arXiv:2509.03871, 2025",arxiv.org,,https://arxiv.org/abs/2509.03871,,"The development of Long-CoT reasoning has advanced LLM performance across varioustasks, including language understanding, complex problem solving, and code generation¬†‚Ä¶",,Survey
TrustLLM,TrustLLM: Trustworthiness in Large Language Models,Security and privacy challenges of large language models: A survey,"BC Das,MH Amini,Y Wu- ACM Computing Surveys, 2025",dl.acm.org,,https://dl.acm.org/doi/abs/10.1145/3712001,,"Large language models (LLMs) have demonstrated extraordinary capabilities andcontributed to multiple fields, such as generating and summarizing text, language¬†‚Ä¶",388.0,Survey
TrustLLM,TrustLLM: Trustworthiness in Large Language Models,Large language models: A survey,"S Minaee,T Mikolov,N Nikzad,M Chenaghlu‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2024",arxiv.org,,https://arxiv.org/abs/2402.06196,,"Large Language Models (LLMs) have drawn a lot of attention due to their strongperformance on a wide range of natural language tasks, since the release of ChatGPT in¬†‚Ä¶",1504.0,Survey
TrustLLM,TrustLLM: Trustworthiness in Large Language Models,"Large language models: a comprehensive survey of its applications, challenges, limitations, and future prospects","MU Hadi,R Qureshi,A Shah,M Irfan,A Zafar‚Ä¶¬†- Authorea¬†‚Ä¶, 2023",researchgate.net,,https://www.researchgate.net/profile/Muhammad-Shaikh-9/publication/383818024_Large_Language_Models_A_Comprehensive_Survey_of_its_Applications_Challenges_Limitations_and_Future_Prospects/links/66dffb06b1606e24c21d8936/Large-Language-Models-A-Comprehensive-Survey-of-its-Applications-Challenges-Limitations-and-Future-Prospects.pdf,,"Within the vast expanse of computerized language processing, a revolutionary entity knownas Large Language Models (LLMs) has emerged, wielding immense power in its capacity to¬†‚Ä¶",601.0,Survey
TrustLLM,TrustLLM: Trustworthiness in Large Language Models,"A comprehensive survey of small language models in the era of large language models: Techniques, enhancements, applications, collaboration with llms, and¬†‚Ä¶","F Wang,Z Zhang,X Zhang,Z Wu, T Mo,Q Lu‚Ä¶¬†- ACM Transactions on¬†‚Ä¶, 2024",dl.acm.org,,https://dl.acm.org/doi/abs/10.1145/3768165,,"Large language models (LLMs) have demonstrated emergent abilities in text generation,question answering, and reasoning, facilitating various tasks and domains. Despite their¬†‚Ä¶",111.0,Survey
TrustLLM,TrustLLM: Trustworthiness in Large Language Models,Factuality challenges in the era of large language models and opportunities for fact-checking,"I Augenstein,T Baldwin,M Cha‚Ä¶¬†- Nature Machine¬†‚Ä¶, 2024",nature.com,,https://www.nature.com/articles/s42256-024-00881-z,,"The emergence of tools based on large language models (LLMs), such as OpenAI'sChatGPT and Google's Gemini, has garnered immense public attention owing to their¬†‚Ä¶",225.0,Survey
MLCommons AILuminate,AILuminate: Introducing v1.0 of the AI Risk and Reliability Benchmark from MLCommons,"Who is responsible? the data, models, users or regulations? a comprehensive survey on responsible generative ai for a sustainable future","S Raza,R Qureshi, A Zahid, S Kamawal‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2025",arxiv.org,,https://arxiv.org/abs/2502.08650,,"Generative AI is moving rapidly from research into real world deployment across sectors,which elevates the need for responsible development, deployment, evaluation, and¬†‚Ä¶",13.0,Survey
ALERT,ALERT: A Comprehensive Benchmark for Assessing Large Language Models' Safety through Red Teaming,Safetyprompts: a systematic review of open datasets for evaluating and improving large language model safety,"P R√∂ttger,F Pernisi,B Vidgen,D Hovy- Proceedings of the AAAI¬†‚Ä¶, 2025",ojs.aaai.org,,https://ojs.aaai.org/index.php/AAAI/article/view/34975,,The last two years have seen a rapid growth in concerns around the safety of largelanguage models (LLMs). Researchers and practitioners have met these concerns by¬†‚Ä¶,52.0,Survey
ALERT,ALERT: A Comprehensive Benchmark for Assessing Large Language Models' Safety through Red Teaming,"Who is responsible? the data, models, users or regulations? a comprehensive survey on responsible generative ai for a sustainable future","S Raza,R Qureshi, A Zahid, S Kamawal‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2025",arxiv.org,,https://arxiv.org/abs/2502.08650,,"Generative AI is moving rapidly from research into real world deployment across sectors,which elevates the need for responsible development, deployment, evaluation, and¬†‚Ä¶",13.0,Survey
ALERT,ALERT: A Comprehensive Benchmark for Assessing Large Language Models' Safety through Red Teaming,"On the trustworthiness of generative foundation models: Guideline, assessment, and perspective","Y Huang,C Gao, S Wu,H Wang,X Wang‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2025",arxiv.org,,https://arxiv.org/abs/2502.14296,,"Generative Foundation Models (GenFMs) have emerged as transformative tools. However,their widespread adoption raises critical concerns regarding trustworthiness across¬†‚Ä¶",32.0,Survey
ALERT,ALERT: A Comprehensive Benchmark for Assessing Large Language Models' Safety through Red Teaming,Blockchain for large language model security and safety: A holistic survey,"C Geren,A Board,GG Dagher,T Andersen‚Ä¶¬†- ACM SIGKDD¬†‚Ä¶, 2025",dl.acm.org,,https://dl.acm.org/doi/abs/10.1145/3715073.3715075,,"With the growing development and deployment of large language models (LLMs) in bothindustrial and academic fields, their security and safety concerns have become increasingly¬†‚Ä¶",21.0,Survey
AgentHarm,AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents,"A comprehensive survey in llm (-agent) full stack safety: Data, training and deployment","K Wang,G Zhang,Z Zhou,J Wu,M Yu,S Zhao‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2025",arxiv.org,,https://arxiv.org/abs/2504.15585,,The remarkable success of Large Language Models (LLMs) has illuminated a promisingpathway toward achieving Artificial General Intelligence for both academic and industrial¬†‚Ä¶,61.0,Survey
AgentHarm,AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents,A survey on llm-as-a-judge,"J Gu,X Jiang,Z Shi,H Tan,X Zhai,C Xu,W Li‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2024",arxiv.org,,https://arxiv.org/abs/2411.15594,,"Accurate and consistent evaluation is crucial for decision-making across numerous fields,yet it remains a challenging task due to inherent subjectivity, variability, and scale. Large¬†‚Ä¶",714.0,Survey
AgentHarm,AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents,"Large language model agent: A survey on methodology, applications and challenges","J Luo,W Zhang,Y Yuan,Y Zhao,J Yang,Y Gu‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2025",arxiv.org,,https://arxiv.org/abs/2503.21460,,"The era of intelligent agents is upon us, driven by revolutionary advancements in largelanguage models. Large Language Model (LLM) agents, with goal-driven behaviors and¬†‚Ä¶",43.0,Survey
AgentHarm,AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents,A survey on trustworthy llm agents: Threats and countermeasures,"M Yu, F Meng,X Zhou,S Wang,J Mao, L Pan‚Ä¶¬†- Proceedings of the 31st¬†‚Ä¶, 2025",dl.acm.org,,https://dl.acm.org/doi/abs/10.1145/3711896.3736561,,"With the rapid evolution of Large Language Models (LLMs), LLM-based agents and Multi-agent Systems (MAS) have significantly expanded the capabilities of LLM ecosystems. This¬†‚Ä¶",43.0,Survey
AgentHarm,AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents,A survey of self-evolving agents: On path to artificial super intelligence,"H Gao,J Geng,W Hua,M Hu,X Juan,H Liu‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2025",arxiv.org,,https://arxiv.org/abs/2507.21046,,"Large Language Models (LLMs) have demonstrated strong capabilities but remainfundamentally static, unable to adapt their internal parameters to novel tasks, evolving¬†‚Ä¶",33.0,Survey
AgentHarm,AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents,"On the trustworthiness of generative foundation models: Guideline, assessment, and perspective","Y Huang,C Gao, S Wu,H Wang,X Wang‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2025",arxiv.org,,https://arxiv.org/abs/2502.14296,,"Generative Foundation Models (GenFMs) have emerged as transformative tools. However,their widespread adoption raises critical concerns regarding trustworthiness across¬†‚Ä¶",32.0,Survey
AgentHarm,AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents,A comprehensive survey of self-evolving ai agents: A new paradigm bridging foundation models and lifelong agentic systems,"J Fang,Y Peng,X Zhang,Y Wang, X Yi‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2025",arxiv.org,,https://arxiv.org/abs/2508.07407,,"Recent advances in large language models have sparked growing interest in AI agentscapable of solving complex, real-world tasks. However, most existing agent systems rely on¬†‚Ä¶",18.0,Survey
AgentHarm,AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents,From llm reasoning to autonomous ai agents: A comprehensive review,"MA Ferrag,N Tihanyi,M Debbah- arXiv preprint arXiv:2504.19678, 2025",arxiv.org,,https://arxiv.org/abs/2504.19678,,"Large language models and autonomous AI agents have evolved rapidly, resulting in adiverse array of evaluation benchmarks, frameworks, and collaboration protocols. However¬†‚Ä¶",61.0,Survey
SimpleSafetyTest,SimpleSafetyTest: Universal Safety Testing for Language Models,Safetyprompts: a systematic review of open datasets for evaluating and improving large language model safety,"P R√∂ttger,F Pernisi,B Vidgen,D Hovy- Proceedings of the AAAI¬†‚Ä¶, 2025",ojs.aaai.org,,https://ojs.aaai.org/index.php/AAAI/article/view/34975,,The last two years have seen a rapid growth in concerns around the safety of largelanguage models (LLMs). Researchers and practitioners have met these concerns by¬†‚Ä¶,52.0,Survey
SimpleSafetyTest,SimpleSafetyTest: Universal Safety Testing for Language Models,The responsible foundation model development cheatsheet: A review of tools & resources,"S Longpre,S Biderman,A Albalak‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2024",arxiv.org,,https://arxiv.org/abs/2406.16746,,"Foundation model development attracts a rapidly expanding body of contributors, scientists,and applications. To help shape responsible development practices, we introduce the¬†‚Ä¶",15.0,Survey
SimpleSafetyTest,SimpleSafetyTest: Universal Safety Testing for Language Models,Against The Achilles' Heel: A Survey on Red Teaming for Generative Models,"L Lin,H Mu,Z Zhai,M Wang,Y Wang,R Wang‚Ä¶¬†- Journal of Artificial¬†‚Ä¶, 2025",jair.org,,https://www.jair.org/index.php/jair/article/view/17654,,"Generative models are rapidly gaining popularity and being integrated into everydayapplications, raising concerns over their safe use as various vulnerabilities are exposed. In¬†‚Ä¶",41.0,Survey
XSTest,XSTest: A Test Suite for Identifying Exaggerated Safety Behaviors in Large Language Models,Adversarial attacks of vision tasks in the past 10 years: A survey,"C Zhang, L Zhou,X Xu, J Wu,Z Liu- ACM Computing Surveys, 2025",dl.acm.org,,https://dl.acm.org/doi/abs/10.1145/3743126,,"With the advent of Large Vision-Language Models (LVLMs), new attack vectors, such ascognitive bias, prompt injection, and jailbreaking, have emerged. Understanding these¬†‚Ä¶",17.0,Survey
XSTest,XSTest: A Test Suite for Identifying Exaggerated Safety Behaviors in Large Language Models,Safetyprompts: a systematic review of open datasets for evaluating and improving large language model safety,"P R√∂ttger,F Pernisi,B Vidgen,D Hovy- Proceedings of the AAAI¬†‚Ä¶, 2025",ojs.aaai.org,,https://ojs.aaai.org/index.php/AAAI/article/view/34975,,The last two years have seen a rapid growth in concerns around the safety of largelanguage models (LLMs). Researchers and practitioners have met these concerns by¬†‚Ä¶,52.0,Survey
