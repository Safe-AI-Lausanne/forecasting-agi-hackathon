benchmark_name,benchmark_paper,title,authors,publication,year,url,pdf_url,abstract,cited_by_count,paper_type
TruthfulQA,TruthfulQA: Measuring How Models Mimic Human Falsehoods,Judging llm-as-a-judge with mt-bench and chatbot arena,"L Zheng,WL Chiang,Y Sheng… - Advances in neural …, 2023",proceedings.neurips.cc,,https://proceedings.neurips.cc/paper_files/paper/2023/hash/91f18a1287b398d378ef22505bf41832-Abstract-Datasets_and_Benchmarks.html,,Evaluating large language model (LLM) based chat assistants is challenging due to theirbroad capabilities and the inadequacy of existing benchmarks in measuring human …,5891.0,Benchmark
TRUE,TRUE: Re-evaluating Factual Consistency Evaluation,Halueval: A large-scale hallucination evaluation benchmark for large language models,"J Li,X Cheng,WX Zhao,JY Nie,JR Wen- arXiv preprint arXiv:2305.11747, 2023",arxiv.org,,https://arxiv.org/abs/2305.11747,,"Large language models (LLMs), such as ChatGPT, are prone to generate hallucinations, ie,content that conflicts with the source or cannot be verified by the factual knowledge. To …",650.0,Benchmark
FIB,Evaluating the Factual Consistency of Large Language Models Through News Summarization,Generating benchmarks for factuality evaluation of language models,"D Muhlgay,O Ram,I Magar,Y Levine, N Ratner… - arXiv preprint arXiv …, 2023",arxiv.org,,https://arxiv.org/abs/2307.06908,,"Before deploying a language model (LM) within a given domain, it is important to measureits tendency to generate factually incorrect information in that domain. Existing methods for …",122.0,Benchmark
FEVER,FEVER: a Large-scale Dataset for Fact Extraction and VERification,Truthfulqa: Measuring how models mimic human falsehoods,"S Lin,J Hilton,O Evans- arXiv preprint arXiv:2109.07958, 2021",arxiv.org,,https://arxiv.org/abs/2109.07958,,We propose a benchmark to measure whether a language model is truthful in generatinganswers to questions. The benchmark comprises 817 questions that span 38 categories …,2553.0,Benchmark
HalluLens,HalluLens: LLM Hallucination Benchmark,AbstentionBench: Reasoning LLMs Fail on Unanswerable Questions,"P Kirichenko,M Ibrahim,K Chaudhuri… - arXiv preprint arXiv …, 2025",arxiv.org,,https://arxiv.org/abs/2506.09038,,"For Large Language Models (LLMs) to be reliably deployed in both everyday and high-stakes domains, knowing when not to answer is equally critical as answering correctly. Real …",13.0,Benchmark
FreshQA,FreshLLMs: Refreshing Large Language Models with Search Engine Augmentation,Long-form factuality in large language models,"J Wei,C Yang,X Song,Y Lu,N Hu… - Advances in …, 2024",proceedings.neurips.cc,,https://proceedings.neurips.cc/paper_files/paper/2024/hash/937ae0e83eb08d2cb8627fe1def8c751-Abstract-Conference.html,,Large language models (LLMs) often generate content that contains factual errors whenresponding to fact-seeking prompts on open-ended topics. To benchmark a model's long …,147.0,Benchmark
Jiminy Cricket,What Would Jiminy Cricket Do? Towards Agents That Behave Morally,Do the rewards justify the means? measuring trade-offs between rewards and ethical behavior in the machiavelli benchmark,"A Pan,JS Chan,A Zou,N Li,S Basart… - International …, 2023",proceedings.mlr.press,,https://proceedings.mlr.press/v202/pan23a.html,,"Artificial agents have traditionally been trained to maximize reward, which may incentivizepower-seeking and deception, analogous to how next-token prediction in language models …",199.0,Benchmark
SCRUPLES,Scruples: A Corpus of Community Ethical Judgments on 32000 Real-Life Anecdotes,Safetybench: Evaluating the safety of large language models,"Z Zhang,L Lei, L Wu, R Sun,Y Huang, C Long… - arXiv preprint arXiv …, 2023",arxiv.org,,https://arxiv.org/abs/2309.07045,,"With the rapid development of Large Language Models (LLMs), increasing attention hasbeen paid to their safety concerns. Consequently, evaluating the safety of LLMs has become …",108.0,Benchmark
MoralBench,MoralBench: Moral Evaluation of LLMs,"Value compass benchmarks: A comprehensive, generative and self-evolving platform for llms' value evaluation","J Yao,X Yi,S Duan,J Wang,Y Bai… - Proceedings of the …, 2025",aclanthology.org,,https://aclanthology.org/2025.acl-demo.64/,,"As large language models (LLMs) are gradually integrated into human daily life, assessingtheir underlying values becomes essential for understanding their risks and alignment with …",3.0,Benchmark
MoralBench,MoralBench: Moral Evaluation of LLMs,Benchmarking and advancing large language models for local life services,"X Lan,J Feng, J Lei, X Shi, Y Li - Proceedings of the 31st ACM SIGKDD …, 2025",dl.acm.org,,https://dl.acm.org/doi/abs/10.1145/3711896.3737196,,"Large language models (LLMs) have exhibited remarkable capabilities and achievedsignificant breakthroughs across various domains, leading to their widespread adoption in …",4.0,Benchmark
MoralBench,MoralBench: Moral Evaluation of LLMs,MoralBench: A MultiModal Moral Benchmark for LVLMs,"B Yan,J Zhang, Z Chen,S Shan, X Chen - arXiv preprint arXiv …, 2024",arxiv.org,,https://arxiv.org/abs/2412.20718,,"Recently, large foundation models, including large language models (LLMs) and largevision-language models (LVLMs), have become essential tools in critical fields such as law …",7.0,Benchmark
MoralBench,MoralBench: Moral Evaluation of LLMs,SafeDialBench: A Fine-Grained Safety Benchmark for Large Language Models in Multi-Turn Dialogues with Diverse Jailbreak Attacks,"H Cao, Y Wang, S Jing, Z Peng, Z Bai, Z Cao… - arXiv preprint arXiv …, 2025",arxiv.org,,https://arxiv.org/abs/2502.11090,,"With the rapid advancement of Large Language Models (LLMs), the safety of LLMs hasbeen a critical concern requiring precise assessment. Current benchmarks primarily …",5.0,Benchmark
STORAL,A Corpus for Understanding and Generating Moral Stories,Uro-bench: A comprehensive benchmark for end-to-end spoken dialogue models,"R Yan,X Li,W Chen,Z Niu,C Yang,Z Ma,K Yu… - arXiv preprint arXiv …, 2025",arxiv.org,,https://arxiv.org/abs/2502.17810,,"In recent years, with advances in large language models (LLMs), end-to-end spokendialogue models (SDMs) have made significant strides. Compared to text-based LLMs, the …",14.0,Benchmark
STORAL,A Corpus for Understanding and Generating Moral Stories,CULEMO: Cultural Lenses on Emotion--Benchmarking LLMs for Cross-Cultural Emotion Understanding,"TD Belay,AH Ahmed,A Grissom II,I Ameer… - arXiv preprint arXiv …, 2025",arxiv.org,,https://arxiv.org/abs/2503.10688,,"NLP research has increasingly focused on subjective tasks such as emotion analysis.However, existing emotion benchmarks suffer from two major shortcomings:(1) they largely …",8.0,Benchmark
STORAL,A Corpus for Understanding and Generating Moral Stories,Bilingual Dialogue Dataset with Personality and Emotion Annotations for Personality Recognition in Education,"Z Liu, Y Xiao,Z Su,L Ye,K Lu, X Peng - Scientific Data, 2025",nature.com,,https://www.nature.com/articles/s41597-025-04836-w,,"Dialogue datasets are essential for advancing natural language processing (NLP) tasks.However, many existing datasets lack integrated annotations for personality and emotion …",1.0,Benchmark
STORAL,A Corpus for Understanding and Generating Moral Stories,MORABLES: A Benchmark for Assessing Abstract Moral Reasoning in LLMs with Fables,"M Marcuzzo,A Zangari,A Albarelli… - arXiv preprint arXiv …, 2025",arxiv.org,,https://arxiv.org/abs/2509.12371,,"As LLMs excel on standard reading comprehension benchmarks, attention is shifting towardevaluating their capacity for complex abstract reasoning and inference. Literature-based …",,Benchmark
STORAL,A Corpus for Understanding and Generating Moral Stories,JETHICS: Japanese Ethics Understanding Evaluation Dataset,"M Takeshita,R Rzepka- arXiv preprint arXiv:2506.16187, 2025",arxiv.org,,https://arxiv.org/abs/2506.16187,,"In this work, we propose JETHICS, a Japanese dataset for evaluating ethics understandingof AI models. JETHICS contains 78K examples and is built by following the construction …",,Benchmark
RobustBench,RobustBench: a standardized adversarial robustness benchmark,Jailbreakbench: An open robustness benchmark for jailbreaking large language models,"P Chao,E Debenedetti,A Robey… - Advances in …, 2024",proceedings.neurips.cc,,https://proceedings.neurips.cc/paper_files/paper/2024/hash/63092d79154adebd7305dfd498cbff70-Abstract-Datasets_and_Benchmarks_Track.html,,"Jailbreak attacks cause large language models (LLMs) to generate harmful, unethical, orotherwise objectionable content. Evaluating these attacks presents a number of challenges …",350.0,Benchmark
RobustBench,RobustBench: a standardized adversarial robustness benchmark,Harmbench: A standardized evaluation framework for automated red teaming and robust refusal,"M Mazeika,L Phan,X Yin,A Zou,Z Wang,N Mu… - arXiv preprint arXiv …, 2024",arxiv.org,,https://arxiv.org/abs/2402.04249,,"Automated red teaming holds substantial promise for uncovering and mitigating the risksassociated with the malicious use of large language models (LLMs), yet the field lacks a …",549.0,Benchmark
AutoAttack,Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks,Harmbench: A standardized evaluation framework for automated red teaming and robust refusal,"M Mazeika,L Phan,X Yin,A Zou,Z Wang,N Mu… - arXiv preprint arXiv …, 2024",arxiv.org,,https://arxiv.org/abs/2402.04249,,"Automated red teaming holds substantial promise for uncovering and mitigating the risksassociated with the malicious use of large language models (LLMs), yet the field lacks a …",549.0,Benchmark
AutoAttack,Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks,Robustbench: a standardized adversarial robustness benchmark,"F Croce,M Andriushchenko,V Sehwag… - arXiv preprint arXiv …, 2020",arxiv.org,,https://arxiv.org/abs/2010.09670,,"As a research community, we are still lacking a systematic understanding of the progress onadversarial robustness which often makes it hard to identify the most promising ideas in …",989.0,Benchmark
WILDS,WILDS: A Benchmark of in-the-Wild Distribution Shifts,Datacomp: In search of the next generation of multimodal datasets,"SY Gadre,G Ilharco,A Fang… - Advances in …, 2023",proceedings.neurips.cc,,https://proceedings.neurips.cc/paper_files/paper/2023/hash/56332d41d55ad7ad8024aac625881be7-Abstract-Datasets_and_Benchmarks.html,,"Multimodal datasets are a critical component in recent breakthroughs such as CLIP, StableDiffusion and GPT-4, yet their design does not receive the same research attention as model …",626.0,Benchmark
OoD-Bench,OoD-Bench: Quantifying and Understanding Two Dimensions of Out-of-Distribution Generalization,Coda: A real-world road corner case dataset for object detection in autonomous driving,"K Li,K Chen,H Wang,L Hong, C Ye,J Han… - European conference on …, 2022",Springer,,https://link.springer.com/chapter/10.1007/978-3-031-19839-7_24,,"Contemporary deep-learning object detection methods for autonomous driving usuallypresume fixed categories of common traffic participants, such as pedestrians and cars. Most …",156.0,Benchmark
OoD-Bench,OoD-Bench: Quantifying and Understanding Two Dimensions of Out-of-Distribution Generalization,Wild-time: A benchmark of in-the-wild distribution shift over time,"H Yao,C Choi,B Cao,Y Lee… - Advances in Neural …, 2022",proceedings.neurips.cc,,https://proceedings.neurips.cc/paper_files/paper/2022/hash/43119db5d59f07cc08fca7ba6820179a-Abstract-Datasets_and_Benchmarks.html,,"Distribution shifts occur when the test distribution differs from the training distribution, andcan considerably degrade performance of machine learning models deployed in the real …",137.0,Benchmark
Adversarial Robustness Benchmark,Benchmarking Adversarial Robustness on Image Classification,Robustbench: a standardized adversarial robustness benchmark,"F Croce,M Andriushchenko,V Sehwag… - arXiv preprint arXiv …, 2020",arxiv.org,,https://arxiv.org/abs/2010.09670,,"As a research community, we are still lacking a systematic understanding of the progress onadversarial robustness which often makes it hard to identify the most promising ideas in …",989.0,Benchmark
Adversarial Robustness Benchmark,Benchmarking Adversarial Robustness on Image Classification,Backdoorbench: A comprehensive benchmark of backdoor learning,"B Wu,H Chen,M Zhang,Z Zhu,S Wei… - Advances in …, 2022",proceedings.neurips.cc,,https://proceedings.neurips.cc/paper_files/paper/2022/hash/4491ea1c91aa2b22c373e5f1dfce234f-Abstract-Datasets_and_Benchmarks.html,,Backdoor learning is an emerging and vital topic for studying deep neural networks'vulnerability (DNNs). Many pioneering backdoor attack and defense methods are being …,205.0,Benchmark
Adversarial Robustness Benchmark,Benchmarking Adversarial Robustness on Image Classification,A comprehensive study on robustness of image classification models: Benchmarking and rethinking,"C Liu,Y Dong,W Xiang,X Yang,H Su,J Zhu… - International Journal of …, 2025",Springer,,https://link.springer.com/article/10.1007/s11263-024-02196-3,,"The robustness of deep neural networks is frequently compromised when faced withadversarial examples, common corruptions, and distribution shifts, posing a significant …",134.0,Benchmark
GRB,Graph Robustness Benchmark: Benchmarking the Adversarial Robustness of Graph Machine Learning,Bond: Benchmarking unsupervised outlier node detection on static attributed graphs,"K Liu,Y Dou,Y Zhao,X Ding,X Hu… - Advances in …, 2022",proceedings.neurips.cc,,https://proceedings.neurips.cc/paper_files/paper/2022/hash/acc1ec4a9c780006c9aafd595104816b-Abstract-Datasets_and_Benchmarks.html,,Detecting which nodes in graphs are outliers is a relatively new machine learning task withnumerous applications. Despite the proliferation of algorithms developed in recent years for …,146.0,Benchmark
GRB,Graph Robustness Benchmark: Benchmarking the Adversarial Robustness of Graph Machine Learning,A comprehensive study on text-attributed graphs: Benchmarking and rethinking,"H Yan,C Li, R Long, C Yan,J Zhao… - Advances in …, 2023",proceedings.neurips.cc,,https://proceedings.neurips.cc/paper_files/paper/2023/hash/37d00f567a18b478065f1a91b95622a0-Abstract-Datasets_and_Benchmarks.html,,"Text-attributed graphs (TAGs) are prevalent in various real-world scenarios, where eachnode is associated with a text description. The cornerstone of representation learning on …",41.0,Benchmark
SafetyBench,SafetyBench: Evaluating the Safety of Large Language Models with Multiple Choice Questions,Agent-safetybench: Evaluating the safety of llm agents,"Z Zhang,S Cui,Y Lu, J Zhou,J Yang,H Wang… - arXiv preprint arXiv …, 2024",arxiv.org,,https://arxiv.org/abs/2412.14470,,"As large language models (LLMs) are increasingly deployed as agents, their integration intointeractive environments and tool use introduce new safety challenges beyond those …",52.0,Benchmark
ToxiGen,ToxiGen: A Large-Scale Machine-Generated Dataset for Adversarial and Implicit Hate Speech Detection,Agieval: A human-centric benchmark for evaluating foundation models,"W Zhong,R Cui,Y Guo,Y Liang,S Lu,Y Wang… - arXiv preprint arXiv …, 2023",arxiv.org,,https://arxiv.org/abs/2304.06364,,Evaluating the general abilities of foundation models to tackle human-level tasks is a vitalaspect of their development and application in the pursuit of Artificial General Intelligence …,599.0,Benchmark
BBQ,BBQ: A Hand-Built Bias Benchmark for Question Answering,Beavertails: Towards improved safety alignment of llm via a human-preference dataset,"J Ji,M Liu,J Dai,X Pan, C Zhang… - Advances in …, 2023",proceedings.neurips.cc,,https://proceedings.neurips.cc/paper_files/paper/2023/hash/4dbb61cb68671edc4ca3712d70083b9f-Abstract-Datasets_and_Benchmarks.html,,"In this paper, we introduce the BeaverTails dataset, aimed at fostering research on safetyalignment in large language models (LLMs). This dataset uniquely separates annotations of …",665.0,Benchmark
BOLD,BOLD: Dataset and Metrics for Measuring Biases in Open-Ended Language Generation,Agieval: A human-centric benchmark for evaluating foundation models,"W Zhong,R Cui,Y Guo,Y Liang,S Lu,Y Wang… - arXiv preprint arXiv …, 2023",arxiv.org,,https://arxiv.org/abs/2304.06364,,Evaluating the general abilities of foundation models to tackle human-level tasks is a vitalaspect of their development and application in the pursuit of Artificial General Intelligence …,599.0,Benchmark
JailbreakBench,JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models,Sorry-bench: Systematically evaluating large language model safety refusal,"T Xie,X Qi,Y Zeng,Y Huang,UM Sehwag… - arXiv preprint arXiv …, 2024",arxiv.org,,https://arxiv.org/abs/2406.14598,,"Evaluating aligned large language models'(LLMs) ability to recognize and reject unsafe userrequests is crucial for safe, policy-compliant deployments. Existing evaluation efforts …",141.0,Benchmark
HarmBench,HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal,Jailbreakbench: An open robustness benchmark for jailbreaking large language models,"P Chao,E Debenedetti,A Robey… - Advances in …, 2024",proceedings.neurips.cc,,https://proceedings.neurips.cc/paper_files/paper/2024/hash/63092d79154adebd7305dfd498cbff70-Abstract-Datasets_and_Benchmarks_Track.html,,"Jailbreak attacks cause large language models (LLMs) to generate harmful, unethical, orotherwise objectionable content. Evaluating these attacks presents a number of challenges …",350.0,Benchmark
HarmBench,HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal,The wmdp benchmark: Measuring and reducing malicious use with unlearning,"N Li,A Pan,A Gopal, S Yue, D Berrios,A Gatti… - arXiv preprint arXiv …, 2024",arxiv.org,,https://arxiv.org/abs/2403.03218,,"The White House Executive Order on Artificial Intelligence highlights the risks of largelanguage models (LLMs) empowering malicious actors in developing biological, cyber, and …",284.0,Benchmark
DecodingTrust,DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models,Jailbreakbench: An open robustness benchmark for jailbreaking large language models,"P Chao,E Debenedetti,A Robey… - Advances in …, 2024",proceedings.neurips.cc,,https://proceedings.neurips.cc/paper_files/paper/2024/hash/63092d79154adebd7305dfd498cbff70-Abstract-Datasets_and_Benchmarks_Track.html,,"Jailbreak attacks cause large language models (LLMs) to generate harmful, unethical, orotherwise objectionable content. Evaluating these attacks presents a number of challenges …",350.0,Benchmark
AdvBench,Universal and Transferable Adversarial Attacks on Aligned Language Models,Jailbreakbench: An open robustness benchmark for jailbreaking large language models,"P Chao,E Debenedetti,A Robey… - Advances in …, 2024",proceedings.neurips.cc,,https://proceedings.neurips.cc/paper_files/paper/2024/hash/63092d79154adebd7305dfd498cbff70-Abstract-Datasets_and_Benchmarks_Track.html,,"Jailbreak attacks cause large language models (LLMs) to generate harmful, unethical, orotherwise objectionable content. Evaluating these attacks presents a number of challenges …",350.0,Benchmark
HarmfulQA,Red-Teaming Large Language Models using Chain of Utterances for Safety-Alignment,Salad-bench: A hierarchical and comprehensive safety benchmark for large language models,"L Li,B Dong,R Wang,X Hu,W Zuo,D Lin… - arXiv preprint arXiv …, 2024",arxiv.org,,https://arxiv.org/abs/2402.05044,,"In the rapidly evolving landscape of Large Language Models (LLMs), ensuring robust safetymeasures is paramount. To meet this crucial need, we propose\emph {SALAD-Bench}, a …",197.0,Benchmark
HarmfulQA,Red-Teaming Large Language Models using Chain of Utterances for Safety-Alignment,R-judge: Benchmarking safety risk awareness for llm agents,"T Yuan,Z He,L Dong,Y Wang, R Zhao, T Xia… - arXiv preprint arXiv …, 2024",arxiv.org,,https://arxiv.org/abs/2401.10019,,"Large language models (LLMs) have exhibited great potential in autonomously completingtasks across real-world applications. Despite this, these LLM agents introduce unexpected …",135.0,Benchmark
HarmfulQA,Red-Teaming Large Language Models using Chain of Utterances for Safety-Alignment,T2vsafetybench: Evaluating the safety of text-to-video generative models,"Y Miao, Y Zhu, L Yu,J Zhu,XS Gao… - Advances in Neural …, 2024",proceedings.neurips.cc,,https://proceedings.neurips.cc/paper_files/paper/2024/hash/74eed5f568354c2e77dd9b018f38a9d4-Abstract-Datasets_and_Benchmarks_Track.html,,The recent development of Sora leads to a new era in text-to-video (T2V) generation. Alongwith this comes the rising concern about its safety risks. The generated videos may contain …,27.0,Benchmark
DoNotAnswer,Do-Not-Answer: A Dataset for Evaluating Safeguards in LLMs,Rewardbench: Evaluating reward models for language modeling,"N Lambert,V Pyatkin,J Morrison,LJ Miranda… - arXiv preprint arXiv …, 2024",arxiv.org,,https://arxiv.org/abs/2403.13787,,"Reward models (RMs) are at the crux of successfully using RLHF to align pretrained modelsto human preferences, yet there has been relatively little study that focuses on evaluation of …",393.0,Benchmark
DoNotAnswer,Do-Not-Answer: A Dataset for Evaluating Safeguards in LLMs,Salad-bench: A hierarchical and comprehensive safety benchmark for large language models,"L Li,B Dong,R Wang,X Hu,W Zuo,D Lin… - arXiv preprint arXiv …, 2024",arxiv.org,,https://arxiv.org/abs/2402.05044,,"In the rapidly evolving landscape of Large Language Models (LLMs), ensuring robust safetymeasures is paramount. To meet this crucial need, we propose\emph {SALAD-Bench}, a …",197.0,Benchmark
DoNotAnswer,Do-Not-Answer: A Dataset for Evaluating Safeguards in LLMs,Sorry-bench: Systematically evaluating large language model safety refusal,"T Xie,X Qi,Y Zeng,Y Huang,UM Sehwag… - arXiv preprint arXiv …, 2024",arxiv.org,,https://arxiv.org/abs/2406.14598,,"Evaluating aligned large language models'(LLMs) ability to recognize and reject unsafe userrequests is crucial for safe, policy-compliant deployments. Existing evaluation efforts …",141.0,Benchmark
DoNotAnswer,Do-Not-Answer: A Dataset for Evaluating Safeguards in LLMs,Judgebench: A benchmark for evaluating llm-based judges,"S Tan,S Zhuang,K Montgomery, WY Tang… - arXiv preprint arXiv …, 2024",arxiv.org,,https://arxiv.org/abs/2410.12784,,"LLM-based judges have emerged as a scalable alternative to human evaluation and areincreasingly used to assess, compare, and improve models. However, the reliability of LLM …",108.0,Benchmark
SALAD-Bench,SALAD-Bench: A Hierarchical and Comprehensive Safety Benchmark for Large Language Models,Sorry-bench: Systematically evaluating large language model safety refusal,"T Xie,X Qi,Y Zeng,Y Huang,UM Sehwag… - arXiv preprint arXiv …, 2024",arxiv.org,,https://arxiv.org/abs/2406.14598,,"Evaluating aligned large language models'(LLMs) ability to recognize and reject unsafe userrequests is crucial for safe, policy-compliant deployments. Existing evaluation efforts …",141.0,Benchmark
SALAD-Bench,SALAD-Bench: A Hierarchical and Comprehensive Safety Benchmark for Large Language Models,Or-bench: An over-refusal benchmark for large language models,"J Cui,WL Chiang,I Stoica,CJ Hsieh- arXiv preprint arXiv:2405.20947, 2024",arxiv.org,,https://arxiv.org/abs/2405.20947,,"Large Language Models (LLMs) require careful safety alignment to prevent maliciousoutputs. While significant research focuses on mitigating harmful content generation, the …",104.0,Benchmark
SALAD-Bench,SALAD-Bench: A Hierarchical and Comprehensive Safety Benchmark for Large Language Models,Sg-bench: Evaluating llm safety generalization across diverse tasks and prompt types,"Y Mou,S Zhang,W Ye- Advances in Neural Information …, 2024",proceedings.neurips.cc,,https://proceedings.neurips.cc/paper_files/paper/2024/hash/de7b99107c53e60257c727dc73daf1d1-Abstract-Datasets_and_Benchmarks_Track.html,,Ensuring the safety of large language model (LLM) applications is essential for developingtrustworthy artificial intelligence. Current LLM safety benchmarks have two limitations. First …,34.0,Benchmark
SALAD-Bench,SALAD-Bench: A Hierarchical and Comprehensive Safety Benchmark for Large Language Models,Audiobench: A universal benchmark for audio large language models,"B Wang, X Zou,G Lin,S Sun,Z Liu,W Zhang… - arXiv preprint arXiv …, 2024",arxiv.org,,https://arxiv.org/abs/2406.16020,,"We introduce AudioBench, a universal benchmark designed to evaluate Audio LargeLanguage Models (AudioLLMs). It encompasses 8 distinct tasks and 26 datasets, among …",107.0,Benchmark
MACHIAVELLI,Do the Rewards Justify the Means? Measuring Trade-Offs Between Rewards and Ethical Behavior in the MACHIAVELLI Benchmark,The wmdp benchmark: Measuring and reducing malicious use with unlearning,"N Li,A Pan,A Gopal, S Yue, D Berrios,A Gatti… - arXiv preprint arXiv …, 2024",arxiv.org,,https://arxiv.org/abs/2403.03218,,"The White House Executive Order on Artificial Intelligence highlights the risks of largelanguage models (LLMs) empowering malicious actors in developing biological, cyber, and …",284.0,Benchmark
OpenDeception,OpenDeception: Benchmarking and Investigating AI Deceptive Behaviors via Open-ended Interaction Simulation,DeceptionBench: A Comprehensive Benchmark for AI Deception Behaviors in Real-world Scenarios,"Y Huang,Y Sun,Y Zhang, R Zhang,Y Dong… - arXiv preprint arXiv …, 2025",arxiv.org,,https://arxiv.org/abs/2510.15501,,"Despite the remarkable advances of Large Language Models (LLMs) across diversecognitive tasks, the rapid enhancement of these capabilities also introduces emergent …",,Benchmark
OpenDeception,OpenDeception: Benchmarking and Investigating AI Deceptive Behaviors via Open-ended Interaction Simulation,WGSR-Bench: Wargame-based Game-theoretic Strategic Reasoning Benchmark for Large Language Models,"Q Yin, P Xu,Q Li, S Liu, S Shen, T Wang, Y Han… - arXiv preprint arXiv …, 2025",arxiv.org,,https://arxiv.org/abs/2506.10264,,"Recent breakthroughs in Large Language Models (LLMs) have led to a qualitative leap inartificial intelligence's performance on reasoning tasks, particularly demonstrating …",,Benchmark
Goal Misgeneralization (Analysis),Goal Misgeneralization: Why Correct Specifications Aren't Enough For Correct Goals,Safetywashing: Do AI Safety Benchmarks Actually Measure Safety Progress?,"R Ren,S Basart, A Khoja,A Gatti… - Advances in …, 2024",proceedings.neurips.cc,,https://proceedings.neurips.cc/paper_files/paper/2024/hash/7ebcdd0de471c027e67a11959c666d74-Abstract-Datasets_and_Benchmarks_Track.html,,"Performance on popular ML benchmarks is highly correlated with model scale, suggestingthat most benchmarks tend to measure a similar underlying factor of general model …",48.0,Benchmark
IPS Index,Instrumental Power-Seeking Index: A Novel Benchmark for Detecting Power-Seeking Behavior in LLMs,The wmdp benchmark: Measuring and reducing malicious use with unlearning,"N Li,A Pan,A Gopal, S Yue, D Berrios,A Gatti… - arXiv preprint arXiv …, 2024",arxiv.org,,https://arxiv.org/abs/2403.03218,,"The White House Executive Order on Artificial Intelligence highlights the risks of largelanguage models (LLMs) empowering malicious actors in developing biological, cyber, and …",284.0,Benchmark
SycEval,SycEval: Evaluating LLM Sycophancy,EchoBench: Benchmarking Sycophancy in Medical Large Vision-Language Models,"B Yuan, Y Zhou, Y Wang,F Huo, Y Jing, L Shen… - arXiv preprint arXiv …, 2025",arxiv.org,,https://arxiv.org/abs/2509.20146,,"Recent benchmarks for medical Large Vision-Language Models (LVLMs) emphasizeleaderboard accuracy, overlooking reliability and safety. We study sycophancy--models' …",1.0,Benchmark
HELM Safety,HELM Safety: Towards Standardized Safety Evaluations of Language Models,AIBENCH: TOWARDS TRUSTWORTHY EVALUA-TION UNDER THE 45 LAW,"Z Zhang,J Wang, Y Guo,F Wen,Z Chen,H Wang, W Li…",researchgate.net,,https://www.researchgate.net/profile/Zicheng-Zhang-9/publication/393362210_AIBENCH_TOWARDS_TRUSTWORTHY_EVALUA-_TION_UNDER_THE_45LAW/links/6867747be4632b045dc9b47c/AIBENCH-TOWARDS-TRUSTWORTHY-EVALUA-TION-UNDER-THE-45LAW.pdf,,"We present AIBench, a flexible and rapidly updating platform that aggregates evaluationresults from commercial platforms, popular open-source leaderboards, and internal …",6.0,Benchmark
TrustLLM,TrustLLM: Trustworthiness in Large Language Models,Jailbreakbench: An open robustness benchmark for jailbreaking large language models,"P Chao,E Debenedetti,A Robey… - Advances in …, 2024",proceedings.neurips.cc,,https://proceedings.neurips.cc/paper_files/paper/2024/hash/63092d79154adebd7305dfd498cbff70-Abstract-Datasets_and_Benchmarks_Track.html,,"Jailbreak attacks cause large language models (LLMs) to generate harmful, unethical, orotherwise objectionable content. Evaluating these attacks presents a number of challenges …",350.0,Benchmark
TrustLLM,TrustLLM: Trustworthiness in Large Language Models,Mllm-as-a-judge: Assessing multimodal llm-as-a-judge with vision-language benchmark,"D Chen,R Chen, S Zhang, Y Wang,Y Liu… - … on Machine Learning, 2024",openreview.net,,https://openreview.net/forum?id=dbFEFHAD79,,"Multimodal Large Language Models (MLLMs) have gained significant attention recently,showing remarkable potential in artificial general intelligence. However, assessing the utility …",210.0,Benchmark
MLCommons AILuminate,AILuminate: Introducing v1.0 of the AI Risk and Reliability Benchmark from MLCommons,D-rex: A benchmark for detecting deceptive reasoning in large language models,"S Krishna,A Zou,R Gupta,EK Jones, N Winter… - arXiv preprint arXiv …, 2025",arxiv.org,,https://arxiv.org/abs/2509.17938,,The safety and alignment of Large Language Models (LLMs) are critical for their responsibledeployment. Current evaluation methods predominantly focus on identifying and preventing …,1.0,Benchmark
MLCommons AILuminate,AILuminate: Introducing v1.0 of the AI Risk and Reliability Benchmark from MLCommons,Is Perceptual Encryption Secure? A Security Benchmark for Perceptual Encryption Methods,"U Kashyap,SK Padhi,SS Ali- IEEE Transactions on Artificial …, 2025",ieeexplore.ieee.org,,https://ieeexplore.ieee.org/abstract/document/10974570/,,"Perceptual encryption methods are the key enablers for protecting image privacy for deeplearning-based applications in the cloud. In perceptual encryption, the image content is …",,Benchmark
ALERT,ALERT: A Comprehensive Benchmark for Assessing Large Language Models' Safety through Red Teaming,Sorry-bench: Systematically evaluating large language model safety refusal,"T Xie,X Qi,Y Zeng,Y Huang,UM Sehwag… - arXiv preprint arXiv …, 2024",arxiv.org,,https://arxiv.org/abs/2406.14598,,"Evaluating aligned large language models'(LLMs) ability to recognize and reject unsafe userrequests is crucial for safe, policy-compliant deployments. Existing evaluation efforts …",141.0,Benchmark
ALERT,ALERT: A Comprehensive Benchmark for Assessing Large Language Models' Safety through Red Teaming,Or-bench: An over-refusal benchmark for large language models,"J Cui,WL Chiang,I Stoica,CJ Hsieh- arXiv preprint arXiv:2405.20947, 2024",arxiv.org,,https://arxiv.org/abs/2405.20947,,"Large Language Models (LLMs) require careful safety alignment to prevent maliciousoutputs. While significant research focuses on mitigating harmful content generation, the …",104.0,Benchmark
ALERT,ALERT: A Comprehensive Benchmark for Assessing Large Language Models' Safety through Red Teaming,Benchmarking adversarial robustness to bias elicitation in large language models: Scalable automated assessment with llm-as-a-judge,"R Cantini,A Orsino, M Ruggiero,D Talia- Machine Learning, 2025",Springer,,https://link.springer.com/article/10.1007/s10994-025-06862-6,,The growing integration of Large Language Models (LLMs) into critical societal domains hasraised concerns about embedded biases that can perpetuate stereotypes and undermine …,13.0,Benchmark
SimpleSafetyTest,SimpleSafetyTest: Universal Safety Testing for Language Models,Xstest: A test suite for identifying exaggerated safety behaviours in large language models,"P Röttger,HR Kirk,B Vidgen,G Attanasio… - arXiv preprint arXiv …, 2023",arxiv.org,,https://arxiv.org/abs/2308.01263,,"Without proper safeguards, large language models will readily follow malicious instructionsand generate toxic content. This risk motivates safety efforts such as red-teaming and large …",349.0,Benchmark
SimpleSafetyTest,SimpleSafetyTest: Universal Safety Testing for Language Models,Sorry-bench: Systematically evaluating large language model safety refusal,"T Xie,X Qi,Y Zeng,Y Huang,UM Sehwag… - arXiv preprint arXiv …, 2024",arxiv.org,,https://arxiv.org/abs/2406.14598,,"Evaluating aligned large language models'(LLMs) ability to recognize and reject unsafe userrequests is crucial for safe, policy-compliant deployments. Existing evaluation efforts …",141.0,Benchmark
XSTest,XSTest: A Test Suite for Identifying Exaggerated Safety Behaviors in Large Language Models,Jailbreakbench: An open robustness benchmark for jailbreaking large language models,"P Chao,E Debenedetti,A Robey… - Advances in …, 2024",proceedings.neurips.cc,,https://proceedings.neurips.cc/paper_files/paper/2024/hash/63092d79154adebd7305dfd498cbff70-Abstract-Datasets_and_Benchmarks_Track.html,,"Jailbreak attacks cause large language models (LLMs) to generate harmful, unethical, orotherwise objectionable content. Evaluating these attacks presents a number of challenges …",350.0,Benchmark
XSTest,XSTest: A Test Suite for Identifying Exaggerated Safety Behaviors in Large Language Models,Rewardbench: Evaluating reward models for language modeling,"N Lambert,V Pyatkin,J Morrison,LJ Miranda… - arXiv preprint arXiv …, 2024",arxiv.org,,https://arxiv.org/abs/2403.13787,,"Reward models (RMs) are at the crux of successfully using RLHF to align pretrained modelsto human preferences, yet there has been relatively little study that focuses on evaluation of …",393.0,Benchmark
