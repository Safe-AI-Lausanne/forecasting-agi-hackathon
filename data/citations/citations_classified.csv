benchmark_name,benchmark_paper,title,authors,publication,year,url,pdf_url,abstract,cited_by_count,paper_type
TruthfulQA,TruthfulQA: Measuring How Models Mimic Human Falsehoods,A survey on evaluation of large language models,"Y Chang,X Wang,J Wang,Y Wu,L Yang‚Ä¶¬†- ACM transactions on¬†‚Ä¶, 2024",dl.acm.org,,https://dl.acm.org/doi/abs/10.1145/3641289,,"Large language models (LLMs) are gaining increasing popularity in both academia andindustry, owing to their unprecedented performance in various applications. As LLMs¬†‚Ä¶",4405.0,Survey
TruthfulQA,TruthfulQA: Measuring How Models Mimic Human Falsehoods,"A survey on large language model (llm) security and privacy: The good, the bad, and the ugly","Y Yao,J Duan,K Xu, Y Cai,Z Sun,Y Zhang- High-Confidence Computing, 2024",Elsevier,,https://www.sciencedirect.com/science/article/pii/S266729522400014X,,"Abstract Large Language Models (LLMs), such as ChatGPT and Bard, have revolutionizednatural language understanding and generation. They possess deep language¬†‚Ä¶",1335.0,Survey
TruthfulQA,TruthfulQA: Measuring How Models Mimic Human Falsehoods,"A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions","L Huang,W Yu,W Ma,W Zhong,Z Feng‚Ä¶¬†- ACM Transactions on¬†‚Ä¶, 2025",dl.acm.org,,https://dl.acm.org/doi/abs/10.1145/3703155,,"The emergence of large language models (LLMs) has marked a significant breakthrough innatural language processing (NLP), fueling a paradigm shift in information acquisition¬†‚Ä¶",3037.0,Survey
TruthfulQA,TruthfulQA: Measuring How Models Mimic Human Falsehoods,üßú Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models,"Y Zhang,Y Li,L Cui,D Cai,L Liu,T Fu‚Ä¶¬†- Computational¬†‚Ä¶, 2025",direct.mit.edu,,https://direct.mit.edu/coli/article/doi/10.1162/coli.a.16/131631,,"While large language models (LLMs) have demonstrated remarkable capabilities across arange of downstream tasks, a significant concern revolves around their propensity to exhibit¬†‚Ä¶",1549.0,Survey
TruthfulQA,TruthfulQA: Measuring How Models Mimic Human Falsehoods,Simpo: Simple preference optimization with a reference-free reward,"Y Meng,M Xia,D Chen- Advances in Neural Information¬†‚Ä¶, 2024",proceedings.neurips.cc,,https://proceedings.neurips.cc/paper_files/paper/2024/hash/e099c1c9699814af0be873a175361713-Abstract-Conference.html,,Abstract Direct Preference Optimization (DPO) is a widely used offline preferenceoptimization algorithm that reparameterizes reward functions in reinforcement learning from¬†‚Ä¶,657.0,Methodology
TruthfulQA,TruthfulQA: Measuring How Models Mimic Human Falsehoods,Llama 2: Open foundation and fine-tuned chat models,"H Touvron,L Martin,K Stone, P Albert‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2023",arxiv.org,,https://arxiv.org/abs/2307.09288,,"In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned largelanguage models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine¬†‚Ä¶",19070.0,Methodology
TruthfulQA,TruthfulQA: Measuring How Models Mimic Human Falsehoods,A comprehensive overview of large language models,"H Naveed,AU Khan,S Qiu,M Saqib,S Anwar‚Ä¶¬†- ACM Transactions on¬†‚Ä¶, 2025",dl.acm.org,,https://dl.acm.org/doi/abs/10.1145/3744746,,Large Language Models (LLMs) have recently demonstrated remarkable capabilities innatural language processing tasks and beyond. This success of LLMs has led to a large¬†‚Ä¶,1815.0,Survey
TruthfulQA,TruthfulQA: Measuring How Models Mimic Human Falsehoods,Large language models: A survey,"S Minaee,T Mikolov,N Nikzad,M Chenaghlu‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2024",arxiv.org,,https://arxiv.org/abs/2402.06196,,"Large Language Models (LLMs) have drawn a lot of attention due to their strongperformance on a wide range of natural language tasks, since the release of ChatGPT in¬†‚Ä¶",1504.0,Survey
TruthfulQA,TruthfulQA: Measuring How Models Mimic Human Falsehoods,Judging llm-as-a-judge with mt-bench and chatbot arena,"L Zheng,WL Chiang,Y Sheng‚Ä¶¬†- Advances in neural¬†‚Ä¶, 2023",proceedings.neurips.cc,,https://proceedings.neurips.cc/paper_files/paper/2023/hash/91f18a1287b398d378ef22505bf41832-Abstract-Datasets_and_Benchmarks.html,,Evaluating large language model (LLM) based chat assistants is challenging due to theirbroad capabilities and the inadequacy of existing benchmarks in measuring human¬†‚Ä¶,5891.0,Benchmark
TruthfulQA,TruthfulQA: Measuring How Models Mimic Human Falsehoods,A survey of large language models,"WX Zhao,K Zhou,J Li,T Tang,X Wang‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2023",researchgate.net,,https://www.researchgate.net/profile/Tang-Tianyi-3/publication/369740832_A_Survey_of_Large_Language_Models/links/665fd2e3637e4448a37dd281/A-Survey-of-Large-Language-Models.pdf,,"Ever since the Turing Test was proposed in the 1950s, humans have explored the masteringof language intelligence by machine. Language is essentially a complex, intricate system of¬†‚Ä¶",6302.0,Survey
HaluEval,HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models,A survey of GPT-3 family large language models including ChatGPT and GPT-4,"KS Kalyan- Natural Language Processing Journal, 2024",Elsevier,,https://www.sciencedirect.com/science/article/pii/S2949719123000456,,"Large language models (LLMs) are a special class of pretrained language models (PLMs)obtained by scaling model size, pretraining corpus and computation. LLMs, because of their¬†‚Ä¶",512.0,Survey
HaluEval,HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models,From google gemini to openai q*(q-star): A survey on reshaping the generative artificial intelligence (ai) research landscape,"TR McIntosh,T Susnjak,T Liu,P Watters,D Xu,D Liu‚Ä¶¬†- Technologies, 2025",mdpi.com,,https://www.mdpi.com/2227-7080/13/2/51,,"This comprehensive survey explored the evolving landscape of generative ArtificialIntelligence (AI), with a specific focus on the recent technological breakthroughs and the¬†‚Ä¶",250.0,Survey
HaluEval,HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models,"A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions","L Huang,W Yu,W Ma,W Zhong,Z Feng‚Ä¶¬†- ACM Transactions on¬†‚Ä¶, 2025",dl.acm.org,,https://dl.acm.org/doi/abs/10.1145/3703155,,"The emergence of large language models (LLMs) has marked a significant breakthrough innatural language processing (NLP), fueling a paradigm shift in information acquisition¬†‚Ä¶",3037.0,Survey
HaluEval,HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models,üßú Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models,"Y Zhang,Y Li,L Cui,D Cai,L Liu,T Fu‚Ä¶¬†- Computational¬†‚Ä¶, 2025",direct.mit.edu,,https://direct.mit.edu/coli/article/doi/10.1162/coli.a.16/131631,,"While large language models (LLMs) have demonstrated remarkable capabilities across arange of downstream tasks, a significant concern revolves around their propensity to exhibit¬†‚Ä¶",1549.0,Survey
HaluEval,HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models,Large language models: A survey,"S Minaee,T Mikolov,N Nikzad,M Chenaghlu‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2024",arxiv.org,,https://arxiv.org/abs/2402.06196,,"Large Language Models (LLMs) have drawn a lot of attention due to their strongperformance on a wide range of natural language tasks, since the release of ChatGPT in¬†‚Ä¶",1504.0,Survey
HaluEval,HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models,A survey of large language models,"WX Zhao,K Zhou,J Li,T Tang,X Wang‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2023",researchgate.net,,https://www.researchgate.net/profile/Tang-Tianyi-3/publication/369740832_A_Survey_of_Large_Language_Models/links/665fd2e3637e4448a37dd281/A-Survey-of-Large-Language-Models.pdf,,"Ever since the Turing Test was proposed in the 1950s, humans have explored the masteringof language intelligence by machine. Language is essentially a complex, intricate system of¬†‚Ä¶",6302.0,Survey
HaluEval,HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models,Hallucination is inevitable: An innate limitation of large language models,"Z Xu,S Jain,M Kankanhalli- arXiv preprint arXiv:2401.11817, 2024",arxiv.org,,https://arxiv.org/abs/2401.11817,,Hallucination has been widely recognized to be a significant drawback for large languagemodels (LLMs). There have been many works that attempt to reduce the extent of¬†‚Ä¶,723.0,Other
HaluEval,HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models,Ragas: Automated evaluation of retrieval augmented generation,"S Es,J James,LE Anke‚Ä¶¬†- Proceedings of the 18th¬†‚Ä¶, 2024",aclanthology.org,,https://aclanthology.org/2024.eacl-demo.16/,,"Abstract We introduce RAGAs (Retrieval Augmented Generation Assessment), a frameworkfor reference-free evaluation of Retrieval Augmented Generation (RAG) pipelines. RAGAs is¬†‚Ä¶",795.0,Methodology
HaluEval,HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models,Large language models for information retrieval: A survey,"Y Zhu,H Yuan,S Wang,J Liu,W Liu,C Deng‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2023",arxiv.org,,https://arxiv.org/abs/2308.07107,,"As a primary means of information acquisition, information retrieval (IR) systems, such assearch engines, have integrated themselves into our daily lives. These systems also serve¬†‚Ä¶",585.0,Survey
HaluEval,HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models,Large legal fictions: Profiling legal hallucinations in large language models,"M Dahl, V Magesh,M Suzgun‚Ä¶¬†- Journal of Legal Analysis, 2024",academic.oup.com,,https://academic.oup.com/jla/article-abstract/16/1/64/7699227,,"Do large language models (LLMs) know the law? LLMs are increasingly being used toaugment legal practice, education, and research, yet their revolutionary potential is¬†‚Ä¶",295.0,Other
TRUE,TRUE: Re-evaluating Factual Consistency Evaluation,A survey on evaluation of large language models,"Y Chang,X Wang,J Wang,Y Wu,L Yang‚Ä¶¬†- ACM transactions on¬†‚Ä¶, 2024",dl.acm.org,,https://dl.acm.org/doi/abs/10.1145/3641289,,"Large language models (LLMs) are gaining increasing popularity in both academia andindustry, owing to their unprecedented performance in various applications. As LLMs¬†‚Ä¶",4405.0,Survey
TRUE,TRUE: Re-evaluating Factual Consistency Evaluation,Survey of hallucination in natural language generation,"Z Ji,N Lee,R Frieske,T Yu,D Su,Y Xu,E Ishii‚Ä¶¬†- ACM computing¬†‚Ä¶, 2023",dl.acm.org,,https://dl.acm.org/doi/abs/10.1145/3571730,,Natural Language Generation (NLG) has improved exponentially in recent years thanks tothe development of sequence-to-sequence deep learning technologies such as Transformer¬†‚Ä¶,5570.0,Survey
TRUE,TRUE: Re-evaluating Factual Consistency Evaluation,Detecting hallucinations in large language models using semantic entropy,"S Farquhar,J Kossen,L Kuhn,Y Gal- Nature, 2024",nature.com,,https://www.nature.com/articles/s41586-024-07421-0,,"Large language model (LLM) systems, such as ChatGPT or Gemini, can show impressivereasoning and question-answering capabilities but often 'hallucinate'false outputs and¬†‚Ä¶",772.0,Methodology
TRUE,TRUE: Re-evaluating Factual Consistency Evaluation,Halueval: A large-scale hallucination evaluation benchmark for large language models,"J Li,X Cheng,WX Zhao,JY Nie,JR Wen- arXiv preprint arXiv:2305.11747, 2023",arxiv.org,,https://arxiv.org/abs/2305.11747,,"Large language models (LLMs), such as ChatGPT, are prone to generate hallucinations, ie,content that conflicts with the source or cannot be verified by the factual knowledge. To¬†‚Ä¶",650.0,Benchmark
TRUE,TRUE: Re-evaluating Factual Consistency Evaluation,Enabling large language models to generate text with citations,"T Gao,H Yen,J Yu,D Chen- arXiv preprint arXiv:2305.14627, 2023",arxiv.org,,https://arxiv.org/abs/2305.14627,,"Large language models (LLMs) have emerged as a widely-used tool for informationseeking, but their generated outputs are prone to hallucination. In this work, our aim is to¬†‚Ä¶",437.0,Methodology
TRUE,TRUE: Re-evaluating Factual Consistency Evaluation,"Rarr: Researching and revising what language models say, using language models","L Gao,Z Dai,P Pasupat,A Chen,AT Chaganty‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2022",arxiv.org,,https://arxiv.org/abs/2210.08726,,"Language models (LMs) now excel at many tasks such as few-shot learning, questionanswering, reasoning, and dialog. However, they sometimes generate unsupported or¬†‚Ä¶",395.0,Methodology
TRUE,TRUE: Re-evaluating Factual Consistency Evaluation,Large language model alignment: A survey,"T Shen,R Jin,Y Huang,C Liu,W Dong, Z Guo‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2023",arxiv.org,,https://arxiv.org/abs/2309.15025,,"Recent years have witnessed remarkable progress made in large language models (LLMs).Such advancements, while garnering significant attention, have concurrently elicited various¬†‚Ä¶",289.0,Survey
TRUE,TRUE: Re-evaluating Factual Consistency Evaluation,Making retrieval-augmented language models robust to irrelevant context,"O Yoran,T Wolfson,O Ram,J Berant- arXiv preprint arXiv:2310.01558, 2023",arxiv.org,,https://arxiv.org/abs/2310.01558,,"Retrieval-augmented language models (RALMs) hold promise to produce languageunderstanding systems that are are factual, efficient, and up-to-date. An important¬†‚Ä¶",259.0,Methodology
TRUE,TRUE: Re-evaluating Factual Consistency Evaluation,Does fine-tuning llms on new knowledge encourage hallucinations?,"Z Gekhman,G Yona,R Aharoni,M Eyal‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2024",arxiv.org,,https://arxiv.org/abs/2405.05904,,"When large language models are aligned via supervised fine-tuning, they may encounternew factual information that was not acquired through pre-training. It is often conjectured that¬†‚Ä¶",198.0,Methodology
TRUE,TRUE: Re-evaluating Factual Consistency Evaluation,AlignScore: Evaluating factual consistency with a unified alignment function,"Y Zha,Y Yang,R Li,Z Hu- arXiv preprint arXiv:2305.16739, 2023",arxiv.org,,https://arxiv.org/abs/2305.16739,,Many text generation applications require the generated text to be factually consistent withinput information. Automatic evaluation of factual consistency is challenging. Previous work¬†‚Ä¶,271.0,Methodology
FIB,Evaluating the Factual Consistency of Large Language Models Through News Summarization,"Survey on factuality in large language models: Knowledge, retrieval and domain-specificity","C Wang,X Liu,Y Yue,X Tang,T Zhang‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2023",arxiv.org,,https://arxiv.org/abs/2310.07521,,"This survey addresses the crucial issue of factuality in Large Language Models (LLMs). AsLLMs find applications across diverse domains, the reliability and accuracy of their outputs¬†‚Ä¶",281.0,Survey
FIB,Evaluating the Factual Consistency of Large Language Models Through News Summarization,Evaluating large language models: A comprehensive survey,"Z Guo,R Jin,C Liu,Y Huang,D Shi, L Yu, Y Liu‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2023",arxiv.org,,https://arxiv.org/abs/2310.19736,,Large language models (LLMs) have demonstrated remarkable capabilities across a broadspectrum of tasks. They have attracted significant attention and been deployed in numerous¬†‚Ä¶,237.0,Survey
FIB,Evaluating the Factual Consistency of Large Language Models Through News Summarization,Trustllm: Trustworthiness in large language models,"Y Huang,L Sun,H Wang, S Wu,Q Zhang, Y Li‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2024",arxiv.org,,https://arxiv.org/abs/2401.05561,,"Large language models (LLMs), exemplified by ChatGPT, have gained considerableattention for their excellent natural language processing capabilities. Nonetheless, these¬†‚Ä¶",487.0,Methodology
FIB,Evaluating the Factual Consistency of Large Language Models Through News Summarization,Cognitive mirage: A review of hallucinations in large language models,"H Ye,T Liu,A Zhang, W Hua,W Jia- arXiv preprint arXiv:2309.06794, 2023",arxiv.org,,https://arxiv.org/abs/2309.06794,,"As large language models continue to develop in the field of AI, text generation systems aresusceptible to a worrisome phenomenon known as hallucination. In this study, we¬†‚Ä¶",218.0,Survey
FIB,Evaluating the Factual Consistency of Large Language Models Through News Summarization,A comprehensive survey on process-oriented automatic text summarization with exploration of llm-based methods,"Y Zhang, H Jin, D Meng, J Wang, J Tan¬†- arXiv preprint arXiv:2403.02901, 2024",arxiv.org,,https://arxiv.org/abs/2403.02901,,"Automatic Text Summarization (ATS), utilizing Natural Language Processing (NLP)algorithms, aims to create concise and accurate summaries, thereby significantly reducing¬†‚Ä¶",223.0,Survey
FIB,Evaluating the Factual Consistency of Large Language Models Through News Summarization,Position: Trustllm: Trustworthiness in large language models,"Y Huang,L Sun,H Wang, S Wu‚Ä¶¬†- International¬†‚Ä¶, 2024",proceedings.mlr.press,,http://proceedings.mlr.press/v235/huang24x.html,,"Large language models (LLMs) have gained considerable attention for their excellentnatural language processing capabilities. Nonetheless, these LLMs present many¬†‚Ä¶",88.0,Methodology
FIB,Evaluating the Factual Consistency of Large Language Models Through News Summarization,A systematic survey of text summarization: From statistical methods to large language models,"H Zhang,PS Yu,J Zhang- ACM Computing Surveys, 2025",dl.acm.org,,https://dl.acm.org/doi/abs/10.1145/3731445,,"Text summarization research has undergone several significant transformations with theadvent of deep neural networks, pre-trained language models (PLMs), and recent large¬†‚Ä¶",113.0,Survey
FIB,Evaluating the Factual Consistency of Large Language Models Through News Summarization,Lm vs lm: Detecting factual errors via cross examination,"R Cohen, M Hamri,M Geva,A Globerson- arXiv preprint arXiv¬†‚Ä¶, 2023",arxiv.org,,https://arxiv.org/abs/2305.13281,,"A prominent weakness of modern language models (LMs) is their tendency to generatefactually incorrect text, which hinders their usability. A natural question is whether such¬†‚Ä¶",180.0,Methodology
FIB,Evaluating the Factual Consistency of Large Language Models Through News Summarization,Lave: Llm-powered agent assistance and language augmentation for video editing,"B Wang,Y Li, Z Lv,H Xia,Y Xu, R Sodhi¬†- Proceedings of the 29th¬†‚Ä¶, 2024",dl.acm.org,,https://dl.acm.org/doi/abs/10.1145/3640543.3645143,,"Video creation has become increasingly popular, yet the expertise and effort required forediting often pose barriers to beginners. In this paper, we explore the integration of large¬†‚Ä¶",86.0,Other
FIB,Evaluating the Factual Consistency of Large Language Models Through News Summarization,Generating benchmarks for factuality evaluation of language models,"D Muhlgay,O Ram,I Magar,Y Levine, N Ratner‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2023",arxiv.org,,https://arxiv.org/abs/2307.06908,,"Before deploying a language model (LM) within a given domain, it is important to measureits tendency to generate factually incorrect information in that domain. Existing methods for¬†‚Ä¶",122.0,Benchmark
FEVER,FEVER: a Large-scale Dataset for Fact Extraction and VERification,A comprehensive overview of large language models,"H Naveed,AU Khan,S Qiu,M Saqib,S Anwar‚Ä¶¬†- ACM Transactions on¬†‚Ä¶, 2025",dl.acm.org,,https://dl.acm.org/doi/abs/10.1145/3744746,,Large Language Models (LLMs) have recently demonstrated remarkable capabilities innatural language processing tasks and beyond. This success of LLMs has led to a large¬†‚Ä¶,1815.0,Survey
FEVER,FEVER: a Large-scale Dataset for Fact Extraction and VERification,Survey of hallucination in natural language generation,"Z Ji,N Lee,R Frieske,T Yu,D Su,Y Xu,E Ishii‚Ä¶¬†- ACM computing¬†‚Ä¶, 2023",dl.acm.org,,https://dl.acm.org/doi/abs/10.1145/3571730,,Natural Language Generation (NLG) has improved exponentially in recent years thanks tothe development of sequence-to-sequence deep learning technologies such as Transformer¬†‚Ä¶,5570.0,Survey
FEVER,FEVER: a Large-scale Dataset for Fact Extraction and VERification,Retrieval-augmented generation for large language models: A survey,"Y Gao, Y Xiong, X Gao, K Jia, J Pan,Y Bi‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2023",simg.baai.ac.cn,,https://simg.baai.ac.cn/paperfile/25a43194-c74c-4cd3-b60f-0a1f27f8b8af.pdf,,"Large language models (LLMs) demonstrate powerful capabilities, but they still facechallenges in practical applications, such as hallucinations, slow knowledge updates, and¬†‚Ä¶",3542.0,Survey
FEVER,FEVER: a Large-scale Dataset for Fact Extraction and VERification,React: Synergizing reasoning and acting in language models,"S Yao,J Zhao,D Yu,N Du,I Shafran‚Ä¶¬†- The eleventh¬†‚Ä¶, 2022",openreview.net,,https://openreview.net/forum?id=WE_vluYUL-X,,"While large language models (LLMs) have demonstrated impressive capabilities acrosstasks in language understanding and interactive decision making, their abilities for¬†‚Ä¶",5155.0,Methodology
FEVER,FEVER: a Large-scale Dataset for Fact Extraction and VERification,Factscore: Fine-grained atomic evaluation of factual precision in long form text generation,"S Min,K Krishna,X Lyu,M Lewis,W Yih‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2023",arxiv.org,,https://arxiv.org/abs/2305.14251,,Evaluating the factuality of long-form text generated by large language models (LMs) is non-trivial because (1) generations often contain a mixture of supported and unsupported pieces¬†‚Ä¶,875.0,Methodology
FEVER,FEVER: a Large-scale Dataset for Fact Extraction and VERification,Beyond the imitation game: Quantifying and extrapolating the capabilities of language models,"A Srivastava, A Rastogi, A Rao,AAM Shoeb‚Ä¶¬†- ‚Ä¶¬†on machine learning¬†‚Ä¶, 2023",openreview.net,,https://openreview.net/forum?id=uyTL5Bvosj&nesting=2&sort=date-desc,,"Language models demonstrate both quantitative improvement and new qualitativecapabilities with increasing scale. Despite their potentially transformative impact, these new¬†‚Ä¶",2020.0,Methodology
FEVER,FEVER: a Large-scale Dataset for Fact Extraction and VERification,Improving text embeddings with large language models,"L Wang,N Yang,X Huang,L Yang‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2023",arxiv.org,,https://arxiv.org/abs/2401.00368,,"In this paper, we introduce a novel and simple method for obtaining high-quality textembeddings using only synthetic data and less than 1k training steps. Unlike existing¬†‚Ä¶",579.0,Methodology
FEVER,FEVER: a Large-scale Dataset for Fact Extraction and VERification,Unleashing the potential of prompt engineering in large language models: a comprehensive review,"B Chen,Z Zhang,N Langren√©,S Zhu- arXiv preprint arXiv:2310.14735, 2023",arxiv.org,,https://arxiv.org/abs/2310.14735,,This comprehensive review delves into the pivotal role of prompt engineering in unleashingthe capabilities of Large Language Models (LLMs). The development of Artificial Intelligence¬†‚Ä¶,568.0,Survey
FEVER,FEVER: a Large-scale Dataset for Fact Extraction and VERification,Truthfulqa: Measuring how models mimic human falsehoods,"S Lin,J Hilton,O Evans- arXiv preprint arXiv:2109.07958, 2021",arxiv.org,,https://arxiv.org/abs/2109.07958,,We propose a benchmark to measure whether a language model is truthful in generatinganswers to questions. The benchmark comprises 817 questions that span 38 categories¬†‚Ä¶,2553.0,Benchmark
FEVER,FEVER: a Large-scale Dataset for Fact Extraction and VERification,Text embeddings by weakly-supervised contrastive pre-training,"L Wang,N Yang,X Huang,B Jiao,L Yang‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2022",arxiv.org,,https://arxiv.org/abs/2212.03533,,"This paper presents E5, a family of state-of-the-art text embeddings that transfer well to awide range of tasks. The model is trained in a contrastive manner with weak supervision¬†‚Ä¶",876.0,Methodology
HalluLens,HalluLens: LLM Hallucination Benchmark,Why language models hallucinate,"AT Kalai,O Nachum,SS Vempala,E Zhang- arXiv preprint arXiv¬†‚Ä¶, 2025",arxiv.org,,https://arxiv.org/abs/2509.04664,,"Like students facing hard exam questions, large language models sometimes guess whenuncertain, producing plausible yet incorrect statements instead of admitting uncertainty¬†‚Ä¶",41.0,Other
HalluLens,HalluLens: LLM Hallucination Benchmark,Towards holistic evaluation of large audio-language models: A comprehensive survey,"CK Yang, NS Ho,H Lee- arXiv preprint arXiv:2505.15957, 2025",arxiv.org,,https://arxiv.org/abs/2505.15957,,"With advancements in large audio-language models (LALMs), which enhance largelanguage models (LLMs) with auditory capabilities, these models are expected to¬†‚Ä¶",19.0,Survey
HalluLens,HalluLens: LLM Hallucination Benchmark,Never compromise to vulnerabilities: A comprehensive survey on ai governance,"Y Jiang,J Zhao, Y Yuan,T Zhang,Y Huang‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2025",arxiv.org,,https://arxiv.org/abs/2508.08789,,"The rapid advancement of AI has expanded its capabilities across domains, yet introducedcritical technical vulnerabilities, such as algorithmic bias and adversarial sensitivity, that¬†‚Ä¶",2.0,Survey
HalluLens,HalluLens: LLM Hallucination Benchmark,A comprehensive taxonomy of hallucinations in large language models,"M Cossio- arXiv preprint arXiv:2508.01781, 2025",arxiv.org,,https://arxiv.org/abs/2508.01781,,"Large language models (LLMs) have revolutionized natural language processing, yet theirpropensity for hallucination, generating plausible but factually incorrect or fabricated content¬†‚Ä¶",9.0,Survey
HalluLens,HalluLens: LLM Hallucination Benchmark,Stress testing deliberative alignment for anti-scheming training,"B Schoen, E Nitishinskaya,M Balesni‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2025",arxiv.org,,https://arxiv.org/abs/2509.15541,,"Highly capable AI systems could secretly pursue misaligned goals--what we call"" scheming"".Because a scheming AI would deliberately try to hide its misaligned goals and actions¬†‚Ä¶",4.0,Methodology
HalluLens,HalluLens: LLM Hallucination Benchmark,The hallucination tax of reinforcement finetuning,"L Song,T Shi,J Zhao- arXiv preprint arXiv:2505.13988, 2025",arxiv.org,,https://arxiv.org/abs/2505.13988,,"Reinforcement finetuning (RFT) has become a standard approach for enhancing thereasoning capabilities of large language models (LLMs). However, its impact on model¬†‚Ä¶",8.0,Methodology
HalluLens,HalluLens: LLM Hallucination Benchmark,AbstentionBench: Reasoning LLMs Fail on Unanswerable Questions,"P Kirichenko,M Ibrahim,K Chaudhuri‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2025",arxiv.org,,https://arxiv.org/abs/2506.09038,,"For Large Language Models (LLMs) to be reliably deployed in both everyday and high-stakes domains, knowing when not to answer is equally critical as answering correctly. Real¬†‚Ä¶",13.0,Benchmark
HalluLens,HalluLens: LLM Hallucination Benchmark,"The risks of ai-generated, hyper-personalized digital advertisements","A LeBrun- Philosophy & Technology, 2025",Springer,,https://link.springer.com/article/10.1007/s13347-025-00935-z,,"Generative AI is set to transform digital advertising, which remains the revenue juggernat formuch of the internet. In the current model, advertisers use personal data to target users with¬†‚Ä¶",2.0,Other
HalluLens,HalluLens: LLM Hallucination Benchmark,MIRAGE-Bench: LLM Agent is Hallucinating and Where to Find Them,"W Zhang,Y Sun, P Huang, J Pu, H Lin‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2025",arxiv.org,,https://arxiv.org/abs/2507.21017,,"Hallucinations pose critical risks for large language model (LLM)-based agents, oftenmanifesting as hallucinative actions resulting from fabricated or misinterpreted information¬†‚Ä¶",1.0,Other
HalluLens,HalluLens: LLM Hallucination Benchmark,Introducing the Swiss Food Knowledge Graph: AI for Context-Aware Nutrition Recommendation,"LA Rahman,I Papathanail,S Mougiakakou- arXiv preprint arXiv¬†‚Ä¶, 2025",arxiv.org,,https://arxiv.org/abs/2507.10156,,"AI has driven significant progress in the nutrition field, especially through multimedia-basedautomatic dietary assessment. However, existing automatic dietary assessment systems¬†‚Ä¶",1.0,Other
FreshQA,FreshLLMs: Refreshing Large Language Models with Search Engine Augmentation,A survey on evaluation of large language models,"Y Chang,X Wang,J Wang,Y Wu,L Yang‚Ä¶¬†- ACM transactions on¬†‚Ä¶, 2024",dl.acm.org,,https://dl.acm.org/doi/abs/10.1145/3641289,,"Large language models (LLMs) are gaining increasing popularity in both academia andindustry, owing to their unprecedented performance in various applications. As LLMs¬†‚Ä¶",4405.0,Survey
FreshQA,FreshLLMs: Refreshing Large Language Models with Search Engine Augmentation,Combating misinformation in the age of llms: Opportunities and challenges,"C Chen,K Shu- AI Magazine, 2024",Wiley Online Library,,https://onlinelibrary.wiley.com/doi/abs/10.1002/aaai.12188,,Misinformation such as fake news and rumors is a serious threat for information ecosystemsand public trust. The emergence of large language models (LLMs) has great potential to¬†‚Ä¶,279.0,Other
FreshQA,FreshLLMs: Refreshing Large Language Models with Search Engine Augmentation,"A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions","L Huang,W Yu,W Ma,W Zhong,Z Feng‚Ä¶¬†- ACM Transactions on¬†‚Ä¶, 2025",dl.acm.org,,https://dl.acm.org/doi/abs/10.1145/3703155,,"The emergence of large language models (LLMs) has marked a significant breakthrough innatural language processing (NLP), fueling a paradigm shift in information acquisition¬†‚Ä¶",3037.0,Survey
FreshQA,FreshLLMs: Refreshing Large Language Models with Search Engine Augmentation,Hallucination is inevitable: An innate limitation of large language models,"Z Xu,S Jain,M Kankanhalli- arXiv preprint arXiv:2401.11817, 2024",arxiv.org,,https://arxiv.org/abs/2401.11817,,Hallucination has been widely recognized to be a significant drawback for large languagemodels (LLMs). There have been many works that attempt to reduce the extent of¬†‚Ä¶,723.0,Other
FreshQA,FreshLLMs: Refreshing Large Language Models with Search Engine Augmentation,Trustllm: Trustworthiness in large language models,"Y Huang,L Sun,H Wang, S Wu,Q Zhang, Y Li‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2024",arxiv.org,,https://arxiv.org/abs/2401.05561,,"Large language models (LLMs), exemplified by ChatGPT, have gained considerableattention for their excellent natural language processing capabilities. Nonetheless, these¬†‚Ä¶",487.0,Methodology
FreshQA,FreshLLMs: Refreshing Large Language Models with Search Engine Augmentation,Large language models for information retrieval: A survey,"Y Zhu,H Yuan,S Wang,J Liu,W Liu,C Deng‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2023",arxiv.org,,https://arxiv.org/abs/2308.07107,,"As a primary means of information acquisition, information retrieval (IR) systems, such assearch engines, have integrated themselves into our daily lives. These systems also serve¬†‚Ä¶",585.0,Survey
FreshQA,FreshLLMs: Refreshing Large Language Models with Search Engine Augmentation,Raft: Adapting language model to domain specific rag,"T Zhang,SG Patil,N Jain,S Shen,M Zaharia‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2024",arxiv.org,,https://arxiv.org/abs/2403.10131,,"Pretraining Large Language Models (LLMs) on large corpora of textual data is now astandard paradigm. When using these LLMs for many downstream applications, it is¬†‚Ä¶",296.0,Methodology
FreshQA,FreshLLMs: Refreshing Large Language Models with Search Engine Augmentation,Long-form factuality in large language models,"J Wei,C Yang,X Song,Y Lu,N Hu‚Ä¶¬†- Advances in¬†‚Ä¶, 2024",proceedings.neurips.cc,,https://proceedings.neurips.cc/paper_files/paper/2024/hash/937ae0e83eb08d2cb8627fe1def8c751-Abstract-Conference.html,,Large language models (LLMs) often generate content that contains factual errors whenresponding to fact-seeking prompts on open-ended topics. To benchmark a model's long¬†‚Ä¶,147.0,Benchmark
FreshQA,FreshLLMs: Refreshing Large Language Models with Search Engine Augmentation,"Survey on factuality in large language models: Knowledge, retrieval and domain-specificity","C Wang,X Liu,Y Yue,X Tang,T Zhang‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2023",arxiv.org,,https://arxiv.org/abs/2310.07521,,"This survey addresses the crucial issue of factuality in Large Language Models (LLMs). AsLLMs find applications across diverse domains, the reliability and accuracy of their outputs¬†‚Ä¶",281.0,Survey
FreshQA,FreshLLMs: Refreshing Large Language Models with Search Engine Augmentation,Knowledge conflicts for llms: A survey,"R Xu,Z Qi,Z Guo,C Wang,H Wang,Y Zhang‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2024",arxiv.org,,https://arxiv.org/abs/2403.08319,,"This survey provides an in-depth analysis of knowledge conflicts for large language models(LLMs), highlighting the complex challenges they encounter when blending contextual and¬†‚Ä¶",188.0,Survey
ETHICS,Aligning AI With Shared Human Values,A survey on evaluation of large language models,"Y Chang,X Wang,J Wang,Y Wu,L Yang‚Ä¶¬†- ACM transactions on¬†‚Ä¶, 2024",dl.acm.org,,https://dl.acm.org/doi/abs/10.1145/3641289,,"Large language models (LLMs) are gaining increasing popularity in both academia andindustry, owing to their unprecedented performance in various applications. As LLMs¬†‚Ä¶",4405.0,Survey
ETHICS,Aligning AI With Shared Human Values,Challenges and applications of large language models,"J Kaddour,J Harris,M Mozes,H Bradley‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2023",arxiv.org,,https://arxiv.org/abs/2307.10169,,"Large Language Models (LLMs) went from non-existent to ubiquitous in the machinelearning discourse within a few years. Due to the fast pace of the field, it is difficult to identify¬†‚Ä¶",768.0,Survey
ETHICS,Aligning AI With Shared Human Values,Universal and transferable adversarial attacks on aligned language models,"A Zou,Z Wang,N Carlini,M Nasr,JZ Kolter‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2023",arxiv.org,,https://arxiv.org/abs/2307.15043,,"Because"" out-of-the-box"" large language models are capable of generating a great deal ofobjectionable content, recent work has focused on aligning these models in an attempt to¬†‚Ä¶",2181.0,Methodology
ETHICS,Aligning AI With Shared Human Values,"Connecting the dots in trustworthy Artificial Intelligence: From AI principles, ethics, and key requirements to responsible AI systems and regulation","N D√≠az-Rodr√≠guez,J Del Ser,M Coeckelbergh‚Ä¶¬†- Information¬†‚Ä¶, 2023",Elsevier,,https://www.sciencedirect.com/science/article/pii/S1566253523002129,,Abstract Trustworthy Artificial Intelligence (AI) is based on seven technical requirementssustained over three main pillars that should be met throughout the system's entire life cycle¬†‚Ä¶,851.0,Survey
ETHICS,Aligning AI With Shared Human Values,Can large language models be an alternative to human evaluations?,"CH Chiang,H Lee- arXiv preprint arXiv:2305.01937, 2023",arxiv.org,,https://arxiv.org/abs/2305.01937,,"Human evaluation is indispensable and inevitable for assessing the quality of textsgenerated by machine learning models or written by humans. However, human evaluation is¬†‚Ä¶",848.0,Other
ETHICS,Aligning AI With Shared Human Values,Beyond the imitation game: Quantifying and extrapolating the capabilities of language models,"A Srivastava, A Rastogi, A Rao,AAM Shoeb‚Ä¶¬†- ‚Ä¶¬†on machine learning¬†‚Ä¶, 2023",openreview.net,,https://openreview.net/forum?id=uyTL5Bvosj&nesting=2&sort=date-desc,,"Language models demonstrate both quantitative improvement and new qualitativecapabilities with increasing scale. Despite their potentially transformative impact, these new¬†‚Ä¶",2020.0,Methodology
ETHICS,Aligning AI With Shared Human Values,DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models.,"B Wang,W Chen,H Pei,C Xie,M Kang,C Zhang,C Xu‚Ä¶¬†- NeurIPS, 2023",blogs.qub.ac.uk,,https://blogs.qub.ac.uk/wp-content/uploads/sites/7/2024/01/A-comprehensive-Assessment-of-Trustworthiness-in-GPT-Models.pdf,,"Abstract Generative Pre-trained Transformer (GPT) models have exhibited exciting progressin their capabilities, capturing the interest of practitioners and the public alike. Yet, while the¬†‚Ä¶",625.0,Survey
ETHICS,Aligning AI With Shared Human Values,Tree of attacks: Jailbreaking black-box llms automatically,"A Mehrotra,M Zampetakis‚Ä¶¬†- Advances in¬†‚Ä¶, 2024",proceedings.neurips.cc,,https://proceedings.neurips.cc/paper_files/paper/2024/hash/70702e8cbb4890b4a467b984ae59828a-Abstract-Conference.html,,"Abstract While Large Language Models (LLMs) display versatile functionality, they continueto generate harmful, biased, and toxic content, as demonstrated by the prevalence of human¬†‚Ä¶",424.0,Methodology
ETHICS,Aligning AI With Shared Human Values,Scaling and evaluating sparse autoencoders,"L Gao,TD la Tour, H Tillman,G Goh, R Troll‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2024",arxiv.org,,https://arxiv.org/abs/2406.04093,,Sparse autoencoders provide a promising unsupervised approach for extractinginterpretable features from a language model by reconstructing activations from a sparse¬†‚Ä¶,303.0,Methodology
ETHICS,Aligning AI With Shared Human Values,Trustllm: Trustworthiness in large language models,"Y Huang,L Sun,H Wang, S Wu,Q Zhang, Y Li‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2024",arxiv.org,,https://arxiv.org/abs/2401.05561,,"Large language models (LLMs), exemplified by ChatGPT, have gained considerableattention for their excellent natural language processing capabilities. Nonetheless, these¬†‚Ä¶",487.0,Methodology
Moral Stories,Moral Stories: Situated Reasoning about Norms Intents Actions and their Consequences,Ai alignment: A comprehensive survey,"J Ji,T Qiu,B Chen, B Zhang,H Lou, K Wang‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2023",arxiv.org,,https://arxiv.org/abs/2310.19852,,"AI alignment aims to make AI systems behave in line with human intentions and values. AsAI systems grow more capable, so do risks from misalignment. To provide a comprehensive¬†‚Ä¶",459.0,Survey
Moral Stories,Moral Stories: Situated Reasoning about Norms Intents Actions and their Consequences,"Natural language reasoning, a survey","F Yu,H Zhang,P Tiwari,B Wang- ACM Computing Surveys, 2024",dl.acm.org,,https://dl.acm.org/doi/abs/10.1145/3664194,,"This survey article proposes a clearer view of Natural Language Reasoning (NLR) in thefield of Natural Language Processing (NLP), both conceptually and practically¬†‚Ä¶",161.0,Survey
Moral Stories,Moral Stories: Situated Reasoning about Norms Intents Actions and their Consequences,DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models.,"B Wang,W Chen,H Pei,C Xie,M Kang,C Zhang,C Xu‚Ä¶¬†- NeurIPS, 2023",blogs.qub.ac.uk,,https://blogs.qub.ac.uk/wp-content/uploads/sites/7/2024/01/A-comprehensive-Assessment-of-Trustworthiness-in-GPT-Models.pdf,,"Abstract Generative Pre-trained Transformer (GPT) models have exhibited exciting progressin their capabilities, capturing the interest of practitioners and the public alike. Yet, while the¬†‚Ä¶",625.0,Survey
Moral Stories,Moral Stories: Situated Reasoning about Norms Intents Actions and their Consequences,From pretraining data to language models to downstream tasks: Tracking the trails of political biases leading to unfair NLP models,"S Feng,CY Park,Y Liu,Y Tsvetkov- arXiv preprint arXiv:2305.08283, 2023",arxiv.org,,https://arxiv.org/abs/2305.08283,,"Language models (LMs) are pretrained on diverse data sources, including news, discussionforums, books, and online encyclopedias. A significant portion of this data includes opinions¬†‚Ä¶",373.0,Survey
Moral Stories,Moral Stories: Situated Reasoning about Norms Intents Actions and their Consequences,Evaluating the moral beliefs encoded in llms,"N Scherrer,C Shi,A Feder‚Ä¶¬†- Advances in Neural¬†‚Ä¶, 2023",proceedings.neurips.cc,,https://proceedings.neurips.cc/paper_files/paper/2023/hash/a2cf225ba392627529efef14dc857e22-Abstract-Conference.html,,"This paper presents a case study on the design, administration, post-processing, andevaluation of surveys on large language models (LLMs). It comprises two components:(1) A¬†‚Ä¶",217.0,Survey
Moral Stories,Moral Stories: Situated Reasoning about Norms Intents Actions and their Consequences,Refiner: Reasoning feedback on intermediate representations,"D Paul,M Ismayilzada,M Peyrard,B Borges‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2023",arxiv.org,,https://arxiv.org/abs/2304.01904,,"Language models (LMs) have recently shown remarkable performance on reasoning tasksby explicitly generating intermediate inferences, eg, chain-of-thought prompting. However¬†‚Ä¶",242.0,Methodology
Moral Stories,Moral Stories: Situated Reasoning about Norms Intents Actions and their Consequences,Training socially aligned language models in simulated human society,"R Liu,R Yang,C Jia,G Zhang,D Zhou‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2023",proceedings.iclr.cc,,https://proceedings.iclr.cc/paper_files/paper/2024/file/d763b4a2dde0ae7b77498516ce9f439e-Paper-Conference.pdf,,"Social alignment in AI systems aims to ensure that these models behave according toestablished societal values. However, unlike humans, who derive consensus on value¬†‚Ä¶",142.0,Methodology
Moral Stories,Moral Stories: Situated Reasoning about Norms Intents Actions and their Consequences,When to make exceptions: Exploring language models as accounts of human moral judgment,"Z Jin,S Levine,F Gonzalez Adauto‚Ä¶¬†- Advances in neural¬†‚Ä¶, 2022",proceedings.neurips.cc,,https://proceedings.neurips.cc/paper_files/paper/2022/hash/b654d6150630a5ba5df7a55621390daf-Abstract-Conference.html,,"AI systems are becoming increasingly intertwined with human life. In order to effectivelycollaborate with humans and ensure safety, AI systems need to be able to understand¬†‚Ä¶",135.0,Other
Moral Stories,Moral Stories: Situated Reasoning about Norms Intents Actions and their Consequences,Knowledge of cultural moral norms in large language models,"A Ramezani,Y Xu- arXiv preprint arXiv:2306.01857, 2023",arxiv.org,,https://arxiv.org/abs/2306.01857,,"Moral norms vary across cultures. A recent line of work suggests that English large languagemodels contain human-like moral biases, but these studies typically do not examine moral¬†‚Ä¶",121.0,Other
Moral Stories,Moral Stories: Situated Reasoning about Norms Intents Actions and their Consequences,Culturally aware and adapted nlp: A taxonomy and a survey of the state of the art,"CC Liu,I Gurevych,A Korhonen- Transactions of the Association for¬†‚Ä¶, 2025",direct.mit.edu,,https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00760/131587,,"The surge of interest in culture in NLP has inspired much recent research, but a sharedunderstanding of ‚Äúculture‚Äù remains unclear, making it difficult to evaluate progress in this¬†‚Ä¶",48.0,Survey
Jiminy Cricket,What Would Jiminy Cricket Do? Towards Agents That Behave Morally,Safetyprompts: a systematic review of open datasets for evaluating and improving large language model safety,"P R√∂ttger,F Pernisi,B Vidgen,D Hovy- Proceedings of the AAAI¬†‚Ä¶, 2025",ojs.aaai.org,,https://ojs.aaai.org/index.php/AAAI/article/view/34975,,The last two years have seen a rapid growth in concerns around the safety of largelanguage models (LLMs). Researchers and practitioners have met these concerns by¬†‚Ä¶,52.0,Survey
Jiminy Cricket,What Would Jiminy Cricket Do? Towards Agents That Behave Morally,DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models.,"B Wang,W Chen,H Pei,C Xie,M Kang,C Zhang,C Xu‚Ä¶¬†- NeurIPS, 2023",blogs.qub.ac.uk,,https://blogs.qub.ac.uk/wp-content/uploads/sites/7/2024/01/A-comprehensive-Assessment-of-Trustworthiness-in-GPT-Models.pdf,,"Abstract Generative Pre-trained Transformer (GPT) models have exhibited exciting progressin their capabilities, capturing the interest of practitioners and the public alike. Yet, while the¬†‚Ä¶",625.0,Survey
Jiminy Cricket,What Would Jiminy Cricket Do? Towards Agents That Behave Morally,Trustllm: Trustworthiness in large language models,"Y Huang,L Sun,H Wang, S Wu,Q Zhang, Y Li‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2024",arxiv.org,,https://arxiv.org/abs/2401.05561,,"Large language models (LLMs), exemplified by ChatGPT, have gained considerableattention for their excellent natural language processing capabilities. Nonetheless, these¬†‚Ä¶",487.0,Methodology
Jiminy Cricket,What Would Jiminy Cricket Do? Towards Agents That Behave Morally,Representation engineering: A top-down approach to ai transparency,"A Zou,L Phan,S Chen,J Campbell,P Guo‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2023",arxiv.org,,https://arxiv.org/abs/2310.01405,,"In this paper, we identify and characterize the emerging area of representation engineering(RepE), an approach to enhancing the transparency of AI systems that draws on insights¬†‚Ä¶",567.0,Survey
Jiminy Cricket,What Would Jiminy Cricket Do? Towards Agents That Behave Morally,Do the rewards justify the means? measuring trade-offs between rewards and ethical behavior in the machiavelli benchmark,"A Pan,JS Chan,A Zou,N Li,S Basart‚Ä¶¬†- International¬†‚Ä¶, 2023",proceedings.mlr.press,,https://proceedings.mlr.press/v202/pan23a.html,,"Artificial agents have traditionally been trained to maximize reward, which may incentivizepower-seeking and deception, analogous to how next-token prediction in language models¬†‚Ä¶",199.0,Benchmark
Jiminy Cricket,What Would Jiminy Cricket Do? Towards Agents That Behave Morally,Unsolved problems in ml safety,"D Hendrycks,N Carlini,J Schulman‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2021",arxiv.org,,https://arxiv.org/abs/2109.13916,,"Machine learning (ML) systems are rapidly increasing in size, are acquiring newcapabilities, and are increasingly deployed in high-stakes settings. As with other powerful¬†‚Ä¶",459.0,Survey
Jiminy Cricket,What Would Jiminy Cricket Do? Towards Agents That Behave Morally,Position: Trustllm: Trustworthiness in large language models,"Y Huang,L Sun,H Wang, S Wu‚Ä¶¬†- International¬†‚Ä¶, 2024",proceedings.mlr.press,,http://proceedings.mlr.press/v235/huang24x.html,,"Large language models (LLMs) have gained considerable attention for their excellentnatural language processing capabilities. Nonetheless, these LLMs present many¬†‚Ä¶",88.0,Methodology
Jiminy Cricket,What Would Jiminy Cricket Do? Towards Agents That Behave Morally,Open-Ethical AI: Advancements in Open-Source Human-Centric Neural Language Models,"S Sicari,JF Cevallos M,A Rizzardi‚Ä¶¬†- ACM Computing¬†‚Ä¶, 2024",dl.acm.org,,https://dl.acm.org/doi/abs/10.1145/3703454,,"This survey summarises the most recent methods for building and assessing helpful, honest,and harmless neural language models, considering small, medium, and large-size models¬†‚Ä¶",9.0,Survey
Jiminy Cricket,What Would Jiminy Cricket Do? Towards Agents That Behave Morally,Multi-turn reinforcement learning with preference human feedback,"L Shani,A Rosenberg,A Cassel‚Ä¶¬†- Advances in¬†‚Ä¶, 2024",proceedings.neurips.cc,,https://proceedings.neurips.cc/paper_files/paper/2024/hash/d77a7b289361abff82bdd2fb537ae152-Abstract-Conference.html,,"Abstract Reinforcement Learning from Human Feedback (RLHF) has become the standardapproach for aligning Large Language Models (LLMs) with human preferences, allowing¬†‚Ä¶",51.0,Methodology
Jiminy Cricket,What Would Jiminy Cricket Do? Towards Agents That Behave Morally,Moca: Measuring human-language model alignment on causal and moral judgment tasks,"A Nie,Y Zhang,AS Amdekar,C Piech‚Ä¶¬†- Advances in¬†‚Ä¶, 2023",proceedings.neurips.cc,,https://proceedings.neurips.cc/paper_files/paper/2023/hash/f751c6f8bfb52c60f43942896fe65904-Abstract-Conference.html,,Human commonsense understanding of the physical and social world is organized aroundintuitive theories. These theories support making causal and moral judgments. When¬†‚Ä¶,69.0,Methodology
SCRUPLES,Scruples: A Corpus of Community Ethical Judgments on 32000 Real-Life Anecdotes,Evaluating large language models: A comprehensive survey,"Z Guo,R Jin,C Liu,Y Huang,D Shi, L Yu, Y Liu‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2023",arxiv.org,,https://arxiv.org/abs/2310.19736,,Large language models (LLMs) have demonstrated remarkable capabilities across a broadspectrum of tasks. They have attracted significant attention and been deployed in numerous¬†‚Ä¶,237.0,Survey
SCRUPLES,Scruples: A Corpus of Community Ethical Judgments on 32000 Real-Life Anecdotes,"Navigating llm ethics: Advancements, challenges, and future directions","J Jiao,S Afroogh,Y Xu,C Phillips- AI and Ethics, 2025",Springer,,https://link.springer.com/article/10.1007/s43681-025-00814-5,,This study addresses ethical issues surrounding Large Language Models (LLMs) within thefield of artificial intelligence. It explores the common ethical challenges posed by both LLMs¬†‚Ä¶,91.0,Survey
SCRUPLES,Scruples: A Corpus of Community Ethical Judgments on 32000 Real-Life Anecdotes,Whose opinions do language models reflect?,"S Santurkar,E Durmus,F Ladhak‚Ä¶¬†- International¬†‚Ä¶, 2023",proceedings.mlr.press,,https://proceedings.mlr.press/v202/santurkar23a?utm_source=chatgpt.com,,"Abstract Language models (LMs) are increasingly being used in open-ended contexts,where the opinions they reflect in response to subjective queries can have a profound¬†‚Ä¶",693.0,Other
SCRUPLES,Scruples: A Corpus of Community Ethical Judgments on 32000 Real-Life Anecdotes,Evaluating the moral beliefs encoded in llms,"N Scherrer,C Shi,A Feder‚Ä¶¬†- Advances in Neural¬†‚Ä¶, 2023",proceedings.neurips.cc,,https://proceedings.neurips.cc/paper_files/paper/2023/hash/a2cf225ba392627529efef14dc857e22-Abstract-Conference.html,,"This paper presents a case study on the design, administration, post-processing, andevaluation of surveys on large language models (LLMs). It comprises two components:(1) A¬†‚Ä¶",217.0,Survey
SCRUPLES,Scruples: A Corpus of Community Ethical Judgments on 32000 Real-Life Anecdotes,Large language model alignment: A survey,"T Shen,R Jin,Y Huang,C Liu,W Dong, Z Guo‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2023",arxiv.org,,https://arxiv.org/abs/2309.15025,,"Recent years have witnessed remarkable progress made in large language models (LLMs).Such advancements, while garnering significant attention, have concurrently elicited various¬†‚Ä¶",289.0,Survey
SCRUPLES,Scruples: A Corpus of Community Ethical Judgments on 32000 Real-Life Anecdotes,When to make exceptions: Exploring language models as accounts of human moral judgment,"Z Jin,S Levine,F Gonzalez Adauto‚Ä¶¬†- Advances in neural¬†‚Ä¶, 2022",proceedings.neurips.cc,,https://proceedings.neurips.cc/paper_files/paper/2022/hash/b654d6150630a5ba5df7a55621390daf-Abstract-Conference.html,,"AI systems are becoming increasingly intertwined with human life. In order to effectivelycollaborate with humans and ensure safety, AI systems need to be able to understand¬†‚Ä¶",135.0,Other
SCRUPLES,Scruples: A Corpus of Community Ethical Judgments on 32000 Real-Life Anecdotes,Knowledge of cultural moral norms in large language models,"A Ramezani,Y Xu- arXiv preprint arXiv:2306.01857, 2023",arxiv.org,,https://arxiv.org/abs/2306.01857,,"Moral norms vary across cultures. A recent line of work suggests that English large languagemodels contain human-like moral biases, but these studies typically do not examine moral¬†‚Ä¶",121.0,Survey
SCRUPLES,Scruples: A Corpus of Community Ethical Judgments on 32000 Real-Life Anecdotes,Moca: Measuring human-language model alignment on causal and moral judgment tasks,"A Nie,Y Zhang,AS Amdekar,C Piech‚Ä¶¬†- Advances in¬†‚Ä¶, 2023",proceedings.neurips.cc,,https://proceedings.neurips.cc/paper_files/paper/2023/hash/f751c6f8bfb52c60f43942896fe65904-Abstract-Conference.html,,Human commonsense understanding of the physical and social world is organized aroundintuitive theories. These theories support making causal and moral judgments. When¬†‚Ä¶,69.0,Methodology
SCRUPLES,Scruples: A Corpus of Community Ethical Judgments on 32000 Real-Life Anecdotes,Safetyprompts: a systematic review of open datasets for evaluating and improving large language model safety,"P R√∂ttger,F Pernisi,B Vidgen,D Hovy- Proceedings of the AAAI¬†‚Ä¶, 2025",ojs.aaai.org,,https://ojs.aaai.org/index.php/AAAI/article/view/34975,,The last two years have seen a rapid growth in concerns around the safety of largelanguage models (LLMs). Researchers and practitioners have met these concerns by¬†‚Ä¶,52.0,Survey
SCRUPLES,Scruples: A Corpus of Community Ethical Judgments on 32000 Real-Life Anecdotes,Safetybench: Evaluating the safety of large language models,"Z Zhang,L Lei, L Wu, R Sun,Y Huang, C Long‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2023",arxiv.org,,https://arxiv.org/abs/2309.07045,,"With the rapid development of Large Language Models (LLMs), increasing attention hasbeen paid to their safety concerns. Consequently, evaluating the safety of LLMs has become¬†‚Ä¶",108.0,Benchmark
MoralBench,MoralBench: Moral Evaluation of LLMs,"Large language model psychometrics: A systematic review of evaluation, validation, and enhancement","H Ye, J Jin,Y Xie, X Zhang,G Song- arXiv preprint arXiv:2505.08245, 2025",arxiv.org,,https://arxiv.org/abs/2505.08245,,"The rapid advancement of large language models (LLMs) has outpaced traditionalevaluation methodologies. It presents novel challenges, such as measuring human-like¬†‚Ä¶",12.0,Survey
MoralBench,MoralBench: Moral Evaluation of LLMs,Recommender systems meet large language model agents: A survey,"X Zhu,Y Wang,H Gao,W Xu,C Wang‚Ä¶¬†- ‚Ä¶¬†and Trends¬Æ in¬†‚Ä¶, 2025",nowpublishers.com,,https://www.nowpublishers.com/article/Details/SEC-050,,"In recent years, the integration of Large Language Models (LLMs) and RecommenderSystems (RS) has revolutionized the way personalized and intelligent user experiences are¬†‚Ä¶",15.0,Survey
MoralBench,MoralBench: Moral Evaluation of LLMs,The pluralistic moral gap: Understanding judgment and value differences between humans and large language models,"G Russo,D Nozza,P R√∂ttger,D Hovy- arXiv preprint arXiv:2507.17216, 2025",arxiv.org,,https://arxiv.org/abs/2507.17216,,"People increasingly rely on Large Language Models (LLMs) for moral advice, which mayinfluence humans' decisions. Yet, little is known about how closely LLMs align with human¬†‚Ä¶",5.0,Other
MoralBench,MoralBench: Moral Evaluation of LLMs,Normative evaluation of large language models with everyday moral dilemmas,"P Sachdeva, T van Nuenen¬†- Proceedings of the 2025 ACM Conference¬†‚Ä¶, 2025",dl.acm.org,,https://dl.acm.org/doi/abs/10.1145/3715275.3732044,,The rapid adoption of large language models (LLMs) has spurred extensive research intotheir encoded moral norms and decision-making processes. While prior work often¬†‚Ä¶,6.0,Survey
MoralBench,MoralBench: Moral Evaluation of LLMs,"Value compass benchmarks: A comprehensive, generative and self-evolving platform for llms' value evaluation","J Yao,X Yi,S Duan,J Wang,Y Bai‚Ä¶¬†- Proceedings of the¬†‚Ä¶, 2025",aclanthology.org,,https://aclanthology.org/2025.acl-demo.64/,,"As large language models (LLMs) are gradually integrated into human daily life, assessingtheir underlying values becomes essential for understanding their risks and alignment with¬†‚Ä¶",3.0,Benchmark
MoralBench,MoralBench: Moral Evaluation of LLMs,Benchmarking and advancing large language models for local life services,"X Lan,J Feng, J Lei, X Shi, Y Li¬†- Proceedings of the 31st ACM SIGKDD¬†‚Ä¶, 2025",dl.acm.org,,https://dl.acm.org/doi/abs/10.1145/3711896.3737196,,"Large language models (LLMs) have exhibited remarkable capabilities and achievedsignificant breakthroughs across various domains, leading to their widespread adoption in¬†‚Ä¶",4.0,Benchmark
MoralBench,MoralBench: Moral Evaluation of LLMs,MoralBench: A MultiModal Moral Benchmark for LVLMs,"B Yan,J Zhang, Z Chen,S Shan, X Chen¬†- arXiv preprint arXiv¬†‚Ä¶, 2024",arxiv.org,,https://arxiv.org/abs/2412.20718,,"Recently, large foundation models, including large language models (LLMs) and largevision-language models (LVLMs), have become essential tools in critical fields such as law¬†‚Ä¶",7.0,Benchmark
MoralBench,MoralBench: Moral Evaluation of LLMs,Whose morality do they speak? Unraveling cultural bias in multilingual language models,"M Aksoy- Natural Language Processing Journal, 2025",Elsevier,,https://www.sciencedirect.com/science/article/pii/S2949719125000482,,"Large language models (LLMs) have become integral tools in diverse domains, yet theirmoral reasoning capabilities across cultural and linguistic contexts remain underexplored¬†‚Ä¶",9.0,Survey
MoralBench,MoralBench: Moral Evaluation of LLMs,Large Language Models meet moral values: A comprehensive assessment of moral abilities,"L Bulla,S De Giorgis,M Mongiov√¨‚Ä¶¬†- Computers in Human¬†‚Ä¶, 2025",Elsevier,,https://www.sciencedirect.com/science/article/pii/S2451958825000247,,"Automatic moral classification in textual data is crucial for various fields including NaturalLanguage Processing (NLP), social sciences, and ethical AI development. Despite¬†‚Ä¶",10.0,Survey
MoralBench,MoralBench: Moral Evaluation of LLMs,SafeDialBench: A Fine-Grained Safety Benchmark for Large Language Models in Multi-Turn Dialogues with Diverse Jailbreak Attacks,"H Cao, Y Wang, S Jing, Z Peng, Z Bai, Z Cao‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2025",arxiv.org,,https://arxiv.org/abs/2502.11090,,"With the rapid advancement of Large Language Models (LLMs), the safety of LLMs hasbeen a critical concern requiring precise assessment. Current benchmarks primarily¬†‚Ä¶",5.0,Benchmark
Social Chemistry 101,Social Chemistry 101: Learning to Reason about Social and Moral Norms,Ai alignment: A comprehensive survey,"J Ji,T Qiu,B Chen, B Zhang,H Lou, K Wang‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2023",arxiv.org,,https://arxiv.org/abs/2310.19852,,"AI alignment aims to make AI systems behave in line with human intentions and values. AsAI systems grow more capable, so do risks from misalignment. To provide a comprehensive¬†‚Ä¶",459.0,Survey
Social Chemistry 101,Social Chemistry 101: Learning to Reason about Social and Moral Norms,Evaluating large language models: A comprehensive survey,"Z Guo,R Jin,C Liu,Y Huang,D Shi, L Yu, Y Liu‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2023",arxiv.org,,https://arxiv.org/abs/2310.19736,,Large language models (LLMs) have demonstrated remarkable capabilities across a broadspectrum of tasks. They have attracted significant attention and been deployed in numerous¬†‚Ä¶,237.0,Survey
Social Chemistry 101,Social Chemistry 101: Learning to Reason about Social and Moral Norms,DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models.,"B Wang,W Chen,H Pei,C Xie,M Kang,C Zhang,C Xu‚Ä¶¬†- NeurIPS, 2023",blogs.qub.ac.uk,,https://blogs.qub.ac.uk/wp-content/uploads/sites/7/2024/01/A-comprehensive-Assessment-of-Trustworthiness-in-GPT-Models.pdf,,"Abstract Generative Pre-trained Transformer (GPT) models have exhibited exciting progressin their capabilities, capturing the interest of practitioners and the public alike. Yet, while the¬†‚Ä¶",625.0,Survey
Social Chemistry 101,Social Chemistry 101: Learning to Reason about Social and Moral Norms,Trustllm: Trustworthiness in large language models,"Y Huang,L Sun,H Wang, S Wu,Q Zhang, Y Li‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2024",arxiv.org,,https://arxiv.org/abs/2401.05561,,"Large language models (LLMs), exemplified by ChatGPT, have gained considerableattention for their excellent natural language processing capabilities. Nonetheless, these¬†‚Ä¶",487.0,Methodology
Social Chemistry 101,Social Chemistry 101: Learning to Reason about Social and Moral Norms,Evaluating the moral beliefs encoded in llms,"N Scherrer,C Shi,A Feder‚Ä¶¬†- Advances in Neural¬†‚Ä¶, 2023",proceedings.neurips.cc,,https://proceedings.neurips.cc/paper_files/paper/2023/hash/a2cf225ba392627529efef14dc857e22-Abstract-Conference.html,,"This paper presents a case study on the design, administration, post-processing, andevaluation of surveys on large language models (LLMs). It comprises two components:(1) A¬†‚Ä¶",217.0,Survey
Social Chemistry 101,Social Chemistry 101: Learning to Reason about Social and Moral Norms,Large language model alignment: A survey,"T Shen,R Jin,Y Huang,C Liu,W Dong, Z Guo‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2023",arxiv.org,,https://arxiv.org/abs/2309.15025,,"Recent years have witnessed remarkable progress made in large language models (LLMs).Such advancements, while garnering significant attention, have concurrently elicited various¬†‚Ä¶",289.0,Survey
Social Chemistry 101,Social Chemistry 101: Learning to Reason about Social and Moral Norms,Position: Trustllm: Trustworthiness in large language models,"Y Huang,L Sun,H Wang, S Wu‚Ä¶¬†- International¬†‚Ä¶, 2024",proceedings.mlr.press,,http://proceedings.mlr.press/v235/huang24x.html,,"Large language models (LLMs) have gained considerable attention for their excellentnatural language processing capabilities. Nonetheless, these LLMs present many¬†‚Ä¶",88.0,Methodology
Social Chemistry 101,Social Chemistry 101: Learning to Reason about Social and Moral Norms,"Natural language reasoning, a survey","F Yu,H Zhang,P Tiwari,B Wang- ACM Computing Surveys, 2024",dl.acm.org,,https://dl.acm.org/doi/abs/10.1145/3664194,,"This survey article proposes a clearer view of Natural Language Reasoning (NLR) in thefield of Natural Language Processing (NLP), both conceptually and practically¬†‚Ä¶",161.0,Survey
Social Chemistry 101,Social Chemistry 101: Learning to Reason about Social and Moral Norms,A survey on fairness in large language models,"Y Li,M Du,R Song,X Wang,Y Wang- arXiv preprint arXiv:2308.10149, 2023",arxiv.org,,https://arxiv.org/abs/2308.10149,,"Large Language Models (LLMs) have shown powerful performance and developmentprospects and are widely deployed in the real world. However, LLMs can capture social¬†‚Ä¶",166.0,Survey
Social Chemistry 101,Social Chemistry 101: Learning to Reason about Social and Moral Norms,Foundation metrics for evaluating effectiveness of healthcare conversations powered by generative AI,"M Abbasian,E Khatibi,I Azimi,D Oniani‚Ä¶¬†- NPJ Digital¬†‚Ä¶, 2024",nature.com,,https://www.nature.com/articles/s41746-024-01074-z,,"Abstract Generative Artificial Intelligence is set to revolutionize healthcare delivery bytransforming traditional patient care into a more personalized, efficient, and proactive¬†‚Ä¶",121.0,Other
Delphi,Delphi: Towards Machine Ethics and Norms,Evaluating the moral beliefs encoded in llms,"N Scherrer,C Shi,A Feder‚Ä¶¬†- Advances in Neural¬†‚Ä¶, 2023",proceedings.neurips.cc,,https://proceedings.neurips.cc/paper_files/paper/2023/hash/a2cf225ba392627529efef14dc857e22-Abstract-Conference.html,,"This paper presents a case study on the design, administration, post-processing, andevaluation of surveys on large language models (LLMs). It comprises two components:(1) A¬†‚Ä¶",217.0,Survey
Delphi,Delphi: Towards Machine Ethics and Norms,The TESCREAL bundle: Eugenics and the promise of utopia through artificial general intelligence,"T Gebru, √âP Torres¬†- First Monday, 2024",firstmonday.org,,http://firstmonday.org/ojs/index.php/fm/article/view/13636,,"The stated goal of many organizations in the field of artificial intelligence (AI) is to developartificial general intelligence (AGI), an imagined system with more intelligence than anything¬†‚Ä¶",198.0,Other
Delphi,Delphi: Towards Machine Ethics and Norms,Collective constitutional ai: Aligning a language model with public input,"S Huang,D Siddarth, L Lovitt,TI Liao‚Ä¶¬†- Proceedings of the¬†‚Ä¶, 2024",dl.acm.org,,https://dl.acm.org/doi/abs/10.1145/3630106.3658979,,"There is growing consensus that language model (LM) developers should not be the soledeciders of LM behavior, creating a need for methods that enable the broader public to¬†‚Ä¶",102.0,Methodology
Delphi,Delphi: Towards Machine Ethics and Norms,Open-Ethical AI: Advancements in Open-Source Human-Centric Neural Language Models,"S Sicari,JF Cevallos M,A Rizzardi‚Ä¶¬†- ACM Computing¬†‚Ä¶, 2024",dl.acm.org,,https://dl.acm.org/doi/abs/10.1145/3703454,,"This survey summarises the most recent methods for building and assessing helpful, honest,and harmless neural language models, considering small, medium, and large-size models¬†‚Ä¶",9.0,Survey
Delphi,Delphi: Towards Machine Ethics and Norms,Personalisation within bounds: A risk taxonomy and policy framework for the alignment of large language models with personalised feedback,"HR Kirk,B Vidgen,P R√∂ttger,SA Hale- arXiv preprint arXiv:2303.05453, 2023",arxiv.org,,https://arxiv.org/abs/2303.05453,,"Large language models (LLMs) are used to generate content for a wide range of tasks, andare set to reach a growing audience in coming years due to integration in product interfaces¬†‚Ä¶",139.0,Other
Delphi,Delphi: Towards Machine Ethics and Norms,Socially intelligent machines that learn from humans and help humans learn,"H Gweon,J Fan,B Kim- Philosophical Transactions of¬†‚Ä¶, 2023",royalsocietypublishing.org,,https://royalsocietypublishing.org/doi/abs/10.1098/rsta.2022.0048,,A hallmark of human intelligence is the ability to understand and influence other minds.Humans engage in inferential social learning (ISL) by using commonsense psychology to¬†‚Ä¶,30.0,Other
Delphi,Delphi: Towards Machine Ethics and Norms,"Value kaleidoscope: Engaging ai with pluralistic human values, rights, and duties","T Sorensen,L Jiang,JD Hwang,S Levine‚Ä¶¬†- Proceedings of the¬†‚Ä¶, 2024",ojs.aaai.org,,https://ojs.aaai.org/index.php/AAAI/article/view/29970,,"Human values are crucial to human decision-making.\textit {Value pluralism} is the view thatmultiple correct values may be held in tension with one another (eg, when considering\textit¬†‚Ä¶",118.0,Other
Delphi,Delphi: Towards Machine Ethics and Norms,Unveiling the implicit toxicity in large language models,"J Wen,P Ke,H Sun,Z Zhang, C Li, J Bai‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2023",arxiv.org,,https://arxiv.org/abs/2311.17391,,The open-endedness of large language models (LLMs) combined with their impressivecapabilities may lead to new safety issues when being exploited for malicious use. While¬†‚Ä¶,109.0,Other
Delphi,Delphi: Towards Machine Ethics and Norms,Bridging the gap: A survey on integrating (human) feedback for natural language generation,"P Fernandes,A Madaan,E Liu,A Farinhas‚Ä¶¬†- Transactions of the¬†‚Ä¶, 2023",direct.mit.edu,,https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00626/118795,,"Natural language generation has witnessed significant advancements due to the training oflarge language models on vast internet-scale datasets. Despite these advancements, there¬†‚Ä¶",99.0,Survey
Delphi,Delphi: Towards Machine Ethics and Norms,NLPositionality: Characterizing design biases of datasets and models,"S Santy,JT Liang,RL Bras,K Reinecke‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2023",arxiv.org,,https://arxiv.org/abs/2306.01943,,"Design biases in NLP systems, such as performance differences for different populations,often stem from their creator's positionality, ie, views and lived experiences shaped by¬†‚Ä¶",103.0,Other
STORAL,A Corpus for Understanding and Generating Moral Stories,Open-world story generation with structured knowledge enhancement: A comprehensive survey,"Y Wang,J Lin,Z Yu,W Hu,BF Karlsson- Neurocomputing, 2023",Elsevier,,https://www.sciencedirect.com/science/article/pii/S0925231223009153,,"Storytelling and narrative are fundamental to human experience, intertwined with our socialand cultural engagement. As such, researchers have long attempted to create systems that¬†‚Ä¶",42.0,Survey
STORAL,A Corpus for Understanding and Generating Moral Stories,Slam-omni: Timbre-controllable voice interaction system with single-stage training,"W Chen,Z Ma,R Yan,Y Liang,X Li, R Xu,Z Niu‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2024",arxiv.org,,https://arxiv.org/abs/2412.15649,,"Recent advancements highlight the potential of end-to-end real-time spoken dialoguesystems, showcasing their low latency and high quality. In this paper, we introduce SLAM¬†‚Ä¶",36.0,Methodology
STORAL,A Corpus for Understanding and Generating Moral Stories,"Values, ethics, morals? on the use of moral concepts in NLP research","K Vida, J Simon,A Lauscher- arXiv preprint arXiv:2310.13915, 2023",arxiv.org,,https://arxiv.org/abs/2310.13915,,"With language technology increasingly affecting individuals' lives, many recent works haveinvestigated the ethical aspects of NLP. Among other topics, researchers focused on the¬†‚Ä¶",24.0,Survey
STORAL,A Corpus for Understanding and Generating Moral Stories,Bridging cultural nuances in dialogue agents through cultural value surveys,"Y Cao,M Chen,D Hershcovich- arXiv preprint arXiv:2401.10352, 2024",arxiv.org,,https://arxiv.org/abs/2401.10352,,The cultural landscape of interactions with dialogue agents is a compelling yet relativelyunexplored territory. It's clear that various sociocultural aspects--from communication styles¬†‚Ä¶,10.0,Survey
STORAL,A Corpus for Understanding and Generating Moral Stories,Uro-bench: A comprehensive benchmark for end-to-end spoken dialogue models,"R Yan,X Li,W Chen,Z Niu,C Yang,Z Ma,K Yu‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2025",arxiv.org,,https://arxiv.org/abs/2502.17810,,"In recent years, with advances in large language models (LLMs), end-to-end spokendialogue models (SDMs) have made significant strides. Compared to text-based LLMs, the¬†‚Ä¶",14.0,Benchmark
STORAL,A Corpus for Understanding and Generating Moral Stories,CULEMO: Cultural Lenses on Emotion--Benchmarking LLMs for Cross-Cultural Emotion Understanding,"TD Belay,AH Ahmed,A Grissom II,I Ameer‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2025",arxiv.org,,https://arxiv.org/abs/2503.10688,,"NLP research has increasingly focused on subjective tasks such as emotion analysis.However, existing emotion benchmarks suffer from two major shortcomings:(1) they largely¬†‚Ä¶",8.0,Benchmark
STORAL,A Corpus for Understanding and Generating Moral Stories,A survey on modelling morality for text analysis,"I Reinig,M Becker, I Rehbein,SP Ponzetto- 2024",madoc.bib.uni-mannheim.de,,https://madoc.bib.uni-mannheim.de/67689/,,"In this survey, we provide a systematic review of recent work on modelling morality in text, anarea of research that has garnered increasing attention in recent years. Our survey is¬†‚Ä¶",9.0,Survey
STORAL,A Corpus for Understanding and Generating Moral Stories,Bilingual Dialogue Dataset with Personality and Emotion Annotations for Personality Recognition in Education,"Z Liu, Y Xiao,Z Su,L Ye,K Lu, X Peng¬†- Scientific Data, 2025",nature.com,,https://www.nature.com/articles/s41597-025-04836-w,,"Dialogue datasets are essential for advancing natural language processing (NLP) tasks.However, many existing datasets lack integrated annotations for personality and emotion¬†‚Ä¶",1.0,Benchmark
STORAL,A Corpus for Understanding and Generating Moral Stories,MORABLES: A Benchmark for Assessing Abstract Moral Reasoning in LLMs with Fables,"M Marcuzzo,A Zangari,A Albarelli‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2025",arxiv.org,,https://arxiv.org/abs/2509.12371,,"As LLMs excel on standard reading comprehension benchmarks, attention is shifting towardevaluating their capacity for complex abstract reasoning and inference. Literature-based¬†‚Ä¶",,Benchmark
STORAL,A Corpus for Understanding and Generating Moral Stories,JETHICS: Japanese Ethics Understanding Evaluation Dataset,"M Takeshita,R Rzepka- arXiv preprint arXiv:2506.16187, 2025",arxiv.org,,https://arxiv.org/abs/2506.16187,,"In this work, we propose JETHICS, a Japanese dataset for evaluating ethics understandingof AI models. JETHICS contains 78K examples and is built by following the construction¬†‚Ä¶",,Benchmark
Moral Integrity Corpus,The Moral Integrity Corpus: A Benchmark for Ethical Dialogue Systems,Ai alignment: A comprehensive survey,"J Ji,T Qiu,B Chen, B Zhang,H Lou, K Wang‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2023",arxiv.org,,https://arxiv.org/abs/2310.19852,,"AI alignment aims to make AI systems behave in line with human intentions and values. AsAI systems grow more capable, so do risks from misalignment. To provide a comprehensive¬†‚Ä¶",459.0,Survey
Moral Integrity Corpus,The Moral Integrity Corpus: A Benchmark for Ethical Dialogue Systems,Evaluating large language models: A comprehensive survey,"Z Guo,R Jin,C Liu,Y Huang,D Shi, L Yu, Y Liu‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2023",arxiv.org,,https://arxiv.org/abs/2310.19736,,Large language models (LLMs) have demonstrated remarkable capabilities across a broadspectrum of tasks. They have attracted significant attention and been deployed in numerous¬†‚Ä¶,237.0,Survey
Moral Integrity Corpus,The Moral Integrity Corpus: A Benchmark for Ethical Dialogue Systems,Large language models in education: Vision and opportunities,"W Gan, Z Qi, J Wu, JCW Lin¬†- 2023 IEEE international¬†‚Ä¶, 2023",ieeexplore.ieee.org,,https://ieeexplore.ieee.org/abstract/document/10386291/,,"With the rapid development of artificial intelligence technology, large language models(LLMs) have become a hot research topic. Education plays an important role in human¬†‚Ä¶",202.0,Survey
Moral Integrity Corpus,The Moral Integrity Corpus: A Benchmark for Ethical Dialogue Systems,Training socially aligned language models in simulated human society,"R Liu,R Yang,C Jia,G Zhang,D Zhou‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2023",proceedings.iclr.cc,,https://proceedings.iclr.cc/paper_files/paper/2024/file/d763b4a2dde0ae7b77498516ce9f439e-Paper-Conference.pdf,,"Social alignment in AI systems aims to ensure that these models behave according toestablished societal values. However, unlike humans, who derive consensus on value¬†‚Ä¶",142.0,Methodology
Moral Integrity Corpus,The Moral Integrity Corpus: A Benchmark for Ethical Dialogue Systems,Personalisation within bounds: A risk taxonomy and policy framework for the alignment of large language models with personalised feedback,"HR Kirk,B Vidgen,P R√∂ttger,SA Hale- arXiv preprint arXiv:2303.05453, 2023",arxiv.org,,https://arxiv.org/abs/2303.05453,,"Large language models (LLMs) are used to generate content for a wide range of tasks, andare set to reach a growing audience in coming years due to integration in product interfaces¬†‚Ä¶",139.0,Other
Moral Integrity Corpus,The Moral Integrity Corpus: A Benchmark for Ethical Dialogue Systems,Mirages: On anthropomorphism in dialogue systems,"G Abercrombie,AC Curry,T Dinkar,V Rieser‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2023",arxiv.org,,https://arxiv.org/abs/2305.09800,,Automated dialogue or conversational systems are anthropomorphised by developers andpersonified by users. While a degree of anthropomorphism may be inevitable due to the¬†‚Ä¶,125.0,Other
Moral Integrity Corpus,The Moral Integrity Corpus: A Benchmark for Ethical Dialogue Systems,Culturally aware and adapted nlp: A taxonomy and a survey of the state of the art,"CC Liu,I Gurevych,A Korhonen- Transactions of the Association for¬†‚Ä¶, 2025",direct.mit.edu,,https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00760/131587,,"The surge of interest in culture in NLP has inspired much recent research, but a sharedunderstanding of ‚Äúculture‚Äù remains unclear, making it difficult to evaluate progress in this¬†‚Ä¶",48.0,Survey
Moral Integrity Corpus,The Moral Integrity Corpus: A Benchmark for Ethical Dialogue Systems,Survey of cultural awareness in language models: Text and beyond,"S Pawar,J Park,J Jin,A Arora,J Myung‚Ä¶¬†- Computational¬†‚Ä¶, 2025",direct.mit.edu,,https://direct.mit.edu/coli/article/doi/10.1162/COLI.a.14/130804,,"Large-scale deployment of large language models (LLMs) in various applications, such aschatbots and virtual assistants, requires LLMs to be culturally sensitive to the user to ensure¬†‚Ä¶",61.0,Survey
Moral Integrity Corpus,The Moral Integrity Corpus: A Benchmark for Ethical Dialogue Systems,Moca: Measuring human-language model alignment on causal and moral judgment tasks,"A Nie,Y Zhang,AS Amdekar,C Piech‚Ä¶¬†- Advances in¬†‚Ä¶, 2023",proceedings.neurips.cc,,https://proceedings.neurips.cc/paper_files/paper/2023/hash/f751c6f8bfb52c60f43942896fe65904-Abstract-Conference.html,,Human commonsense understanding of the physical and social world is organized aroundintuitive theories. These theories support making causal and moral judgments. When¬†‚Ä¶,69.0,Methodology
Moral Integrity Corpus,The Moral Integrity Corpus: A Benchmark for Ethical Dialogue Systems,Safetyprompts: a systematic review of open datasets for evaluating and improving large language model safety,"P R√∂ttger,F Pernisi,B Vidgen,D Hovy- Proceedings of the AAAI¬†‚Ä¶, 2025",ojs.aaai.org,,https://ojs.aaai.org/index.php/AAAI/article/view/34975,,The last two years have seen a rapid growth in concerns around the safety of largelanguage models (LLMs). Researchers and practitioners have met these concerns by¬†‚Ä¶,52.0,Survey
RobustBench,RobustBench: a standardized adversarial robustness benchmark,Trustworthy AI: From principles to practices,"B Li,P Qi, B Liu, S Di,J Liu,J Pei,J Yi‚Ä¶¬†- ACM Computing Surveys, 2023",dl.acm.org,,https://dl.acm.org/doi/abs/10.1145/3555803,,"The rapid development of Artificial Intelligence (AI) technology has enabled the deploymentof various systems based on it. However, many current AI systems are found vulnerable to¬†‚Ä¶",737.0,Other
RobustBench,RobustBench: a standardized adversarial robustness benchmark,"Interpretable deep learning: Interpretation, interpretability, trustworthiness, and beyond","X Li,H Xiong,X Li, X Wu,X Zhang,J Liu,J Bian‚Ä¶¬†- ‚Ä¶¬†and Information Systems, 2022",Springer,,https://link.springer.com/article/10.1007/s10115-022-01756-8,,"Deep neural networks have been well-known for their superb handling of various machinelearning and artificial intelligence tasks. However, due to their over-parameterized black-box¬†‚Ä¶",573.0,Survey
RobustBench,RobustBench: a standardized adversarial robustness benchmark,Jailbreakbench: An open robustness benchmark for jailbreaking large language models,"P Chao,E Debenedetti,A Robey‚Ä¶¬†- Advances in¬†‚Ä¶, 2024",proceedings.neurips.cc,,https://proceedings.neurips.cc/paper_files/paper/2024/hash/63092d79154adebd7305dfd498cbff70-Abstract-Datasets_and_Benchmarks_Track.html,,"Jailbreak attacks cause large language models (LLMs) to generate harmful, unethical, orotherwise objectionable content. Evaluating these attacks presents a number of challenges¬†‚Ä¶",350.0,Benchmark
RobustBench,RobustBench: a standardized adversarial robustness benchmark,Harmbench: A standardized evaluation framework for automated red teaming and robust refusal,"M Mazeika,L Phan,X Yin,A Zou,Z Wang,N Mu‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2024",arxiv.org,,https://arxiv.org/abs/2402.04249,,"Automated red teaming holds substantial promise for uncovering and mitigating the risksassociated with the malicious use of large language models (LLMs), yet the field lacks a¬†‚Ä¶",549.0,Benchmark
RobustBench,RobustBench: a standardized adversarial robustness benchmark,Better diffusion models further improve adversarial training,"Z Wang,T Pang,C Du,M Lin‚Ä¶¬†- ‚Ä¶¬†on machine learning, 2023",proceedings.mlr.press,,http://proceedings.mlr.press/v202/wang23ad.html,,It has been recognized that the data generated by the denoising diffusion probabilisticmodel (DDPM) improves adversarial training. After two years of rapid development in¬†‚Ä¶,347.0,Methodology
RobustBench,RobustBench: a standardized adversarial robustness benchmark,Visual adversarial examples jailbreak aligned large language models,"X Qi,K Huang,A Panda,P Henderson‚Ä¶¬†- Proceedings of the¬†‚Ä¶, 2024",ojs.aaai.org,,https://ojs.aaai.org/index.php/AAAI/article/view/30150,,"Warning: this paper contains data, prompts, and model outputs that are offensive in nature.Recently, there has been a surge of interest in integrating vision into Large Language¬†‚Ä¶",331.0,Methodology
RobustBench,RobustBench: a standardized adversarial robustness benchmark,Diffusion models for adversarial purification,"W Nie,B Guo,Y Huang,C Xiao,A Vahdat‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2022",arxiv.org,,https://arxiv.org/abs/2205.07460,,Adversarial purification refers to a class of defense methods that remove adversarialperturbations using a generative model. These methods do not make assumptions on the¬†‚Ä¶,743.0,Methodology
RobustBench,RobustBench: a standardized adversarial robustness benchmark,Continual test-time domain adaptation,"Q Wang,O Fink,L Van Gool‚Ä¶¬†- Proceedings of the IEEE¬†‚Ä¶, 2022",openaccess.thecvf.com,,http://openaccess.thecvf.com/content/CVPR2022/html/Wang_Continual_Test-Time_Domain_Adaptation_CVPR_2022_paper.html,,Test-time domain adaptation aims to adapt a source pre-trained model to a target domainwithout using any source data. Existing works mainly consider the case where the target¬†‚Ä¶,709.0,Methodology
RobustBench,RobustBench: a standardized adversarial robustness benchmark,Foundational challenges in assuring alignment and safety of large language models,"U Anwar,A Saparov,J Rando,D Paleka‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2024",arxiv.org,,https://arxiv.org/abs/2404.09932,,This work identifies 18 foundational challenges in assuring the alignment and safety of largelanguage models (LLMs). These challenges are organized into three different categories¬†‚Ä¶,254.0,Survey
RobustBench,RobustBench: a standardized adversarial robustness benchmark,Jailbreaking leading safety-aligned llms with simple adaptive attacks,"M Andriushchenko,F Croce,N Flammarion- arXiv preprint arXiv¬†‚Ä¶, 2024",arxiv.org,,https://arxiv.org/abs/2404.02151,,"We show that even the most recent safety-aligned LLMs are not robust to simple adaptivejailbreaking attacks. First, we demonstrate how to successfully leverage access to logprobs¬†‚Ä¶",284.0,Methodology
AutoAttack,Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks,"A survey of attacks on large vision‚Äìlanguage models: Resources, advances, and future trends","D Liu, M Yang,X Qu,P Zhou‚Ä¶¬†- IEEE Transactions on¬†‚Ä¶, 2025",ieeexplore.ieee.org,,https://ieeexplore.ieee.org/abstract/document/11127221/,,"With the significant development of large models in recent years, large vision‚Äìlanguagemodels (LVLMs) have demonstrated remarkable capabilities across a wide range of¬†‚Ä¶",86.0,Survey
AutoAttack,Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks,How deep learning sees the world: A survey on adversarial attacks & defenses,"JC Costa,T Roxo,H Proen√ßa,PRM Inacio- IEEE Access, 2024",ieeexplore.ieee.org,,https://ieeexplore.ieee.org/abstract/document/10510296/,,"Deep Learning is currently used to perform multiple tasks, such as object recognition, facerecognition, and natural language processing. However, Deep Neural Networks (DNNs) are¬†‚Ä¶",143.0,Survey
AutoAttack,Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks,Cross-entropy loss functions: Theoretical analysis and applications,"A Mao,M Mohri,Y Zhong- International conference on¬†‚Ä¶, 2023",proceedings.mlr.press,,http://proceedings.mlr.press/v202/mao23b.html,,"Cross-entropy is a widely used loss function in applications. It coincides with the logistic lossapplied to the outputs of a neural network, when the softmax is used. But, what guarantees¬†‚Ä¶",948.0,Other
AutoAttack,Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks,Harmbench: A standardized evaluation framework for automated red teaming and robust refusal,"M Mazeika,L Phan,X Yin,A Zou,Z Wang,N Mu‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2024",arxiv.org,,https://arxiv.org/abs/2402.04249,,"Automated red teaming holds substantial promise for uncovering and mitigating the risksassociated with the malicious use of large language models (LLMs), yet the field lacks a¬†‚Ä¶",549.0,Benchmark
AutoAttack,Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks,Better diffusion models further improve adversarial training,"Z Wang,T Pang,C Du,M Lin‚Ä¶¬†- ‚Ä¶¬†on machine learning, 2023",proceedings.mlr.press,,http://proceedings.mlr.press/v202/wang23ad.html,,It has been recognized that the data generated by the denoising diffusion probabilisticmodel (DDPM) improves adversarial training. After two years of rapid development in¬†‚Ä¶,347.0,Methodology
AutoAttack,Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks,Diffusion models for adversarial purification,"W Nie,B Guo,Y Huang,C Xiao,A Vahdat‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2022",arxiv.org,,https://arxiv.org/abs/2205.07460,,Adversarial purification refers to a class of defense methods that remove adversarialperturbations using a generative model. These methods do not make assumptions on the¬†‚Ä¶,743.0,Methodology
AutoAttack,Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks,Square attack: a query-efficient black-box adversarial attack via random search,"M Andriushchenko,F Croce,N Flammarion‚Ä¶¬†- European conference on¬†‚Ä¶, 2020",Springer,,https://link.springer.com/chapter/10.1007/978-3-030-58592-1_29,,"Abstract We propose the Square Attack, a score-based black-box l 2-and l‚àû-adversarialattack that does not rely on local gradient information and thus is not affected by gradient¬†‚Ä¶",1428.0,Methodology
AutoAttack,Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks,Robustbench: a standardized adversarial robustness benchmark,"F Croce,M Andriushchenko,V Sehwag‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2020",arxiv.org,,https://arxiv.org/abs/2010.09670,,"As a research community, we are still lacking a systematic understanding of the progress onadversarial robustness which often makes it hard to identify the most promising ideas in¬†‚Ä¶",989.0,Benchmark
AutoAttack,Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks,Data augmentation can improve robustness,"SA Rebuffi,S Gowal,DA Calian‚Ä¶¬†- Advances in neural¬†‚Ä¶, 2021",proceedings.neurips.cc,,https://proceedings.neurips.cc/paper/2021/hash/fb4c48608ce8825b558ccf07169a3421-Abstract.html,,"Adversarial training suffers from robust overfitting, a phenomenon where the robust testaccuracy starts to decrease during training. In this paper, we focus on reducing robust¬†‚Ä¶",524.0,Methodology
AutoAttack,Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks,Defensive unlearning with adversarial training for robust concept erasure in diffusion models,"Y Zhang,X Chen,J Jia,Y Zhang‚Ä¶¬†- Advances in neural¬†‚Ä¶, 2024",proceedings.neurips.cc,,https://proceedings.neurips.cc/paper_files/paper/2024/hash/40954ac18a457dd5f11145bae6454cdf-Abstract-Conference.html,,"Diffusion models (DMs) have achieved remarkable success in text-to-image generation, butthey also pose safety risks, such as the potential generation of harmful content and copyright¬†‚Ä¶",103.0,Methodology
ImageNet-C / ImageNet-P,Benchmarking Neural Network Robustness to Common Corruptions and Perturbations,The rise and potential of large language model based agents: A survey,"Z Xi,W Chen,X Guo,W He,Y Ding, B Hong‚Ä¶¬†- Science China¬†‚Ä¶, 2025",Springer,,https://link.springer.com/article/10.1007/s11432-024-4222-0,,"For a long time, researchers have sought artificial intelligence (AI) that matches or exceedshuman intelligence. AI agents, which are artificial entities capable of sensing the¬†‚Ä¶",1558.0,Survey
ImageNet-C / ImageNet-P,Benchmarking Neural Network Robustness to Common Corruptions and Perturbations,Ai alignment: A comprehensive survey,"J Ji,T Qiu,B Chen, B Zhang,H Lou, K Wang‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2023",arxiv.org,,https://arxiv.org/abs/2310.19852,,"AI alignment aims to make AI systems behave in line with human intentions and values. AsAI systems grow more capable, so do risks from misalignment. To provide a comprehensive¬†‚Ä¶",459.0,Survey
ImageNet-C / ImageNet-P,Benchmarking Neural Network Robustness to Common Corruptions and Perturbations,Dinov2: Learning robust visual features without supervision,"M Oquab,T Darcet,T Moutakanni,H Vo‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2023",arxiv.org,,https://arxiv.org/abs/2304.07193,,The recent breakthroughs in natural language processing for model pretraining on largequantities of data have opened the way for similar foundation models in computer vision¬†‚Ä¶,5336.0,Methodology
ImageNet-C / ImageNet-P,Benchmarking Neural Network Robustness to Common Corruptions and Perturbations,Reproducible scaling laws for contrastive language-image learning,"M Cherti, R Beaumont,R Wightman‚Ä¶¬†- Proceedings of the¬†‚Ä¶, 2023",openaccess.thecvf.com,,https://openaccess.thecvf.com/content/CVPR2023/html/Cherti_Reproducible_Scaling_Laws_for_Contrastive_Language-Image_Learning_CVPR_2023_paper,,"Scaling up neural networks has led to remarkable performance across a wide range oftasks. Moreover, performance often follows reliable scaling laws as a function of training set¬†‚Ä¶",1238.0,Methodology
ImageNet-C / ImageNet-P,Benchmarking Neural Network Robustness to Common Corruptions and Perturbations,Scaling vision transformers to 22 billion parameters,"M Dehghani,J Djolonga,B Mustafa‚Ä¶¬†- International¬†‚Ä¶, 2023",proceedings.mlr.press,,http://proceedings.mlr.press/v202/dehghani23a.html,,"The scaling of Transformers has driven breakthrough capabilities for language models. Atpresent, the largest large language models (LLMs) contain upwards of 100B parameters¬†‚Ä¶",824.0,Methodology
ImageNet-C / ImageNet-P,Benchmarking Neural Network Robustness to Common Corruptions and Perturbations,Cellpose 2.0: how to train your own model,"M Pachitariu,C Stringer- Nature methods, 2022",nature.com,,https://www.nature.com/articles/s41592-022-01663-4,,"Pretrained neural network models for biological segmentation can provide good out-of-the-box results for many image types. However, such models do not allow users to adapt the¬†‚Ä¶",1063.0,Methodology
ImageNet-C / ImageNet-P,Benchmarking Neural Network Robustness to Common Corruptions and Perturbations,A convnet for the 2020s,"Z Liu,H Mao,CY Wu,C Feichtenhofer‚Ä¶¬†- Proceedings of the¬†‚Ä¶, 2022",openaccess.thecvf.com,,http://openaccess.thecvf.com/content/CVPR2022/html/Liu_A_ConvNet_for_the_2020s_CVPR_2022_paper.html,,"The"" Roaring 20s"" of visual recognition began with the introduction of Vision Transformers(ViTs), which quickly superseded ConvNets as the state-of-the-art image classification¬†‚Ä¶",10224.0,Methodology
ImageNet-C / ImageNet-P,Benchmarking Neural Network Robustness to Common Corruptions and Perturbations,Repvit: Revisiting mobile cnn from vit perspective,"A Wang,H Chen,Z Lin,J Han‚Ä¶¬†- Proceedings of the IEEE¬†‚Ä¶, 2024",openaccess.thecvf.com,,http://openaccess.thecvf.com/content/CVPR2024/html/Wang_RepViT_Revisiting_Mobile_CNN_From_ViT_Perspective_CVPR_2024_paper.html,,Abstract Recently lightweight Vision Transformers (ViTs) demonstrate superior performanceand lower latency compared with lightweight Convolutional Neural Networks (CNNs) on¬†‚Ä¶,580.0,Methodology
ImageNet-C / ImageNet-P,Benchmarking Neural Network Robustness to Common Corruptions and Perturbations,Masked autoencoders are scalable vision learners,"K He,X Chen,S Xie,Y Li,P Doll√°r‚Ä¶¬†- Proceedings of the¬†‚Ä¶, 2022",openaccess.thecvf.com,,https://openaccess.thecvf.com/content/CVPR2022/html/He_Masked_Autoencoders_Are_Scalable_Vision_Learners_CVPR_2022_paper,,This paper shows that masked autoencoders (MAE) are scalable self-supervised learnersfor computer vision. Our MAE approach is simple: we mask random patches of the input¬†‚Ä¶,12162.0,Methodology
ImageNet-C / ImageNet-P,Benchmarking Neural Network Robustness to Common Corruptions and Perturbations,Learning transferable visual models from natural language supervision,"A Radford,JW Kim, C Hallacy‚Ä¶¬†- International¬†‚Ä¶, 2021",proceedings.mlr.press,,http://proceedings.mlr.press/v139/radford21a,,State-of-the-art computer vision systems are trained to predict a fixed set of predeterminedobject categories. This restricted form of supervision limits their generality and usability since¬†‚Ä¶,46323.0,Methodology
WILDS,WILDS: A Benchmark of in-the-Wild Distribution Shifts,Scientific discovery in the age of artificial intelligence,"H Wang, T Fu,Y Du,W Gao,K Huang,Z Liu‚Ä¶¬†- Nature, 2023",nature.com,,https://www.nature.com/articles/s41586-023-06221-2,,"Artificial intelligence (AI) is being increasingly integrated into scientific discovery to augmentand accelerate research, helping scientists to generate hypotheses, design experiments¬†‚Ä¶",1660.0,Other
WILDS,WILDS: A Benchmark of in-the-Wild Distribution Shifts,The current and future state of AI interpretation of medical images,"P Rajpurkar,MP Lungren- New England Journal of Medicine, 2023",Mass Medical Soc,,https://www.nejm.org/doi/full/10.1056/NEJMra2301725,,The Current and Future State of AI Interpretation of Medical Images | New England Journal ofMedicine Skip to main content The New England Journal of Medicine homepage Advanced¬†‚Ä¶,396.0,Survey
WILDS,WILDS: A Benchmark of in-the-Wild Distribution Shifts,Towards a general-purpose foundation model for computational pathology,"RJ Chen,T Ding,MY Lu,DFK Williamson,G Jaume‚Ä¶¬†- Nature medicine, 2024",nature.com,,https://www.nature.com/articles/s41591-024-02857-3,,"Quantitative evaluation of tissue images is crucial for computational pathology (CPath) tasks,requiring the objective characterization of histopathological entities from whole-slide images¬†‚Ä¶",1031.0,Methodology
WILDS,WILDS: A Benchmark of in-the-Wild Distribution Shifts,Holistic evaluation of language models,"P Liang,R Bommasani,T Lee, D Tsipras‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2022",arxiv.org,,https://arxiv.org/abs/2211.09110,,"Language models (LMs) are becoming the foundation for almost all major languagetechnologies, but their capabilities, limitations, and risks are not well understood. We present¬†‚Ä¶",1905.0,Survey
WILDS,WILDS: A Benchmark of in-the-Wild Distribution Shifts,Datacomp: In search of the next generation of multimodal datasets,"SY Gadre,G Ilharco,A Fang‚Ä¶¬†- Advances in¬†‚Ä¶, 2023",proceedings.neurips.cc,,https://proceedings.neurips.cc/paper_files/paper/2023/hash/56332d41d55ad7ad8024aac625881be7-Abstract-Datasets_and_Benchmarks.html,,"Multimodal datasets are a critical component in recent breakthroughs such as CLIP, StableDiffusion and GPT-4, yet their design does not receive the same research attention as model¬†‚Ä¶",626.0,Benchmark
WILDS,WILDS: A Benchmark of in-the-Wild Distribution Shifts,DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models.,"B Wang,W Chen,H Pei,C Xie,M Kang,C Zhang,C Xu‚Ä¶¬†- NeurIPS, 2023",blogs.qub.ac.uk,,https://blogs.qub.ac.uk/wp-content/uploads/sites/7/2024/01/A-comprehensive-Assessment-of-Trustworthiness-in-GPT-Models.pdf,,"Abstract Generative Pre-trained Transformer (GPT) models have exhibited exciting progressin their capabilities, capturing the interest of practitioners and the public alike. Yet, while the¬†‚Ä¶",625.0,Survey
WILDS,WILDS: A Benchmark of in-the-Wild Distribution Shifts,Generalized out-of-distribution detection: A survey,"J Yang,K Zhou,Y Li,Z Liu- International Journal of Computer Vision, 2024",Springer,,https://link.springer.com/article/10.1007/s11263-024-02117-4,,"Abstract Out-of-distribution (OOD) detection is critical to ensuring the reliability and safety ofmachine learning systems. For instance, in autonomous driving, we would like the driving¬†‚Ä¶",1451.0,Survey
WILDS,WILDS: A Benchmark of in-the-Wild Distribution Shifts,Trustworthy llms: a survey and guideline for evaluating large language models' alignment,"Y Liu,Y Yao,JF Ton,X Zhang,R Guo,H Cheng‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2023",arxiv.org,,https://arxiv.org/abs/2308.05374,,"Ensuring alignment, which refers to making models behave in accordance with humanintentions [1, 2], has become a critical task before deploying large language models (LLMs)¬†‚Ä¶",516.0,Survey
WILDS,WILDS: A Benchmark of in-the-Wild Distribution Shifts,Data-centric artificial intelligence: A survey,"D Zha, ZP Bhat,KH Lai,F Yang,Z Jiang‚Ä¶¬†- ACM Computing¬†‚Ä¶, 2025",dl.acm.org,,https://dl.acm.org/doi/abs/10.1145/3711118,,Artificial Intelligence (AI) is making a profound impact in almost every domain. A vital enablerof its great success is the availability of abundant and high-quality data for building machine¬†‚Ä¶,441.0,Survey
WILDS,WILDS: A Benchmark of in-the-Wild Distribution Shifts,Transfer learning in environmental remote sensing,"Y Ma,S Chen,S Ermon,DB Lobell- Remote Sensing of Environment, 2024",Elsevier,,https://www.sciencedirect.com/science/article/pii/S0034425723004765,,Abstract Machine learning (ML) has proven to be a powerful tool for utilizing the rapidlyincreasing amounts of remote sensing data for environmental monitoring. Yet ML models¬†‚Ä¶,291.0,Methodology
OoD-Bench,OoD-Bench: Quantifying and Understanding Two Dimensions of Out-of-Distribution Generalization,AUC maximization in the era of big data and AI: A survey,"T Yang,Y Ying- ACM computing surveys, 2022",dl.acm.org,,https://dl.acm.org/doi/abs/10.1145/3554729,,"Area under the ROC curve, aka AUC, is a measure of choice for assessing the performanceof a classifier for imbalanced data. AUC maximization refers to a learning paradigm that¬†‚Ä¶",146.0,Survey
OoD-Bench,OoD-Bench: Quantifying and Understanding Two Dimensions of Out-of-Distribution Generalization,"Physics-informed machine learning: A survey on problems, methods and applications","Z Hao,S Liu,Y Zhang,C Ying,Y Feng,H Su‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2022",arxiv.org,,https://arxiv.org/abs/2211.08064,,"Recent advances of data-driven machine learning have revolutionized fields like computervision, reinforcement learning, and many scientific and engineering domains. In many real¬†‚Ä¶",263.0,Survey
OoD-Bench,OoD-Bench: Quantifying and Understanding Two Dimensions of Out-of-Distribution Generalization,A survey on evaluation of out-of-distribution generalization,"H Yu,J Liu,X Zhang,J Wu,P Cui- arXiv preprint arXiv:2403.01874, 2024",arxiv.org,,https://arxiv.org/abs/2403.01874,,"Machine learning models, while progressively advanced, rely heavily on the IID assumption,which is often unfulfilled in practice due to inevitable distribution shifts. This renders them¬†‚Ä¶",35.0,Survey
OoD-Bench,OoD-Bench: Quantifying and Understanding Two Dimensions of Out-of-Distribution Generalization,A sentence speaks a thousand images: Domain generalization through distilling clip with language guidance,"Z Huang,A Zhou,Z Ling,M Cai‚Ä¶¬†- Proceedings of the¬†‚Ä¶, 2023",openaccess.thecvf.com,,http://openaccess.thecvf.com/content/ICCV2023/html/Huang_A_Sentence_Speaks_a_Thousand_Images_Domain_Generalization_through_Distilling_ICCV_2023_paper.html,,Abstract Domain generalization studies the problem of training a model with samples fromseveral domains (or distributions) and then testing the model with samples from a new¬†‚Ä¶,58.0,Methodology
OoD-Bench,OoD-Bench: Quantifying and Understanding Two Dimensions of Out-of-Distribution Generalization,Improved test-time adaptation for domain generalization,"L Chen, Y Zhang,Y Song,Y Shan‚Ä¶¬†- Proceedings of the¬†‚Ä¶, 2023",openaccess.thecvf.com,,http://openaccess.thecvf.com/content/CVPR2023/html/Chen_Improved_Test-Time_Adaptation_for_Domain_Generalization_CVPR_2023_paper.html,,The main challenge in domain generalization (DG) is to handle the distribution shift problemthat lies between the training and test data. Recent studies suggest that test-time training¬†‚Ä¶,84.0,Methodology
OoD-Bench,OoD-Bench: Quantifying and Understanding Two Dimensions of Out-of-Distribution Generalization,Coda: A real-world road corner case dataset for object detection in autonomous driving,"K Li,K Chen,H Wang,L Hong, C Ye,J Han‚Ä¶¬†- European conference on¬†‚Ä¶, 2022",Springer,,https://link.springer.com/chapter/10.1007/978-3-031-19839-7_24,,"Contemporary deep-learning object detection methods for autonomous driving usuallypresume fixed categories of common traffic participants, such as pedestrians and cars. Most¬†‚Ä¶",156.0,Benchmark
OoD-Bench,OoD-Bench: Quantifying and Understanding Two Dimensions of Out-of-Distribution Generalization,Wild-time: A benchmark of in-the-wild distribution shift over time,"H Yao,C Choi,B Cao,Y Lee‚Ä¶¬†- Advances in Neural¬†‚Ä¶, 2022",proceedings.neurips.cc,,https://proceedings.neurips.cc/paper_files/paper/2022/hash/43119db5d59f07cc08fca7ba6820179a-Abstract-Datasets_and_Benchmarks.html,,"Distribution shifts occur when the test distribution differs from the training distribution, andcan considerably degrade performance of machine learning models deployed in the real¬†‚Ä¶",137.0,Benchmark
OoD-Bench,OoD-Bench: Quantifying and Understanding Two Dimensions of Out-of-Distribution Generalization,Feed two birds with one scone: Exploiting wild data for both out-of-distribution generalization and detection,"H Bai,G Canal,X Du,J Kwon‚Ä¶¬†- ‚Ä¶¬†on Machine Learning, 2023",proceedings.mlr.press,,https://proceedings.mlr.press/v202/bai23a.html,,"Modern machine learning models deployed in the wild can encounter both covariate andsemantic shifts, giving rise to the problems of out-of-distribution (OOD) generalization and¬†‚Ä¶",65.0,Methodology
OoD-Bench,OoD-Bench: Quantifying and Understanding Two Dimensions of Out-of-Distribution Generalization,Sparse invariant risk minimization,"X Zhou,Y Lin,W Zhang‚Ä¶¬†- ‚Ä¶¬†Conference on Machine¬†‚Ä¶, 2022",proceedings.mlr.press,,https://proceedings.mlr.press/v162/zhou22e.html,,"Abstract Invariant Risk Minimization (IRM) is an emerging invariant feature extractingtechnique to help generalization with distributional shift. However, we find that there exists a¬†‚Ä¶",92.0,Methodology
OoD-Bench,OoD-Bench: Quantifying and Understanding Two Dimensions of Out-of-Distribution Generalization,Domain generalization via rationale invariance,"L Chen, Y Zhang,Y Song‚Ä¶¬†- Proceedings of the¬†‚Ä¶, 2023",openaccess.thecvf.com,,http://openaccess.thecvf.com/content/ICCV2023/html/Chen_Domain_Generalization_via_Rationale_Invariance_ICCV_2023_paper.html,,"This paper offers a new perspective to ease the challenge of domain generalization, whichinvolves maintaining robust results even in unseen environments. Our design focuses on the¬†‚Ä¶",44.0,Methodology
SoK: Certified Robustness,SoK: Certified Robustness for Deep Neural Networks,Security and privacy challenges of large language models: A survey,"BC Das,MH Amini,Y Wu- ACM Computing Surveys, 2025",dl.acm.org,,https://dl.acm.org/doi/abs/10.1145/3712001,,"Large language models (LLMs) have demonstrated extraordinary capabilities andcontributed to multiple fields, such as generating and summarizing text, language¬†‚Ä¶",388.0,Survey
SoK: Certified Robustness,SoK: Certified Robustness for Deep Neural Networks,Universal and transferable adversarial attacks on aligned language models,"A Zou,Z Wang,N Carlini,M Nasr,JZ Kolter‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2023",arxiv.org,,https://arxiv.org/abs/2307.15043,,"Because"" out-of-the-box"" large language models are capable of generating a great deal ofobjectionable content, recent work has focused on aligning these models in an attempt to¬†‚Ä¶",2181.0,Methodology
SoK: Certified Robustness,SoK: Certified Robustness for Deep Neural Networks,Visual adversarial examples jailbreak aligned large language models,"X Qi,K Huang,A Panda,P Henderson‚Ä¶¬†- Proceedings of the¬†‚Ä¶, 2024",ojs.aaai.org,,https://ojs.aaai.org/index.php/AAAI/article/view/30150,,"Warning: this paper contains data, prompts, and model outputs that are offensive in nature.Recently, there has been a surge of interest in integrating vision into Large Language¬†‚Ä¶",331.0,Methodology
SoK: Certified Robustness,SoK: Certified Robustness for Deep Neural Networks,"Adversarial machine learning: a review of methods, tools, and critical industry sectors.","S Pelekis,T Koutroubas,A Blika‚Ä¶¬†- Artificial Intelligence¬†‚Ä¶, 2025",drive.google.com,,https://drive.google.com/file/d/1N1s5ndgZkIXhlJeo4kisJ1ey2Fw6_zHY/view,,"The rapid advancement of Artificial Intelligence (AI), particularly Machine Learning (ML) andDeep Learning (DL), has produced high-performance models widely used in various¬†‚Ä¶",14.0,Survey
SoK: Certified Robustness,SoK: Certified Robustness for Deep Neural Networks,Invisible for both camera and lidar: Security of multi-sensor fusion based perception in autonomous driving under physical-world attacks,"Y Cao,N Wang,C Xiao, D Yang,J Fang‚Ä¶¬†- ‚Ä¶¬†IEEE symposium on¬†‚Ä¶, 2021",ieeexplore.ieee.org,,https://ieeexplore.ieee.org/abstract/document/9519442/,,"In Autonomous Driving (AD) systems, perception is both security and safety critical. Despitevarious prior studies on its security issues, all of them only consider attacks on camera-or¬†‚Ä¶",356.0,Other
SoK: Certified Robustness,SoK: Certified Robustness for Deep Neural Networks,Attention-enhancing backdoor attacks against bert-based models,"W Lyu,S Zheng,L Pang,H Ling,C Chen- arXiv preprint arXiv:2310.14480, 2023",arxiv.org,,https://arxiv.org/abs/2310.14480,,Recent studies have revealed that\textit {Backdoor Attacks} can threaten the safety of naturallanguage processing (NLP) models. Investigating the strategies of backdoor attacks will help¬†‚Ä¶,82.0,Methodology
SoK: Certified Robustness,SoK: Certified Robustness for Deep Neural Networks,Adversarial robustness of deep neural networks: A survey from a formal verification perspective,"MH Meng,G Bai,SG Teo, Z Hou,Y Xiao‚Ä¶¬†- ‚Ä¶¬†on Dependable and¬†‚Ä¶, 2022",ieeexplore.ieee.org,,https://ieeexplore.ieee.org/abstract/document/9785704/,,"Neural networks have been widely applied in security applications such as spam andphishing detection, intrusion prevention, and malware detection. This black-box method¬†‚Ä¶",109.0,Survey
SoK: Certified Robustness,SoK: Certified Robustness for Deep Neural Networks,Rab: Provable robustness against backdoor attacks,"M Weber,X Xu,B Karla≈°,C Zhang‚Ä¶¬†- 2023 IEEE Symposium¬†‚Ä¶, 2023",ieeexplore.ieee.org,,https://ieeexplore.ieee.org/abstract/document/10179451/,,"Recent studies have shown that deep neural net-works (DNNs) are vulnerable toadversarial attacks, including evasion and backdoor (poisoning) attacks. On the defense¬†‚Ä¶",223.0,Methodology
SoK: Certified Robustness,SoK: Certified Robustness for Deep Neural Networks,A comprehensive survey of robust deep learning in computer vision,"J Liu,Y Jin- Journal of Automation and Intelligence, 2023",Elsevier,,https://www.sciencedirect.com/science/article/pii/S294985542300045X,,"Deep learning has presented remarkable progress in various tasks. Despite the excellentperformance, deep learning models remain not robust, especially to well-designed¬†‚Ä¶",42.0,Survey
SoK: Certified Robustness,SoK: Certified Robustness for Deep Neural Networks,Text-crs: A generalized certified robustness framework against textual adversarial attacks,"X Zhang,H Hong,Y Hong,P Huang‚Ä¶¬†- ‚Ä¶¬†IEEE Symposium on¬†‚Ä¶, 2024",ieeexplore.ieee.org,,https://ieeexplore.ieee.org/abstract/document/10646716/,,"The language models, especially the basic text classification models, have been shown tobe susceptible to textual adversarial attacks such as synonym substitution and word¬†‚Ä¶",37.0,Methodology
Adversarial Robustness Benchmark,Benchmarking Adversarial Robustness on Image Classification,Data-centric artificial intelligence: A survey,"D Zha, ZP Bhat,KH Lai,F Yang,Z Jiang‚Ä¶¬†- ACM Computing¬†‚Ä¶, 2025",dl.acm.org,,https://dl.acm.org/doi/abs/10.1145/3711118,,Artificial Intelligence (AI) is making a profound impact in almost every domain. A vital enablerof its great success is the availability of abundant and high-quality data for building machine¬†‚Ä¶,441.0,Survey
Adversarial Robustness Benchmark,Benchmarking Adversarial Robustness on Image Classification,Toward the third generation artificial intelligence,"B Zhang,J Zhu,H Su- Science China Information Sciences, 2023",Springer,,https://link.springer.com/article/10.1007/s11432-021-3449-x,,"There have been two competing paradigms in artificial intelligence (AI) development eversince its birth in 1956, ie, symbolism and connectionism (or sub-symbolism). While¬†‚Ä¶",346.0,Survey
Adversarial Robustness Benchmark,Benchmarking Adversarial Robustness on Image Classification,Better diffusion models further improve adversarial training,"Z Wang,T Pang,C Du,M Lin‚Ä¶¬†- ‚Ä¶¬†on machine learning, 2023",proceedings.mlr.press,,http://proceedings.mlr.press/v202/wang23ad.html,,It has been recognized that the data generated by the denoising diffusion probabilisticmodel (DDPM) improves adversarial training. After two years of rapid development in¬†‚Ä¶,347.0,Methodology
Adversarial Robustness Benchmark,Benchmarking Adversarial Robustness on Image Classification,Robustbench: a standardized adversarial robustness benchmark,"F Croce,M Andriushchenko,V Sehwag‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2020",arxiv.org,,https://arxiv.org/abs/2010.09670,,"As a research community, we are still lacking a systematic understanding of the progress onadversarial robustness which often makes it hard to identify the most promising ideas in¬†‚Ä¶",989.0,Benchmark
Adversarial Robustness Benchmark,Benchmarking Adversarial Robustness on Image Classification,Interpreting adversarial examples in deep learning: A review,"S Han,C Lin,C Shen,Q Wang, X Guan¬†- ACM Computing Surveys, 2023",dl.acm.org,,https://dl.acm.org/doi/abs/10.1145/3594869,,Deep learning technology is increasingly being applied in safety-critical scenarios but hasrecently been found to be susceptible to imperceptible adversarial perturbations. This raises¬†‚Ä¶,102.0,Survey
Adversarial Robustness Benchmark,Benchmarking Adversarial Robustness on Image Classification,Robustness in deep learning models for medical diagnostics: security and adversarial challenges towards robust AI applications,"H Javed,S El-Sappagh,T Abuhmed- Artificial Intelligence Review, 2024",Springer,,https://link.springer.com/article/10.1007/s10462-024-11005-9,,The current study investigates the robustness of deep learning models for accurate medicaldiagnosis systems with a specific focus on their ability to maintain performance in the¬†‚Ä¶,85.0,Methodology
Adversarial Robustness Benchmark,Benchmarking Adversarial Robustness on Image Classification,Backdoorbench: A comprehensive benchmark of backdoor learning,"B Wu,H Chen,M Zhang,Z Zhu,S Wei‚Ä¶¬†- Advances in¬†‚Ä¶, 2022",proceedings.neurips.cc,,https://proceedings.neurips.cc/paper_files/paper/2022/hash/4491ea1c91aa2b22c373e5f1dfce234f-Abstract-Datasets_and_Benchmarks.html,,Backdoor learning is an emerging and vital topic for studying deep neural networks'vulnerability (DNNs). Many pioneering backdoor attack and defense methods are being¬†‚Ä¶,205.0,Benchmark
Adversarial Robustness Benchmark,Benchmarking Adversarial Robustness on Image Classification,A comprehensive study on robustness of image classification models: Benchmarking and rethinking,"C Liu,Y Dong,W Xiang,X Yang,H Su,J Zhu‚Ä¶¬†- International Journal of¬†‚Ä¶, 2025",Springer,,https://link.springer.com/article/10.1007/s11263-024-02196-3,,"The robustness of deep neural networks is frequently compromised when faced withadversarial examples, common corruptions, and distribution shifts, posing a significant¬†‚Ä¶",134.0,Benchmark
Adversarial Robustness Benchmark,Benchmarking Adversarial Robustness on Image Classification,On adversarial robustness of trajectory prediction for autonomous vehicles,"Q Zhang,S Hu,J Sun,QA Chen‚Ä¶¬†- Proceedings of the¬†‚Ä¶, 2022",openaccess.thecvf.com,,http://openaccess.thecvf.com/content/CVPR2022/html/Zhang_On_Adversarial_Robustness_of_Trajectory_Prediction_for_Autonomous_Vehicles_CVPR_2022_paper.html,,"Trajectory prediction is a critical component for autonomous vehicles (AVs) to perform safeplanning and navigation. However, few studies have analyzed the adversarial robustness of¬†‚Ä¶",209.0,Methodology
Adversarial Robustness Benchmark,Benchmarking Adversarial Robustness on Image Classification,Understanding the robustness of 3D object detection with bird's-eye-view representations in autonomous driving,"Z Zhu,Y Zhang,H Chen,Y Dong‚Ä¶¬†- Proceedings of the¬†‚Ä¶, 2023",openaccess.thecvf.com,,http://openaccess.thecvf.com/content/CVPR2023/html/Zhu_Understanding_the_Robustness_of_3D_Object_Detection_With_Birds-Eye-View_Representations_CVPR_2023_paper.html,,Abstract 3D object detection is an essential perception task in autonomous driving tounderstand the environments. The Bird's-Eye-View (BEV) representations have significantly¬†‚Ä¶,86.0,Methodology
GRB,Graph Robustness Benchmark: Benchmarking the Adversarial Robustness of Graph Machine Learning,Evaluating explainability for graph neural networks,"C Agarwal,O Queen,H Lakkaraju,M Zitnik- Scientific Data, 2023",nature.com,,https://www.nature.com/articles/s41597-023-01974-x,,"As explanations are increasingly used to understand the behavior of graph neural networks(GNNs), evaluating the quality and reliability of GNN explanations is crucial. However¬†‚Ä¶",214.0,Methodology
GRB,Graph Robustness Benchmark: Benchmarking the Adversarial Robustness of Graph Machine Learning,AI robustness: a human-centered perspective on technological challenges and opportunities,"A Tocchetti,L Corti,A Balayn,M Yurrita‚Ä¶¬†- ACM Computing¬†‚Ä¶, 2025",dl.acm.org,,https://dl.acm.org/doi/abs/10.1145/3665926,,"Despite the impressive performance of Artificial Intelligence (AI) systems, their robustnessremains elusive and constitutes a key issue that impedes large-scale adoption. Besides¬†‚Ä¶",50.0,Other
GRB,Graph Robustness Benchmark: Benchmarking the Adversarial Robustness of Graph Machine Learning,"Trustworthy graph neural networks: Aspects, methods, and trends","H Zhang,B Wu,X Yuan,S Pan,H Tong‚Ä¶¬†- Proceedings of the¬†‚Ä¶, 2024",ieeexplore.ieee.org,,https://ieeexplore.ieee.org/abstract/document/10477407/,,"Graph neural networks (GNNs) have emerged as a series of competent graph learningmethods for diverse real-world scenarios, ranging from daily applications such as¬†‚Ä¶",189.0,Survey
GRB,Graph Robustness Benchmark: Benchmarking the Adversarial Robustness of Graph Machine Learning,Adversarial attack and defense on graph data: A survey,"L Sun,Y Dou,C Yang,K Zhang,J Wang‚Ä¶¬†- ‚Ä¶¬†on Knowledge and¬†‚Ä¶, 2022",ieeexplore.ieee.org,,https://ieeexplore.ieee.org/abstract/document/9878092/,,"Deep neural networks (DNNs) have been widely applied to various applications, includingimage classification, text generation, audio recognition, and graph data analysis. However¬†‚Ä¶",467.0,Survey
GRB,Graph Robustness Benchmark: Benchmarking the Adversarial Robustness of Graph Machine Learning,Bond: Benchmarking unsupervised outlier node detection on static attributed graphs,"K Liu,Y Dou,Y Zhao,X Ding,X Hu‚Ä¶¬†- Advances in¬†‚Ä¶, 2022",proceedings.neurips.cc,,https://proceedings.neurips.cc/paper_files/paper/2022/hash/acc1ec4a9c780006c9aafd595104816b-Abstract-Datasets_and_Benchmarks.html,,Detecting which nodes in graphs are outliers is a relatively new machine learning task withnumerous applications. Despite the proliferation of algorithms developed in recent years for¬†‚Ä¶,146.0,Benchmark
GRB,Graph Robustness Benchmark: Benchmarking the Adversarial Robustness of Graph Machine Learning,Are defenses for graph neural networks robust?,"F Mujkanovic,S Geisler‚Ä¶¬†- Advances in Neural¬†‚Ä¶, 2022",proceedings.neurips.cc,,https://proceedings.neurips.cc/paper_files/paper/2022/hash/3ac904a31f9141444009777abef2ed8e-Abstract-Conference.html,,"A cursory reading of the literature suggests that we have made a lot of progress in designingeffective adversarial defenses for Graph Neural Networks (GNNs). Yet, the standard¬†‚Ä¶",99.0,Survey
GRB,Graph Robustness Benchmark: Benchmarking the Adversarial Robustness of Graph Machine Learning,Adversarial robustness in graph neural networks: A hamiltonian approach,"K Zhao,Q Kang,Y Song,R She‚Ä¶¬†- Advances in Neural¬†‚Ä¶, 2023",proceedings.neurips.cc,,https://proceedings.neurips.cc/paper_files/paper/2023/hash/0a443a000e1cb2281480b3bac395b3b8-Abstract-Conference.html,,"Graph neural networks (GNNs) are vulnerable to adversarial perturbations, including thosethat affect both node features and graph topology. This paper investigates GNNs derived¬†‚Ä¶",44.0,Methodology
GRB,Graph Robustness Benchmark: Benchmarking the Adversarial Robustness of Graph Machine Learning,"Adversarial training for graph neural networks: Pitfalls, solutions, and new directions","L Gosch,S Geisler, D Sturm‚Ä¶¬†- Advances in neural¬†‚Ä¶, 2023",proceedings.neurips.cc,,https://proceedings.neurips.cc/paper_files/paper/2023/hash/b5a801e6bc4f4ffa3e6786518a324488-Abstract-Conference.html,,"Despite its success in the image domain, adversarial training did not (yet) stand out as aneffective defense for Graph Neural Networks (GNNs) against graph structure perturbations¬†‚Ä¶",55.0,Survey
GRB,Graph Robustness Benchmark: Benchmarking the Adversarial Robustness of Graph Machine Learning,A comprehensive study on text-attributed graphs: Benchmarking and rethinking,"H Yan,C Li, R Long, C Yan,J Zhao‚Ä¶¬†- Advances in¬†‚Ä¶, 2023",proceedings.neurips.cc,,https://proceedings.neurips.cc/paper_files/paper/2023/hash/37d00f567a18b478065f1a91b95622a0-Abstract-Datasets_and_Benchmarks.html,,"Text-attributed graphs (TAGs) are prevalent in various real-world scenarios, where eachnode is associated with a text description. The cornerstone of representation learning on¬†‚Ä¶",41.0,Benchmark
GRB,Graph Robustness Benchmark: Benchmarking the Adversarial Robustness of Graph Machine Learning,Can large language models improve the adversarial robustness of graph neural networks?,"Z Zhang,X Wang,H Zhou,Y Yu, M Zhang‚Ä¶¬†- Proceedings of the 31st¬†‚Ä¶, 2025",dl.acm.org,,https://dl.acm.org/doi/abs/10.1145/3690624.3709256,,"Graph neural networks (GNNs) are vulnerable to adversarial attacks, especially for topologyperturbations, and many methods that improve the robustness of GNNs have received¬†‚Ä¶",24.0,Methodology
RealToxicityPrompts,RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models,A survey on evaluation of large language models,"Y Chang,X Wang,J Wang,Y Wu,L Yang‚Ä¶¬†- ACM transactions on¬†‚Ä¶, 2024",dl.acm.org,,https://dl.acm.org/doi/abs/10.1145/3641289,,"Large language models (LLMs) are gaining increasing popularity in both academia andindustry, owing to their unprecedented performance in various applications. As LLMs¬†‚Ä¶",4405.0,Survey
RealToxicityPrompts,RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models,"A survey on large language model (llm) security and privacy: The good, the bad, and the ugly","Y Yao,J Duan,K Xu, Y Cai,Z Sun,Y Zhang- High-Confidence Computing, 2024",Elsevier,,https://www.sciencedirect.com/science/article/pii/S266729522400014X,,"Abstract Large Language Models (LLMs), such as ChatGPT and Bard, have revolutionizednatural language understanding and generation. They possess deep language¬†‚Ä¶",1335.0,Survey
RealToxicityPrompts,RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models,Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context,"G Team,P Georgiev, VI Lei, R Burnell, L Bai‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2024",arxiv.org,,https://arxiv.org/abs/2403.05530,,"In this report, we introduce the Gemini 1.5 family of models, representing the next generationof highly compute-efficient multimodal models capable of recalling and reasoning over fine¬†‚Ä¶",2793.0,Methodology
RealToxicityPrompts,RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models,A comprehensive overview of large language models,"H Naveed,AU Khan,S Qiu,M Saqib,S Anwar‚Ä¶¬†- ACM Transactions on¬†‚Ä¶, 2025",dl.acm.org,,https://dl.acm.org/doi/abs/10.1145/3744746,,Large Language Models (LLMs) have recently demonstrated remarkable capabilities innatural language processing tasks and beyond. This success of LLMs has led to a large¬†‚Ä¶,1815.0,Survey
RealToxicityPrompts,RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models,Jailbroken: How does llm safety training fail?,"A Wei,N Haghtalab‚Ä¶¬†- Advances in Neural¬†‚Ä¶, 2023",proceedings.neurips.cc,,https://proceedings.neurips.cc/paper_files/paper/2023/hash/fd6613131889a4b656206c50a8bd7790-Abstract-Conference.html,,"Large language models trained for safety and harmlessness remain susceptible toadversarial misuse, as evidenced by the prevalence of ‚Äújailbreak‚Äù attacks on early releases¬†‚Ä¶",1426.0,Other
RealToxicityPrompts,RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models,Palm 2 technical report,"R Anil,AM Dai,O Firat,M Johnson,D Lepikhin‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2023",arxiv.org,,https://arxiv.org/abs/2305.10403,,"We introduce PaLM 2, a new state-of-the-art language model that has better multilingual andreasoning capabilities and is more compute-efficient than its predecessor PaLM. PaLM 2 is¬†‚Ä¶",2120.0,Methodology
RealToxicityPrompts,RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models,A survey of large language models,"WX Zhao,K Zhou,J Li,T Tang,X Wang‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2023",researchgate.net,,https://www.researchgate.net/profile/Tang-Tianyi-3/publication/369740832_A_Survey_of_Large_Language_Models/links/665fd2e3637e4448a37dd281/A-Survey-of-Large-Language-Models.pdf,,"Ever since the Turing Test was proposed in the 1950s, humans have explored the masteringof language intelligence by machine. Language is essentially a complex, intricate system of¬†‚Ä¶",6302.0,Survey
RealToxicityPrompts,RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models,Gpt-4 technical report,"J Achiam,S Adler,S Agarwal,L Ahmad‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2023",arxiv.org,,https://arxiv.org/abs/2303.08774,,"We report the development of GPT-4, a large-scale, multimodal model which can acceptimage and text inputs and produce text outputs. While less capable than humans in many¬†‚Ä¶",19617.0,Methodology
RealToxicityPrompts,RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models,Starcoder: may the source be with you!,"R Li,LB Allal,Y Zi,N Muennighoff,D Kocetkov‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2023",arxiv.org,,https://arxiv.org/abs/2305.06161,,"The BigCode community, an open-scientific collaboration working on the responsibledevelopment of Large Language Models for Code (Code LLMs), introduces StarCoder and¬†‚Ä¶",1296.0,Methodology
RealToxicityPrompts,RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models,Holistic evaluation of language models,"P Liang,R Bommasani,T Lee, D Tsipras‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2022",arxiv.org,,https://arxiv.org/abs/2211.09110,,"Language models (LMs) are becoming the foundation for almost all major languagetechnologies, but their capabilities, limitations, and risks are not well understood. We present¬†‚Ä¶",1905.0,Survey
SafetyBench,SafetyBench: Evaluating the Safety of Large Language Models with Multiple Choice Questions,Igniting language intelligence: The hitchhiker's guide from chain-of-thought reasoning to language agents,"Z Zhang,Y Yao,A Zhang,X Tang,X Ma,Z He‚Ä¶¬†- ACM Computing¬†‚Ä¶, 2025",dl.acm.org,,https://dl.acm.org/doi/abs/10.1145/3719341,,"Large language models (LLMs) have dramatically enhanced the field of languageintelligence, as demonstrably evidenced by their formidable empirical performance across a¬†‚Ä¶",100.0,Methodology
SafetyBench,SafetyBench: Evaluating the Safety of Large Language Models with Multiple Choice Questions,"A comprehensive survey in llm (-agent) full stack safety: Data, training and deployment","K Wang,G Zhang,Z Zhou,J Wu,M Yu,S Zhao‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2025",arxiv.org,,https://arxiv.org/abs/2504.15585,,The remarkable success of Large Language Models (LLMs) has illuminated a promisingpathway toward achieving Artificial General Intelligence for both academic and industrial¬†‚Ä¶,61.0,Survey
SafetyBench,SafetyBench: Evaluating the Safety of Large Language Models with Multiple Choice Questions,The art of saying no: Contextual noncompliance in language models,"F Brahman,S Kumar,V Balachandran‚Ä¶¬†- Advances in¬†‚Ä¶, 2024",proceedings.neurips.cc,,https://proceedings.neurips.cc/paper_files/paper/2024/hash/58e79894267cf72c66202228ad9c6057-Abstract-Datasets_and_Benchmarks_Track.html,,"Chat-based language models are designed to be helpful, yet they should not comply withevery user request. While most existing work primarily focuses on refusal of``unsafe''queries¬†‚Ä¶",58.0,Methodology
SafetyBench,SafetyBench: Evaluating the Safety of Large Language Models with Multiple Choice Questions,Safetyprompts: a systematic review of open datasets for evaluating and improving large language model safety,"P R√∂ttger,F Pernisi,B Vidgen,D Hovy- Proceedings of the AAAI¬†‚Ä¶, 2025",ojs.aaai.org,,https://ojs.aaai.org/index.php/AAAI/article/view/34975,,The last two years have seen a rapid growth in concerns around the safety of largelanguage models (LLMs). Researchers and practitioners have met these concerns by¬†‚Ä¶,52.0,Survey
SafetyBench,SafetyBench: Evaluating the Safety of Large Language Models with Multiple Choice Questions,Agent-safetybench: Evaluating the safety of llm agents,"Z Zhang,S Cui,Y Lu, J Zhou,J Yang,H Wang‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2024",arxiv.org,,https://arxiv.org/abs/2412.14470,,"As large language models (LLMs) are increasingly deployed as agents, their integration intointeractive environments and tool use introduce new safety challenges beyond those¬†‚Ä¶",52.0,Benchmark
SafetyBench,SafetyBench: Evaluating the Safety of Large Language Models with Multiple Choice Questions,"On the trustworthiness of generative foundation models: Guideline, assessment, and perspective","Y Huang,C Gao, S Wu,H Wang,X Wang‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2025",arxiv.org,,https://arxiv.org/abs/2502.14296,,"Generative Foundation Models (GenFMs) have emerged as transformative tools. However,their widespread adoption raises critical concerns regarding trustworthiness across¬†‚Ä¶",32.0,Survey
SafetyBench,SafetyBench: Evaluating the Safety of Large Language Models with Multiple Choice Questions,Safety at scale: A comprehensive survey of large model safety,"X Ma, Y Gao,Y Wang,R Wang,X Wang,Y Sun‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2025",arxiv.org,,https://arxiv.org/abs/2502.05206,,"The rapid advancement of large models, driven by their exceptional abilities in learning andgeneralization through large-scale pre-training, has reshaped the landscape of Artificial¬†‚Ä¶",37.0,Survey
SafetyBench,SafetyBench: Evaluating the Safety of Large Language Models with Multiple Choice Questions,Big5-chat: Shaping llm personalities through training on human-grounded data,"W Li,J Liu,A Liu,X Zhou,M Diab,M Sap- arXiv preprint arXiv:2410.16491, 2024",arxiv.org,,https://arxiv.org/abs/2410.16491,,"In this work, we tackle the challenge of embedding realistic human personality traits intoLLMs. Previous approaches have primarily focused on prompt-based methods that describe¬†‚Ä¶",24.0,Methodology
SafetyBench,SafetyBench: Evaluating the Safety of Large Language Models with Multiple Choice Questions,Machine Against the {RAG}: Jamming {Retrieval-Augmented} Generation with Blocker Documents,"A Shafran,R Schuster,V Shmatikov- 34th USENIX Security Symposium¬†‚Ä¶, 2025",usenix.org,,https://www.usenix.org/conference/usenixsecurity25/presentation/shafran,,Retrieval-augmented generation (RAG) systems respond to queries by retrieving relevantdocuments from a knowledge database and applying an LLM to the retrieved documents¬†‚Ä¶,15.0,Methodology
SafetyBench,SafetyBench: Evaluating the Safety of Large Language Models with Multiple Choice Questions,Toward generalizable evaluation in the llm era: A survey beyond benchmarks,"Y Cao, S Hong,X Li,J Ying,Y Ma, H Liang‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2025",arxiv.org,,https://arxiv.org/abs/2504.18838,,"Large Language Models (LLMs) are advancing at an amazing speed and have becomeindispensable across academia, industry, and daily applications. To keep pace with the¬†‚Ä¶",19.0,Survey
ToxiGen,ToxiGen: A Large-Scale Machine-Generated Dataset for Adversarial and Implicit Hate Speech Detection,A survey of GPT-3 family large language models including ChatGPT and GPT-4,"KS Kalyan- Natural Language Processing Journal, 2024",Elsevier,,https://www.sciencedirect.com/science/article/pii/S2949719123000456,,"Large language models (LLMs) are a special class of pretrained language models (PLMs)obtained by scaling model size, pretraining corpus and computation. LLMs, because of their¬†‚Ä¶",512.0,Survey
ToxiGen,ToxiGen: A Large-Scale Machine-Generated Dataset for Adversarial and Implicit Hate Speech Detection,Ai alignment: A comprehensive survey,"J Ji,T Qiu,B Chen, B Zhang,H Lou, K Wang‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2023",arxiv.org,,https://arxiv.org/abs/2310.19852,,"AI alignment aims to make AI systems behave in line with human intentions and values. AsAI systems grow more capable, so do risks from misalignment. To provide a comprehensive¬†‚Ä¶",459.0,Survey
ToxiGen,ToxiGen: A Large-Scale Machine-Generated Dataset for Adversarial and Implicit Hate Speech Detection,Llama 2: Open foundation and fine-tuned chat models,"H Touvron,L Martin,K Stone, P Albert‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2023",arxiv.org,,https://arxiv.org/abs/2307.09288,,"In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned largelanguage models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine¬†‚Ä¶",19070.0,Methodology
ToxiGen,ToxiGen: A Large-Scale Machine-Generated Dataset for Adversarial and Implicit Hate Speech Detection,The llama 3 herd of models,"A Grattafiori,A Dubey, A Jauhri, A Pandey‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2024",arxiv.org,,https://arxiv.org/abs/2407.21783,,"Modern artificial intelligence (AI) systems are powered by foundation models. This paperpresents a new set of foundation models, called Llama 3. It is a herd of language models¬†‚Ä¶",3144.0,Methodology
ToxiGen,ToxiGen: A Large-Scale Machine-Generated Dataset for Adversarial and Implicit Hate Speech Detection,Trustllm: Trustworthiness in large language models,"Y Huang,L Sun,H Wang, S Wu,Q Zhang, Y Li‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2024",arxiv.org,,https://arxiv.org/abs/2401.05561,,"Large language models (LLMs), exemplified by ChatGPT, have gained considerableattention for their excellent natural language processing capabilities. Nonetheless, these¬†‚Ä¶",487.0,Methodology
ToxiGen,ToxiGen: A Large-Scale Machine-Generated Dataset for Adversarial and Implicit Hate Speech Detection,Agieval: A human-centric benchmark for evaluating foundation models,"W Zhong,R Cui,Y Guo,Y Liang,S Lu,Y Wang‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2023",arxiv.org,,https://arxiv.org/abs/2304.06364,,Evaluating the general abilities of foundation models to tackle human-level tasks is a vitalaspect of their development and application in the pursuit of Artificial General Intelligence¬†‚Ä¶,599.0,Benchmark
ToxiGen,ToxiGen: A Large-Scale Machine-Generated Dataset for Adversarial and Implicit Hate Speech Detection,Textbooks are all you need ii: phi-1.5 technical report,"Y Li,S Bubeck,R Eldan,A Del Giorno‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2023",arxiv.org,,https://arxiv.org/abs/2309.05463,,We continue the investigation into the power of smaller Transformer-based languagemodels as initiated by\textbf {TinyStories}--a 10 million parameter model that can produce¬†‚Ä¶,597.0,Methodology
ToxiGen,ToxiGen: A Large-Scale Machine-Generated Dataset for Adversarial and Implicit Hate Speech Detection,Orca: Progressive learning from complex explanation traces of gpt-4,"S Mukherjee,A Mitra,G Jawahar,S Agarwal‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2023",arxiv.org,,https://arxiv.org/abs/2306.02707,,"Recent research has focused on enhancing the capability of smaller models throughimitation learning, drawing on the outputs generated by large foundation models (LFMs). A¬†‚Ä¶",482.0,Methodology
ToxiGen,ToxiGen: A Large-Scale Machine-Generated Dataset for Adversarial and Implicit Hate Speech Detection,Almanac‚Äîretrieval-augmented language models for clinical medicine,"C Zakka,R Shad,A Chaurasia,AR Dalal, JL Kim‚Ä¶¬†- Nejm ai, 2024",ai.nejm.org,,https://ai.nejm.org/doi/abs/10.1056/AIoa2300068,,"Abstract Background Large language models (LLMs) have recently shown impressive zero-shot capabilities, whereby they can use auxiliary data, without the availability of task-specific¬†‚Ä¶",386.0,Methodology
ToxiGen,ToxiGen: A Large-Scale Machine-Generated Dataset for Adversarial and Implicit Hate Speech Detection,Meditron-70b: Scaling medical pretraining for large language models,"Z Chen,AH Cano,A Romanou, A Bonnet‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2023",arxiv.org,,https://arxiv.org/abs/2311.16079,,Large language models (LLMs) can potentially democratize access to medical knowledge.While many efforts have been made to harness and improve LLMs' medical knowledge and¬†‚Ä¶,461.0,Methodology
BBQ,BBQ: A Hand-Built Bias Benchmark for Question Answering,A survey on evaluation of large language models,"Y Chang,X Wang,J Wang,Y Wu,L Yang‚Ä¶¬†- ACM transactions on¬†‚Ä¶, 2024",dl.acm.org,,https://dl.acm.org/doi/abs/10.1145/3641289,,"Large language models (LLMs) are gaining increasing popularity in both academia andindustry, owing to their unprecedented performance in various applications. As LLMs¬†‚Ä¶",4405.0,Survey
BBQ,BBQ: A Hand-Built Bias Benchmark for Question Answering,A comprehensive overview of large language models,"H Naveed,AU Khan,S Qiu,M Saqib,S Anwar‚Ä¶¬†- ACM Transactions on¬†‚Ä¶, 2025",dl.acm.org,,https://dl.acm.org/doi/abs/10.1145/3744746,,Large Language Models (LLMs) have recently demonstrated remarkable capabilities innatural language processing tasks and beyond. This success of LLMs has led to a large¬†‚Ä¶,1815.0,Survey
BBQ,BBQ: A Hand-Built Bias Benchmark for Question Answering,Gemini: a family of highly capable multimodal models,"G Team, R Anil, S Borgeaud, JB Alayrac, J Yu‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2023",arxiv.org,,https://arxiv.org/abs/2312.11805,,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkablecapabilities across image, audio, video, and text understanding. The Gemini family consists¬†‚Ä¶",6181.0,Methodology
BBQ,BBQ: A Hand-Built Bias Benchmark for Question Answering,Mixtral of experts,"AQ Jiang,A Sablayrolles, A Roux,A Mensch‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2024",arxiv.org,,https://arxiv.org/abs/2401.04088,,"We introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral hasthe same architecture as Mistral 7B, with the difference that each layer is composed of 8¬†‚Ä¶",2440.0,Methodology
BBQ,BBQ: A Hand-Built Bias Benchmark for Question Answering,Palm 2 technical report,"R Anil,AM Dai,O Firat,M Johnson,D Lepikhin‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2023",arxiv.org,,https://arxiv.org/abs/2305.10403,,"We introduce PaLM 2, a new state-of-the-art language model that has better multilingual andreasoning capabilities and is more compute-efficient than its predecessor PaLM. PaLM 2 is¬†‚Ä¶",2120.0,Methodology
BBQ,BBQ: A Hand-Built Bias Benchmark for Question Answering,Holistic evaluation of language models,"P Liang,R Bommasani,T Lee, D Tsipras‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2022",arxiv.org,,https://arxiv.org/abs/2211.09110,,"Language models (LMs) are becoming the foundation for almost all major languagetechnologies, but their capabilities, limitations, and risks are not well understood. We present¬†‚Ä¶",1905.0,Survey
BBQ,BBQ: A Hand-Built Bias Benchmark for Question Answering,Beavertails: Towards improved safety alignment of llm via a human-preference dataset,"J Ji,M Liu,J Dai,X Pan, C Zhang‚Ä¶¬†- Advances in¬†‚Ä¶, 2023",proceedings.neurips.cc,,https://proceedings.neurips.cc/paper_files/paper/2023/hash/4dbb61cb68671edc4ca3712d70083b9f-Abstract-Datasets_and_Benchmarks.html,,"In this paper, we introduce the BeaverTails dataset, aimed at fostering research on safetyalignment in large language models (LLMs). This dataset uniquely separates annotations of¬†‚Ä¶",665.0,Benchmark
BBQ,BBQ: A Hand-Built Bias Benchmark for Question Answering,Training a helpful and harmless assistant with reinforcement learning from human feedback,"Y Bai,A Jones,K Ndousse,A Askell, A Chen‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2022",arxiv.org,,https://arxiv.org/abs/2204.05862,,We apply preference modeling and reinforcement learning from human feedback (RLHF) tofinetune language models to act as helpful and harmless assistants. We find this alignment¬†‚Ä¶,2994.0,Methodology
BBQ,BBQ: A Hand-Built Bias Benchmark for Question Answering,Language models don't always say what they think: Unfaithful explanations in chain-of-thought prompting,"M Turpin,J Michael,E Perez‚Ä¶¬†- Advances in Neural¬†‚Ä¶, 2023",proceedings.neurips.cc,,https://proceedings.neurips.cc/paper_files/paper/2023/hash/ed3fea9033a80fea1376299fa7863f4a-Abstract-Conference.html,,"Abstract Large Language Models (LLMs) can achieve strong performance on many tasks byproducing step-by-step reasoning before giving a final output, often referred to as chain-of¬†‚Ä¶",668.0,Methodology
BBQ,BBQ: A Hand-Built Bias Benchmark for Question Answering,Beyond the imitation game: Quantifying and extrapolating the capabilities of language models,"A Srivastava, A Rastogi, A Rao,AAM Shoeb‚Ä¶¬†- ‚Ä¶¬†on machine learning¬†‚Ä¶, 2023",openreview.net,,https://openreview.net/forum?id=uyTL5Bvosj&nesting=2&sort=date-desc,,"Language models demonstrate both quantitative improvement and new qualitativecapabilities with increasing scale. Despite their potentially transformative impact, these new¬†‚Ä¶",2020.0,Methodology
BOLD,BOLD: Dataset and Metrics for Measuring Biases in Open-Ended Language Generation,A survey on evaluation of large language models,"Y Chang,X Wang,J Wang,Y Wu,L Yang‚Ä¶¬†- ACM transactions on¬†‚Ä¶, 2024",dl.acm.org,,https://dl.acm.org/doi/abs/10.1145/3641289,,"Large language models (LLMs) are gaining increasing popularity in both academia andindustry, owing to their unprecedented performance in various applications. As LLMs¬†‚Ä¶",4405.0,Survey
BOLD,BOLD: Dataset and Metrics for Measuring Biases in Open-Ended Language Generation,A comprehensive survey of ai-generated content (aigc): A history of generative ai from gan to chatgpt,"Y Cao,S Li,Y Liu,Z Yan,Y Dai,PS Yu‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2023",arxiv.org,,https://arxiv.org/abs/2303.04226,,"Recently, ChatGPT, along with DALL-E-2 and Codex, has been gaining significant attentionfrom society. As a result, many individuals have become interested in related resources and¬†‚Ä¶",1247.0,Survey
BOLD,BOLD: Dataset and Metrics for Measuring Biases in Open-Ended Language Generation,Mixtral of experts,"AQ Jiang,A Sablayrolles, A Roux,A Mensch‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2024",arxiv.org,,https://arxiv.org/abs/2401.04088,,"We introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral hasthe same architecture as Mistral 7B, with the difference that each layer is composed of 8¬†‚Ä¶",2440.0,Methodology
BOLD,BOLD: Dataset and Metrics for Measuring Biases in Open-Ended Language Generation,Llama 2: Open foundation and fine-tuned chat models,"H Touvron,L Martin,K Stone, P Albert‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2023",arxiv.org,,https://arxiv.org/abs/2307.09288,,"In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned largelanguage models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine¬†‚Ä¶",19070.0,Methodology
BOLD,BOLD: Dataset and Metrics for Measuring Biases in Open-Ended Language Generation,Holistic evaluation of language models,"P Liang,R Bommasani,T Lee, D Tsipras‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2022",arxiv.org,,https://arxiv.org/abs/2211.09110,,"Language models (LMs) are becoming the foundation for almost all major languagetechnologies, but their capabilities, limitations, and risks are not well understood. We present¬†‚Ä¶",1905.0,Survey
BOLD,BOLD: Dataset and Metrics for Measuring Biases in Open-Ended Language Generation,Training language models to follow instructions with human feedback,"L Ouyang,J Wu, X Jiang, D Almeida‚Ä¶¬†- Advances in neural¬†‚Ä¶, 2022",proceedings.neurips.cc,,https://proceedings.neurips.cc/paper_files/paper/2022/hash/b1efde53be364a73914f58805a001731-Abstract-Conference.html,,"Making language models bigger does not inherently make them better at following a user'sintent. For example, large language models can generate outputs that are untruthful, toxic, or¬†‚Ä¶",19069.0,Methodology
BOLD,BOLD: Dataset and Metrics for Measuring Biases in Open-Ended Language Generation,Whose opinions do language models reflect?,"S Santurkar,E Durmus,F Ladhak‚Ä¶¬†- International¬†‚Ä¶, 2023",proceedings.mlr.press,,https://proceedings.mlr.press/v202/santurkar23a?utm_source=chatgpt.com,,"Abstract Language models (LMs) are increasingly being used in open-ended contexts,where the opinions they reflect in response to subjective queries can have a profound¬†‚Ä¶",693.0,Other
BOLD,BOLD: Dataset and Metrics for Measuring Biases in Open-Ended Language Generation,DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models.,"B Wang,W Chen,H Pei,C Xie,M Kang,C Zhang,C Xu‚Ä¶¬†- NeurIPS, 2023",blogs.qub.ac.uk,,https://blogs.qub.ac.uk/wp-content/uploads/sites/7/2024/01/A-comprehensive-Assessment-of-Trustworthiness-in-GPT-Models.pdf,,"Abstract Generative Pre-trained Transformer (GPT) models have exhibited exciting progressin their capabilities, capturing the interest of practitioners and the public alike. Yet, while the¬†‚Ä¶",625.0,Survey
BOLD,BOLD: Dataset and Metrics for Measuring Biases in Open-Ended Language Generation,Starcoder 2 and the stack v2: The next generation,"A Lozhkov,R Li,LB Allal,F Cassano‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2024",arxiv.org,,https://arxiv.org/abs/2402.19173,,"The BigCode project, an open-scientific collaboration focused on the responsibledevelopment of Large Language Models for Code (Code LLMs), introduces StarCoder2. In¬†‚Ä¶",456.0,Methodology
BOLD,BOLD: Dataset and Metrics for Measuring Biases in Open-Ended Language Generation,Agieval: A human-centric benchmark for evaluating foundation models,"W Zhong,R Cui,Y Guo,Y Liang,S Lu,Y Wang‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2023",arxiv.org,,https://arxiv.org/abs/2304.06364,,Evaluating the general abilities of foundation models to tackle human-level tasks is a vitalaspect of their development and application in the pursuit of Artificial General Intelligence¬†‚Ä¶,599.0,Benchmark
JailbreakBench,JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models,"A comprehensive survey of small language models in the era of large language models: Techniques, enhancements, applications, collaboration with llms, and¬†‚Ä¶","F Wang,Z Zhang,X Zhang,Z Wu, T Mo,Q Lu‚Ä¶¬†- ACM Transactions on¬†‚Ä¶, 2024",dl.acm.org,,https://dl.acm.org/doi/abs/10.1145/3768165,,"Large language models (LLMs) have demonstrated emergent abilities in text generation,question answering, and reasoning, facilitating various tasks and domains. Despite their¬†‚Ä¶",111.0,Survey
JailbreakBench,JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models,"A comprehensive survey in llm (-agent) full stack safety: Data, training and deployment","K Wang,G Zhang,Z Zhou,J Wu,M Yu,S Zhao‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2025",arxiv.org,,https://arxiv.org/abs/2504.15585,,The remarkable success of Large Language Models (LLMs) has illuminated a promisingpathway toward achieving Artificial General Intelligence for both academic and industrial¬†‚Ä¶,61.0,Survey
JailbreakBench,JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models,Jailbreaking black box large language models in twenty queries,"P Chao,A Robey,E Dobriban‚Ä¶¬†- ‚Ä¶¬†IEEE Conference on¬†‚Ä¶, 2025",ieeexplore.ieee.org,,https://ieeexplore.ieee.org/abstract/document/10992337/,,"There is growing interest in ensuring that large language models (LLMs) align with humanvalues. However, the alignment of such models is vulnerable to adversarial jailbreaks, which¬†‚Ä¶",953.0,Methodology
JailbreakBench,JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models,Refusal in language models is mediated by a single direction,"A Arditi,O Obeso,A Syed,D Paleka‚Ä¶¬†- Advances in¬†‚Ä¶, 2024",proceedings.neurips.cc,,https://proceedings.neurips.cc/paper_files/paper/2024/hash/f545448535dfde4f9786555403ab7c49-Abstract-Conference.html,,"Conversational large language models are fine-tuned for both instruction-following andsafety, resulting in models that obey benign requests but refuse harmful ones. While this¬†‚Ä¶",265.0,Methodology
JailbreakBench,JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models,Jailbreaking leading safety-aligned llms with simple adaptive attacks,"M Andriushchenko,F Croce,N Flammarion- arXiv preprint arXiv¬†‚Ä¶, 2024",arxiv.org,,https://arxiv.org/abs/2404.02151,,"We show that even the most recent safety-aligned LLMs are not robust to simple adaptivejailbreaking attacks. First, we demonstrate how to successfully leverage access to logprobs¬†‚Ä¶",284.0,Methodology
JailbreakBench,JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models,Deepinception: Hypnotize large language model to be jailbreaker,"X Li,Z Zhou,J Zhu,J Yao,T Liu,B Han- arXiv preprint arXiv:2311.03191, 2023",arxiv.org,,https://arxiv.org/abs/2311.03191,,"Despite remarkable success in various applications, large language models (LLMs) arevulnerable to adversarial jailbreaks that make the safety guardrails void. However, previous¬†‚Ä¶",282.0,Methodology
JailbreakBench,JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models,Rainbow teaming: Open-ended generation of diverse adversarial prompts,"M Samvelyan, SC Raparthy,A Lupu‚Ä¶¬†- Advances in¬†‚Ä¶, 2024",proceedings.neurips.cc,,https://proceedings.neurips.cc/paper_files/paper/2024/hash/8147a43d030b43a01020774ae1d3e3bb-Abstract-Conference.html,,"As large language models (LLMs) become increasingly prevalent across many real-worldapplications, understanding and enhancing their robustness to adversarial attacks is of¬†‚Ä¶",118.0,Methodology
JailbreakBench,JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models,Deliberative alignment: Reasoning enables safer language models,"MY Guan,M Joglekar,E Wallace,S Jain‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2024",arxiv.org,,https://arxiv.org/abs/2412.16339,,"As large-scale language models increasingly impact safety-critical domains, ensuring theirreliable adherence to well-defined principles remains a fundamental challenge. We¬†‚Ä¶",153.0,Methodology
JailbreakBench,JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models,Sorry-bench: Systematically evaluating large language model safety refusal,"T Xie,X Qi,Y Zeng,Y Huang,UM Sehwag‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2024",arxiv.org,,https://arxiv.org/abs/2406.14598,,"Evaluating aligned large language models'(LLMs) ability to recognize and reject unsafe userrequests is crucial for safe, policy-compliant deployments. Existing evaluation efforts¬†‚Ä¶",141.0,Benchmark
JailbreakBench,JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models,Jailbreak attacks and defenses against large language models: A survey,"S Yi,Y Liu,Z Sun,T Cong,X He, J Song,K Xu‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2024",arxiv.org,,https://arxiv.org/abs/2407.04295,,"Large Language Models (LLMs) have performed exceptionally in various text-generativetasks, including question answering, translation, code completion, etc. However, the over¬†‚Ä¶",200.0,Survey
HarmBench,HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal,"A comprehensive survey in llm (-agent) full stack safety: Data, training and deployment","K Wang,G Zhang,Z Zhou,J Wu,M Yu,S Zhao‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2025",arxiv.org,,https://arxiv.org/abs/2504.15585,,The remarkable success of Large Language Models (LLMs) has illuminated a promisingpathway toward achieving Artificial General Intelligence for both academic and industrial¬†‚Ä¶,61.0,Survey
HarmBench,HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal,Adversarial attacks of vision tasks in the past 10 years: A survey,"C Zhang, L Zhou,X Xu, J Wu,Z Liu- ACM Computing Surveys, 2025",dl.acm.org,,https://dl.acm.org/doi/abs/10.1145/3743126,,"With the advent of Large Vision-Language Models (LVLMs), new attack vectors, such ascognitive bias, prompt injection, and jailbreaking, have emerged. Understanding these¬†‚Ä¶",17.0,Survey
HarmBench,HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal,Jailbreakbench: An open robustness benchmark for jailbreaking large language models,"P Chao,E Debenedetti,A Robey‚Ä¶¬†- Advances in¬†‚Ä¶, 2024",proceedings.neurips.cc,,https://proceedings.neurips.cc/paper_files/paper/2024/hash/63092d79154adebd7305dfd498cbff70-Abstract-Datasets_and_Benchmarks_Track.html,,"Jailbreak attacks cause large language models (LLMs) to generate harmful, unethical, orotherwise objectionable content. Evaluating these attacks presents a number of challenges¬†‚Ä¶",350.0,Benchmark
HarmBench,HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal,Tree of attacks: Jailbreaking black-box llms automatically,"A Mehrotra,M Zampetakis‚Ä¶¬†- Advances in¬†‚Ä¶, 2024",proceedings.neurips.cc,,https://proceedings.neurips.cc/paper_files/paper/2024/hash/70702e8cbb4890b4a467b984ae59828a-Abstract-Conference.html,,"Abstract While Large Language Models (LLMs) display versatile functionality, they continueto generate harmful, biased, and toxic content, as demonstrated by the prevalence of human¬†‚Ä¶",424.0,Methodology
HarmBench,HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal,Refusal in language models is mediated by a single direction,"A Arditi,O Obeso,A Syed,D Paleka‚Ä¶¬†- Advances in¬†‚Ä¶, 2024",proceedings.neurips.cc,,https://proceedings.neurips.cc/paper_files/paper/2024/hash/f545448535dfde4f9786555403ab7c49-Abstract-Conference.html,,"Conversational large language models are fine-tuned for both instruction-following andsafety, resulting in models that obey benign requests but refuse harmful ones. While this¬†‚Ä¶",265.0,Methodology
HarmBench,HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal,The wmdp benchmark: Measuring and reducing malicious use with unlearning,"N Li,A Pan,A Gopal, S Yue, D Berrios,A Gatti‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2024",arxiv.org,,https://arxiv.org/abs/2403.03218,,"The White House Executive Order on Artificial Intelligence highlights the risks of largelanguage models (LLMs) empowering malicious actors in developing biological, cyber, and¬†‚Ä¶",284.0,Benchmark
HarmBench,HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal,Jailbreaking leading safety-aligned llms with simple adaptive attacks,"M Andriushchenko,F Croce,N Flammarion- arXiv preprint arXiv¬†‚Ä¶, 2024",arxiv.org,,https://arxiv.org/abs/2404.02151,,"We show that even the most recent safety-aligned LLMs are not robust to simple adaptivejailbreaking attacks. First, we demonstrate how to successfully leverage access to logprobs¬†‚Ä¶",284.0,Methodology
HarmBench,HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal,Figstep: Jailbreaking large vision-language models via typographic visual prompts,"Y Gong, D Ran,J Liu, C Wang,T Cong‚Ä¶¬†- Proceedings of the¬†‚Ä¶, 2025",ojs.aaai.org,,https://ojs.aaai.org/index.php/AAAI/article/view/34568,,"Abstract Large Vision-Language Models (LVLMs) signify a groundbreaking paradigm shiftwithin the Artificial Intelligence (AI) community, extending beyond the capabilities of Large¬†‚Ä¶",264.0,Methodology
HarmBench,HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal,"Wildguard: Open one-stop moderation tools for safety risks, jailbreaks, and refusals of llms","S Han,K Rao,A Ettinger,L Jiang‚Ä¶¬†- Advances in¬†‚Ä¶, 2024",proceedings.neurips.cc,,https://proceedings.neurips.cc/paper_files/paper/2024/hash/0f69b4b96a46f284b726fbd70f74fb3b-Abstract-Datasets_and_Benchmarks_Track.html,,"We introduce WildGuard---an open, light-weight moderation tool for LLM safety thatachieves three goals:(1) identifying malicious intent in user prompts,(2) detecting safety risks¬†‚Ä¶",169.0,Methodology
HarmBench,HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal,Tulu 3: Pushing frontiers in open language model post-training,"N Lambert,J Morrison,V Pyatkin,S Huang‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2024",arxiv.org,,https://arxiv.org/abs/2411.15124,,"Language model post-training is applied to refine behaviors and unlock new skills across awide range of recent language models, but open recipes for applying these techniques lag¬†‚Ä¶",252.0,Methodology
DecodingTrust,DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models,A survey on evaluation of large language models,"Y Chang,X Wang,J Wang,Y Wu,L Yang‚Ä¶¬†- ACM transactions on¬†‚Ä¶, 2024",dl.acm.org,,https://dl.acm.org/doi/abs/10.1145/3641289,,"Large language models (LLMs) are gaining increasing popularity in both academia andindustry, owing to their unprecedented performance in various applications. As LLMs¬†‚Ä¶",4405.0,Survey
DecodingTrust,DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models,Mm-llms: Recent advances in multimodal large language models,"D Zhang,Y Yu,J Dong,C Li,D Su,C Chu‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2024",arxiv.org,,https://arxiv.org/abs/2401.13601,,"In the past year, MultiModal Large Language Models (MM-LLMs) have undergonesubstantial advancements, augmenting off-the-shelf LLMs to support MM inputs or outputs¬†‚Ä¶",493.0,Survey
DecodingTrust,DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models,Simpo: Simple preference optimization with a reference-free reward,"Y Meng,M Xia,D Chen- Advances in Neural Information¬†‚Ä¶, 2024",proceedings.neurips.cc,,https://proceedings.neurips.cc/paper_files/paper/2024/hash/e099c1c9699814af0be873a175361713-Abstract-Conference.html,,Abstract Direct Preference Optimization (DPO) is a widely used offline preferenceoptimization algorithm that reparameterizes reward functions in reinforcement learning from¬†‚Ä¶,657.0,Methodology
DecodingTrust,DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models,""" do anything now"": Characterizing and evaluating in-the-wild jailbreak prompts on large language models","X Shen,Z Chen,M Backes,Y Shen‚Ä¶¬†- Proceedings of the 2024 on¬†‚Ä¶, 2024",dl.acm.org,,https://dl.acm.org/doi/abs/10.1145/3658644.3670388,,"The misuse of large language models (LLMs) has drawn significant attention from thegeneral public and LLM vendors. One particular type of adversarial prompt, known as¬†‚Ä¶",849.0,Other
DecodingTrust,DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models,Jailbreakbench: An open robustness benchmark for jailbreaking large language models,"P Chao,E Debenedetti,A Robey‚Ä¶¬†- Advances in¬†‚Ä¶, 2024",proceedings.neurips.cc,,https://proceedings.neurips.cc/paper_files/paper/2024/hash/63092d79154adebd7305dfd498cbff70-Abstract-Datasets_and_Benchmarks_Track.html,,"Jailbreak attacks cause large language models (LLMs) to generate harmful, unethical, orotherwise objectionable content. Evaluating these attacks presents a number of challenges¬†‚Ä¶",350.0,Benchmark
DecodingTrust,DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models,Trustllm: Trustworthiness in large language models,"Y Huang,L Sun,H Wang, S Wu,Q Zhang, Y Li‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2024",arxiv.org,,https://arxiv.org/abs/2401.05561,,"Large language models (LLMs), exemplified by ChatGPT, have gained considerableattention for their excellent natural language processing capabilities. Nonetheless, these¬†‚Ä¶",487.0,Methodology
DecodingTrust,DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models,Trustworthy llms: a survey and guideline for evaluating large language models' alignment,"Y Liu,Y Yao,JF Ton,X Zhang,R Guo,H Cheng‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2023",arxiv.org,,https://arxiv.org/abs/2308.05374,,"Ensuring alignment, which refers to making models behave in accordance with humanintentions [1, 2], has become a critical task before deploying large language models (LLMs)¬†‚Ä¶",516.0,Survey
DecodingTrust,DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models,Gptfuzzer: Red teaming large language models with auto-generated jailbreak prompts,"J Yu,X Lin,Z Yu,X Xing- arXiv preprint arXiv:2309.10253, 2023",arxiv.org,,https://arxiv.org/abs/2309.10253,,"Large language models (LLMs) have recently experienced tremendous popularity and arewidely used from casual conversations to AI-driven programming. However, despite their¬†‚Ä¶",446.0,Methodology
DecodingTrust,DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models,Combating misinformation in the age of llms: Opportunities and challenges,"C Chen,K Shu- AI Magazine, 2024",Wiley Online Library,,https://onlinelibrary.wiley.com/doi/abs/10.1002/aaai.12188,,Misinformation such as fake news and rumors is a serious threat for information ecosystemsand public trust. The emergence of large language models (LLMs) has great potential to¬†‚Ä¶,279.0,Other
DecodingTrust,DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models,Medical large language models are vulnerable to data-poisoning attacks,"DA Alber,Z Yang,A Alyakin,E Yang,S Rai‚Ä¶¬†- Nature Medicine, 2025",nature.com,,https://www.nature.com/articles/s41591-024-03445-1,,The adoption of large language models (LLMs) in healthcare demands a careful analysis oftheir potential to spread false medical knowledge. Because LLMs ingest massive volumes of¬†‚Ä¶,117.0,Other
AdvBench,Universal and Transferable Adversarial Attacks on Aligned Language Models,"A survey on large language model (llm) security and privacy: The good, the bad, and the ugly","Y Yao,J Duan,K Xu, Y Cai,Z Sun,Y Zhang- High-Confidence Computing, 2024",Elsevier,,https://www.sciencedirect.com/science/article/pii/S266729522400014X,,"Abstract Large Language Models (LLMs), such as ChatGPT and Bard, have revolutionizednatural language understanding and generation. They possess deep language¬†‚Ä¶",1335.0,Survey
AdvBench,Universal and Transferable Adversarial Attacks on Aligned Language Models,The rise and potential of large language model based agents: A survey,"Z Xi,W Chen,X Guo,W He,Y Ding, B Hong‚Ä¶¬†- Science China¬†‚Ä¶, 2025",Springer,,https://link.springer.com/article/10.1007/s11432-024-4222-0,,"For a long time, researchers have sought artificial intelligence (AI) that matches or exceedshuman intelligence. AI agents, which are artificial entities capable of sensing the¬†‚Ä¶",1558.0,Survey
AdvBench,Universal and Transferable Adversarial Attacks on Aligned Language Models,Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context,"G Team,P Georgiev, VI Lei, R Burnell, L Bai‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2024",arxiv.org,,https://arxiv.org/abs/2403.05530,,"In this report, we introduce the Gemini 1.5 family of models, representing the next generationof highly compute-efficient multimodal models capable of recalling and reasoning over fine¬†‚Ä¶",2793.0,Methodology
AdvBench,Universal and Transferable Adversarial Attacks on Aligned Language Models,üßú Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models,"Y Zhang,Y Li,L Cui,D Cai,L Liu,T Fu‚Ä¶¬†- Computational¬†‚Ä¶, 2025",direct.mit.edu,,https://direct.mit.edu/coli/article/doi/10.1162/coli.a.16/131631,,"While large language models (LLMs) have demonstrated remarkable capabilities across arange of downstream tasks, a significant concern revolves around their propensity to exhibit¬†‚Ä¶",1549.0,Survey
AdvBench,Universal and Transferable Adversarial Attacks on Aligned Language Models,Jailbreaking black box large language models in twenty queries,"P Chao,A Robey,E Dobriban‚Ä¶¬†- ‚Ä¶¬†IEEE Conference on¬†‚Ä¶, 2025",ieeexplore.ieee.org,,https://ieeexplore.ieee.org/abstract/document/10992337/,,"There is growing interest in ensuring that large language models (LLMs) align with humanvalues. However, the alignment of such models is vulnerable to adversarial jailbreaks, which¬†‚Ä¶",953.0,Methodology
AdvBench,Universal and Transferable Adversarial Attacks on Aligned Language Models,""" do anything now"": Characterizing and evaluating in-the-wild jailbreak prompts on large language models","X Shen,Z Chen,M Backes,Y Shen‚Ä¶¬†- Proceedings of the 2024 on¬†‚Ä¶, 2024",dl.acm.org,,https://dl.acm.org/doi/abs/10.1145/3658644.3670388,,"The misuse of large language models (LLMs) has drawn significant attention from thegeneral public and LLM vendors. One particular type of adversarial prompt, known as¬†‚Ä¶",849.0,Other
AdvBench,Universal and Transferable Adversarial Attacks on Aligned Language Models,"Fine-tuning aligned language models compromises safety, even when users do not intend to!","X Qi,Y Zeng,T Xie,PY Chen,R Jia,P Mittal‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2023",arxiv.org,,https://arxiv.org/abs/2310.03693,,Optimizing large language models (LLMs) for downstream use cases often involves thecustomization of pre-trained LLMs through further fine-tuning. Meta's open release of Llama¬†‚Ä¶,841.0,Other
AdvBench,Universal and Transferable Adversarial Attacks on Aligned Language Models,Jailbreakbench: An open robustness benchmark for jailbreaking large language models,"P Chao,E Debenedetti,A Robey‚Ä¶¬†- Advances in¬†‚Ä¶, 2024",proceedings.neurips.cc,,https://proceedings.neurips.cc/paper_files/paper/2024/hash/63092d79154adebd7305dfd498cbff70-Abstract-Datasets_and_Benchmarks_Track.html,,"Jailbreak attacks cause large language models (LLMs) to generate harmful, unethical, orotherwise objectionable content. Evaluating these attacks presents a number of challenges¬†‚Ä¶",350.0,Benchmark
AdvBench,Universal and Transferable Adversarial Attacks on Aligned Language Models,Large language models cannot self-correct reasoning yet,"J Huang,X Chen,S Mishra,HS Zheng,AW Yu‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2023",arxiv.org,,https://arxiv.org/abs/2310.01798,,Large Language Models (LLMs) have emerged as a groundbreaking technology with theirunparalleled text generation capabilities across various applications. Nevertheless¬†‚Ä¶,643.0,Other
AdvBench,Universal and Transferable Adversarial Attacks on Aligned Language Models,Tree of attacks: Jailbreaking black-box llms automatically,"A Mehrotra,M Zampetakis‚Ä¶¬†- Advances in¬†‚Ä¶, 2024",proceedings.neurips.cc,,https://proceedings.neurips.cc/paper_files/paper/2024/hash/70702e8cbb4890b4a467b984ae59828a-Abstract-Conference.html,,"Abstract While Large Language Models (LLMs) display versatile functionality, they continueto generate harmful, biased, and toxic content, as demonstrated by the prevalence of human¬†‚Ä¶",424.0,Methodology
HarmfulQA,Red-Teaming Large Language Models using Chain of Utterances for Safety-Alignment,A survey of GPT-3 family large language models including ChatGPT and GPT-4,"KS Kalyan- Natural Language Processing Journal, 2024",Elsevier,,https://www.sciencedirect.com/science/article/pii/S2949719123000456,,"Large language models (LLMs) are a special class of pretrained language models (PLMs)obtained by scaling model size, pretraining corpus and computation. LLMs, because of their¬†‚Ä¶",512.0,Survey
HarmfulQA,Red-Teaming Large Language Models using Chain of Utterances for Safety-Alignment,Survey of vulnerabilities in large language models revealed by adversarial attacks,"E Shayegani,MAA Mamun,Y Fu,P Zaree‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2023",arxiv.org,,https://arxiv.org/abs/2310.10844,,"Large Language Models (LLMs) are swiftly advancing in architecture and capability, and asthey integrate more deeply into complex systems, the urgency to scrutinize their security¬†‚Ä¶",234.0,Survey
HarmfulQA,Red-Teaming Large Language Models using Chain of Utterances for Safety-Alignment,Trustllm: Trustworthiness in large language models,"Y Huang,L Sun,H Wang, S Wu,Q Zhang, Y Li‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2024",arxiv.org,,https://arxiv.org/abs/2401.05561,,"Large language models (LLMs), exemplified by ChatGPT, have gained considerableattention for their excellent natural language processing capabilities. Nonetheless, these¬†‚Ä¶",487.0,Methodology
HarmfulQA,Red-Teaming Large Language Models using Chain of Utterances for Safety-Alignment,Smoothllm: Defending large language models against jailbreaking attacks,"A Robey,E Wong,H Hassani,GJ Pappas- arXiv preprint arXiv¬†‚Ä¶, 2023",arxiv.org,,https://arxiv.org/abs/2310.03684,,"Despite efforts to align large language models (LLMs) with human intentions, widely-usedLLMs such as GPT, Llama, and Claude are susceptible to jailbreaking attacks, wherein an¬†‚Ä¶",417.0,Methodology
HarmfulQA,Red-Teaming Large Language Models using Chain of Utterances for Safety-Alignment,Multilingual jailbreak challenges in large language models,"Y Deng,W Zhang,SJ Pan,L Bing- arXiv preprint arXiv:2310.06474, 2023",arxiv.org,,https://arxiv.org/abs/2310.06474,,"While large language models (LLMs) exhibit remarkable capabilities across a wide range oftasks, they pose potential safety concerns, such as the``jailbreak''problem, wherein¬†‚Ä¶",359.0,Other
HarmfulQA,Red-Teaming Large Language Models using Chain of Utterances for Safety-Alignment,Salad-bench: A hierarchical and comprehensive safety benchmark for large language models,"L Li,B Dong,R Wang,X Hu,W Zuo,D Lin‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2024",arxiv.org,,https://arxiv.org/abs/2402.05044,,"In the rapidly evolving landscape of Large Language Models (LLMs), ensuring robust safetymeasures is paramount. To meet this crucial need, we propose\emph {SALAD-Bench}, a¬†‚Ä¶",197.0,Benchmark
HarmfulQA,Red-Teaming Large Language Models using Chain of Utterances for Safety-Alignment,Jailbreak attacks and defenses against large language models: A survey,"S Yi,Y Liu,Z Sun,T Cong,X He, J Song,K Xu‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2024",arxiv.org,,https://arxiv.org/abs/2407.04295,,"Large Language Models (LLMs) have performed exceptionally in various text-generativetasks, including question answering, translation, code completion, etc. However, the over¬†‚Ä¶",200.0,Survey
HarmfulQA,Red-Teaming Large Language Models using Chain of Utterances for Safety-Alignment,Red-teaming for generative AI: Silver bullet or security theater?,"M Feffer,A Sinha,WH Deng,ZC Lipton‚Ä¶¬†- Proceedings of the AAAI¬†‚Ä¶, 2024",ojs.aaai.org,,https://ojs.aaai.org/index.php/AIES/article/view/31647,,"In response to rising concerns surrounding the safety, security, and trustworthiness ofGenerative AI (GenAI) models, practitioners and regulators alike have pointed to AI red¬†‚Ä¶",115.0,Other
HarmfulQA,Red-Teaming Large Language Models using Chain of Utterances for Safety-Alignment,R-judge: Benchmarking safety risk awareness for llm agents,"T Yuan,Z He,L Dong,Y Wang, R Zhao, T Xia‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2024",arxiv.org,,https://arxiv.org/abs/2401.10019,,"Large language models (LLMs) have exhibited great potential in autonomously completingtasks across real-world applications. Despite this, these LLM agents introduce unexpected¬†‚Ä¶",135.0,Benchmark
HarmfulQA,Red-Teaming Large Language Models using Chain of Utterances for Safety-Alignment,T2vsafetybench: Evaluating the safety of text-to-video generative models,"Y Miao, Y Zhu, L Yu,J Zhu,XS Gao‚Ä¶¬†- Advances in Neural¬†‚Ä¶, 2024",proceedings.neurips.cc,,https://proceedings.neurips.cc/paper_files/paper/2024/hash/74eed5f568354c2e77dd9b018f38a9d4-Abstract-Datasets_and_Benchmarks_Track.html,,The recent development of Sora leads to a new era in text-to-video (T2V) generation. Alongwith this comes the rising concern about its safety risks. The generated videos may contain¬†‚Ä¶,27.0,Benchmark
DoNotAnswer,Do-Not-Answer: A Dataset for Evaluating Safeguards in LLMs,Combating misinformation in the age of llms: Opportunities and challenges,"C Chen,K Shu- AI Magazine, 2024",Wiley Online Library,,https://onlinelibrary.wiley.com/doi/abs/10.1002/aaai.12188,,Misinformation such as fake news and rumors is a serious threat for information ecosystemsand public trust. The emergence of large language models (LLMs) has great potential to¬†‚Ä¶,279.0,Other
DoNotAnswer,Do-Not-Answer: A Dataset for Evaluating Safeguards in LLMs,A survey of confidence estimation and calibration in large language models,"J Geng,F Cai,Y Wang,H Koeppl,P Nakov‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2023",arxiv.org,,https://arxiv.org/abs/2311.08298,,"Large language models (LLMs) have demonstrated remarkable capabilities across a widerange of tasks in various domains. Despite their impressive performance, they can be¬†‚Ä¶",149.0,Survey
DoNotAnswer,Do-Not-Answer: A Dataset for Evaluating Safeguards in LLMs,Trustllm: Trustworthiness in large language models,"Y Huang,L Sun,H Wang, S Wu,Q Zhang, Y Li‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2024",arxiv.org,,https://arxiv.org/abs/2401.05561,,"Large language models (LLMs), exemplified by ChatGPT, have gained considerableattention for their excellent natural language processing capabilities. Nonetheless, these¬†‚Ä¶",487.0,Methodology
DoNotAnswer,Do-Not-Answer: A Dataset for Evaluating Safeguards in LLMs,Rewardbench: Evaluating reward models for language modeling,"N Lambert,V Pyatkin,J Morrison,LJ Miranda‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2024",arxiv.org,,https://arxiv.org/abs/2403.13787,,"Reward models (RMs) are at the crux of successfully using RLHF to align pretrained modelsto human preferences, yet there has been relatively little study that focuses on evaluation of¬†‚Ä¶",393.0,Benchmark
DoNotAnswer,Do-Not-Answer: A Dataset for Evaluating Safeguards in LLMs,Low-resource languages jailbreak gpt-4,"ZX Yong,C Menghini,SH Bach- arXiv preprint arXiv:2310.02446, 2023",arxiv.org,,https://arxiv.org/abs/2310.02446,,AI safety training and red-teaming of large language models (LLMs) are measures tomitigate the generation of unsafe content. Our work exposes the inherent cross-lingual¬†‚Ä¶,313.0,Other
DoNotAnswer,Do-Not-Answer: A Dataset for Evaluating Safeguards in LLMs,Salad-bench: A hierarchical and comprehensive safety benchmark for large language models,"L Li,B Dong,R Wang,X Hu,W Zuo,D Lin‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2024",arxiv.org,,https://arxiv.org/abs/2402.05044,,"In the rapidly evolving landscape of Large Language Models (LLMs), ensuring robust safetymeasures is paramount. To meet this crucial need, we propose\emph {SALAD-Bench}, a¬†‚Ä¶",197.0,Benchmark
DoNotAnswer,Do-Not-Answer: A Dataset for Evaluating Safeguards in LLMs,Position: Trustllm: Trustworthiness in large language models,"Y Huang,L Sun,H Wang, S Wu‚Ä¶¬†- International¬†‚Ä¶, 2024",proceedings.mlr.press,,http://proceedings.mlr.press/v235/huang24x.html,,"Large language models (LLMs) have gained considerable attention for their excellentnatural language processing capabilities. Nonetheless, these LLMs present many¬†‚Ä¶",88.0,Methodology
DoNotAnswer,Do-Not-Answer: A Dataset for Evaluating Safeguards in LLMs,Sorry-bench: Systematically evaluating large language model safety refusal,"T Xie,X Qi,Y Zeng,Y Huang,UM Sehwag‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2024",arxiv.org,,https://arxiv.org/abs/2406.14598,,"Evaluating aligned large language models'(LLMs) ability to recognize and reject unsafe userrequests is crucial for safe, policy-compliant deployments. Existing evaluation efforts¬†‚Ä¶",141.0,Benchmark
DoNotAnswer,Do-Not-Answer: A Dataset for Evaluating Safeguards in LLMs,Jailbreak attacks and defenses against large language models: A survey,"S Yi,Y Liu,Z Sun,T Cong,X He, J Song,K Xu‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2024",arxiv.org,,https://arxiv.org/abs/2407.04295,,"Large Language Models (LLMs) have performed exceptionally in various text-generativetasks, including question answering, translation, code completion, etc. However, the over¬†‚Ä¶",200.0,Survey
DoNotAnswer,Do-Not-Answer: A Dataset for Evaluating Safeguards in LLMs,Judgebench: A benchmark for evaluating llm-based judges,"S Tan,S Zhuang,K Montgomery, WY Tang‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2024",arxiv.org,,https://arxiv.org/abs/2410.12784,,"LLM-based judges have emerged as a scalable alternative to human evaluation and areincreasingly used to assess, compare, and improve models. However, the reliability of LLM¬†‚Ä¶",108.0,Benchmark
SALAD-Bench,SALAD-Bench: A Hierarchical and Comprehensive Safety Benchmark for Large Language Models,"A comprehensive survey in llm (-agent) full stack safety: Data, training and deployment","K Wang,G Zhang,Z Zhou,J Wu,M Yu,S Zhao‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2025",arxiv.org,,https://arxiv.org/abs/2504.15585,,The remarkable success of Large Language Models (LLMs) has illuminated a promisingpathway toward achieving Artificial General Intelligence for both academic and industrial¬†‚Ä¶,61.0,Survey
SALAD-Bench,SALAD-Bench: A Hierarchical and Comprehensive Safety Benchmark for Large Language Models,Adversarial attacks of vision tasks in the past 10 years: A survey,"C Zhang, L Zhou,X Xu, J Wu,Z Liu- ACM Computing Surveys, 2025",dl.acm.org,,https://dl.acm.org/doi/abs/10.1145/3743126,,"With the advent of Large Vision-Language Models (LVLMs), new attack vectors, such ascognitive bias, prompt injection, and jailbreaking, have emerged. Understanding these¬†‚Ä¶",17.0,Survey
SALAD-Bench,SALAD-Bench: A Hierarchical and Comprehensive Safety Benchmark for Large Language Models,A survey on llm-as-a-judge,"J Gu,X Jiang,Z Shi,H Tan,X Zhai,C Xu,W Li‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2024",arxiv.org,,https://arxiv.org/abs/2411.15594,,"Accurate and consistent evaluation is crucial for decision-making across numerous fields,yet it remains a challenging task due to inherent subjectivity, variability, and scale. Large¬†‚Ä¶",714.0,Survey
SALAD-Bench,SALAD-Bench: A Hierarchical and Comprehensive Safety Benchmark for Large Language Models,"Wildguard: Open one-stop moderation tools for safety risks, jailbreaks, and refusals of llms","S Han,K Rao,A Ettinger,L Jiang‚Ä¶¬†- Advances in¬†‚Ä¶, 2024",proceedings.neurips.cc,,https://proceedings.neurips.cc/paper_files/paper/2024/hash/0f69b4b96a46f284b726fbd70f74fb3b-Abstract-Datasets_and_Benchmarks_Track.html,,"We introduce WildGuard---an open, light-weight moderation tool for LLM safety thatachieves three goals:(1) identifying malicious intent in user prompts,(2) detecting safety risks¬†‚Ä¶",169.0,Methodology
SALAD-Bench,SALAD-Bench: A Hierarchical and Comprehensive Safety Benchmark for Large Language Models,Sorry-bench: Systematically evaluating large language model safety refusal,"T Xie,X Qi,Y Zeng,Y Huang,UM Sehwag‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2024",arxiv.org,,https://arxiv.org/abs/2406.14598,,"Evaluating aligned large language models'(LLMs) ability to recognize and reject unsafe userrequests is crucial for safe, policy-compliant deployments. Existing evaluation efforts¬†‚Ä¶",141.0,Benchmark
SALAD-Bench,SALAD-Bench: A Hierarchical and Comprehensive Safety Benchmark for Large Language Models,Or-bench: An over-refusal benchmark for large language models,"J Cui,WL Chiang,I Stoica,CJ Hsieh- arXiv preprint arXiv:2405.20947, 2024",arxiv.org,,https://arxiv.org/abs/2405.20947,,"Large Language Models (LLMs) require careful safety alignment to prevent maliciousoutputs. While significant research focuses on mitigating harmful content generation, the¬†‚Ä¶",104.0,Benchmark
SALAD-Bench,SALAD-Bench: A Hierarchical and Comprehensive Safety Benchmark for Large Language Models,Optimization-based prompt injection attack to llm-as-a-judge,"J Shi,Z Yuan,Y Liu,Y Huang,P Zhou,L Sun‚Ä¶¬†- Proceedings of the¬†‚Ä¶, 2024",dl.acm.org,,https://dl.acm.org/doi/abs/10.1145/3658644.3690291,,LLM-as-a-Judge uses a large language model (LLM) to select the best response from a setof candidates for a given question. LLM-as-a-Judge has many applications such as LLM¬†‚Ä¶,108.0,Methodology
SALAD-Bench,SALAD-Bench: A Hierarchical and Comprehensive Safety Benchmark for Large Language Models,Sg-bench: Evaluating llm safety generalization across diverse tasks and prompt types,"Y Mou,S Zhang,W Ye- Advances in Neural Information¬†‚Ä¶, 2024",proceedings.neurips.cc,,https://proceedings.neurips.cc/paper_files/paper/2024/hash/de7b99107c53e60257c727dc73daf1d1-Abstract-Datasets_and_Benchmarks_Track.html,,Ensuring the safety of large language model (LLM) applications is essential for developingtrustworthy artificial intelligence. Current LLM safety benchmarks have two limitations. First¬†‚Ä¶,34.0,Benchmark
SALAD-Bench,SALAD-Bench: A Hierarchical and Comprehensive Safety Benchmark for Large Language Models,Audiobench: A universal benchmark for audio large language models,"B Wang, X Zou,G Lin,S Sun,Z Liu,W Zhang‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2024",arxiv.org,,https://arxiv.org/abs/2406.16020,,"We introduce AudioBench, a universal benchmark designed to evaluate Audio LargeLanguage Models (AudioLLMs). It encompasses 8 distinct tasks and 26 datasets, among¬†‚Ä¶",107.0,Benchmark
SALAD-Bench,SALAD-Bench: A Hierarchical and Comprehensive Safety Benchmark for Large Language Models,Shieldgemma: Generative ai content moderation based on gemma,"W Zeng, Y Liu,R Mullins,L Peran, J Fernandez‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2024",arxiv.org,,https://arxiv.org/abs/2407.21772,,"We present ShieldGemma, a comprehensive suite of LLM-based safety content moderationmodels built upon Gemma2. These models provide robust, state-of-the-art predictions of¬†‚Ä¶",85.0,Methodology
MACHIAVELLI,Do the Rewards Justify the Means? Measuring Trade-Offs Between Rewards and Ethical Behavior in the MACHIAVELLI Benchmark,Ai alignment: A comprehensive survey,"J Ji,T Qiu,B Chen, B Zhang,H Lou, K Wang‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2023",arxiv.org,,https://arxiv.org/abs/2310.19852,,"AI alignment aims to make AI systems behave in line with human intentions and values. AsAI systems grow more capable, so do risks from misalignment. To provide a comprehensive¬†‚Ä¶",459.0,Survey
MACHIAVELLI,Do the Rewards Justify the Means? Measuring Trade-Offs Between Rewards and Ethical Behavior in the MACHIAVELLI Benchmark,"AI deception: A survey of examples, risks, and potential solutions","PS Park,S Goldstein,A O'Gara,M Chen,D Hendrycks- Patterns, 2024",cell.com,,https://www.cell.com/patterns/fulltext/S2666-3899(24)00103-X?ref=aiexec.whitegloveai.com,,This paper argues that a range of current AI systems have learned how to deceive humans.We define deception as the systematic inducement of false beliefs in the pursuit of some¬†‚Ä¶,341.0,Survey
MACHIAVELLI,Do the Rewards Justify the Means? Measuring Trade-Offs Between Rewards and Ethical Behavior in the MACHIAVELLI Benchmark,DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models.,"B Wang,W Chen,H Pei,C Xie,M Kang,C Zhang,C Xu‚Ä¶¬†- NeurIPS, 2023",blogs.qub.ac.uk,,https://blogs.qub.ac.uk/wp-content/uploads/sites/7/2024/01/A-comprehensive-Assessment-of-Trustworthiness-in-GPT-Models.pdf,,"Abstract Generative Pre-trained Transformer (GPT) models have exhibited exciting progressin their capabilities, capturing the interest of practitioners and the public alike. Yet, while the¬†‚Ä¶",625.0,Survey
MACHIAVELLI,Do the Rewards Justify the Means? Measuring Trade-Offs Between Rewards and Ethical Behavior in the MACHIAVELLI Benchmark,Representation engineering: A top-down approach to ai transparency,"A Zou,L Phan,S Chen,J Campbell,P Guo‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2023",arxiv.org,,https://arxiv.org/abs/2310.01405,,"In this paper, we identify and characterize the emerging area of representation engineering(RepE), an approach to enhancing the transparency of AI systems that draws on insights¬†‚Ä¶",567.0,Survey
MACHIAVELLI,Do the Rewards Justify the Means? Measuring Trade-Offs Between Rewards and Ethical Behavior in the MACHIAVELLI Benchmark,Managing extreme AI risks amid rapid progress,"Y Bengio,G Hinton,A Yao,D Song,P Abbeel,T Darrell‚Ä¶¬†- Science, 2024",science.org,,https://www.science.org/doi/abs/10.1126/science.adn0117,,"Artificial intelligence (AI) is progressing rapidly, and companies are shifting their focus todeveloping generalist AI systems that can autonomously act and pursue goals. Increases in¬†‚Ä¶",435.0,Other
MACHIAVELLI,Do the Rewards Justify the Means? Measuring Trade-Offs Between Rewards and Ethical Behavior in the MACHIAVELLI Benchmark,The wmdp benchmark: Measuring and reducing malicious use with unlearning,"N Li,A Pan,A Gopal, S Yue, D Berrios,A Gatti‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2024",arxiv.org,,https://arxiv.org/abs/2403.03218,,"The White House Executive Order on Artificial Intelligence highlights the risks of largelanguage models (LLMs) empowering malicious actors in developing biological, cyber, and¬†‚Ä¶",284.0,Benchmark
MACHIAVELLI,Do the Rewards Justify the Means? Measuring Trade-Offs Between Rewards and Ethical Behavior in the MACHIAVELLI Benchmark,Foundational challenges in assuring alignment and safety of large language models,"U Anwar,A Saparov,J Rando,D Paleka‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2024",arxiv.org,,https://arxiv.org/abs/2404.09932,,This work identifies 18 foundational challenges in assuring the alignment and safety of largelanguage models (LLMs). These challenges are organized into three different categories¬†‚Ä¶,254.0,Survey
MACHIAVELLI,Do the Rewards Justify the Means? Measuring Trade-Offs Between Rewards and Ethical Behavior in the MACHIAVELLI Benchmark,An overview of catastrophic AI risks,"D Hendrycks,M Mazeika,T Woodside- arXiv preprint arXiv:2306.12001, 2023",arxiv.org,,https://arxiv.org/abs/2306.12001,,"Rapid advancements in artificial intelligence (AI) have sparked growing concerns amongexperts, policymakers, and world leaders regarding the potential for increasingly advanced¬†‚Ä¶",316.0,Survey
MACHIAVELLI,Do the Rewards Justify the Means? Measuring Trade-Offs Between Rewards and Ethical Behavior in the MACHIAVELLI Benchmark,Review of large vision models and visual prompt engineering,"J Wang,Z Liu,L Zhao,Z Wu,C Ma, S Yu,H Dai‚Ä¶¬†- Meta-Radiology, 2023",Elsevier,,https://www.sciencedirect.com/science/article/pii/S2950162823000474,,"Visual prompt engineering is a fundamental methodology in the field of visual and imageartificial general intelligence. As the development of large vision models progresses, the¬†‚Ä¶",244.0,Survey
MACHIAVELLI,Do the Rewards Justify the Means? Measuring Trade-Offs Between Rewards and Ethical Behavior in the MACHIAVELLI Benchmark,Rewarded soups: towards pareto-optimal alignment by interpolating weights fine-tuned on diverse rewards,"A Rame,G Couairon,C Dancette‚Ä¶¬†- Advances in¬†‚Ä¶, 2023",proceedings.neurips.cc,,https://proceedings.neurips.cc/paper_files/paper/2023/hash/e12a3b98b67e8395f639fde4c2b03168-Abstract-Conference.html,,"Foundation models are first pre-trained on vast unsupervised datasets and then fine-tunedon labeled data. Reinforcement learning, notably from human feedback (RLHF), can further¬†‚Ä¶",208.0,Methodology
ELEPHANT,ELEPHANT: Measuring and Understanding Social Sycophancy in LLMs,Training language models to be warm and empathetic makes them less reliable and more sycophantic,"L Ibrahim,FS Hafner,L Rocher- arXiv preprint arXiv:2507.21919, 2025",arxiv.org,,https://arxiv.org/abs/2507.21919,,"Artificial intelligence (AI) developers are increasingly building language models with warmand empathetic personas that millions of people now use for advice, therapy, and¬†‚Ä¶",7.0,Other
ELEPHANT,ELEPHANT: Measuring and Understanding Social Sycophancy in LLMs,"Selective agreement, not sycophancy: investigating opinion dynamics in LLM interactions","E Cau,V Pansanella,D Pedreschi,G Rossetti- EPJ Data Science, 2025",Springer,,https://link.springer.com/content/pdf/10.1140/epjds/s13688-025-00579-1.pdf,,"Understanding how opinions evolve is essential for addressing phenomena such aspolarization, radicalization, and consensus formation. In this work, we investigate how¬†‚Ä¶",4.0,Other
ELEPHANT,ELEPHANT: Measuring and Understanding Social Sycophancy in LLMs,Sycophancy under pressure: Evaluating and mitigating sycophantic bias via adversarial dialogues in scientific qa,"K Zhang, Q Jia, Z Chen,W Sun,X Zhu,C Li‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2025",arxiv.org,,https://arxiv.org/abs/2508.13743,,"Large language models (LLMs), while increasingly used in domains requiring factual rigor,often display a troubling behavior: sycophancy, the tendency to align with user beliefs¬†‚Ä¶",3.0,Methodology
ELEPHANT,ELEPHANT: Measuring and Understanding Social Sycophancy in LLMs,The personality illusion: Revealing dissociation between self-reports & behavior in llms,"P Han,R Kocielnik,P Song,R Debnath‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2025",arxiv.org,,https://arxiv.org/abs/2509.03730,,Personality traits have long been studied as predictors of human behavior. Recent advancesin Large Language Models (LLMs) suggest similar patterns may emerge in artificial systems¬†‚Ä¶,2.0,Other
ELEPHANT,ELEPHANT: Measuring and Understanding Social Sycophancy in LLMs,The pimmur principles: Ensuring validity in collective behavior of llm societies,"J Zhou,J Huang,X Zhou,MH Lam,X Wang‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2025",arxiv.org,,https://arxiv.org/abs/2509.18052,,"Large Language Models (LLMs) are increasingly used for social simulation, wherepopulations of agents are expected to reproduce human-like collective behavior. However¬†‚Ä¶",1.0,Methodology
ELEPHANT,ELEPHANT: Measuring and Understanding Social Sycophancy in LLMs,Simulating and Understanding Deceptive Behaviors in Long-Horizon Interactions,"Y Xu,X Zhang,MH Yeh,J Dhamala,O Dia‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2025",arxiv.org,,https://arxiv.org/abs/2510.03999,,Deception is a pervasive feature of human communication and an emerging concern inlarge language models (LLMs). While recent studies document instances of LLM deception¬†‚Ä¶,1.0,Other
ELEPHANT,ELEPHANT: Measuring and Understanding Social Sycophancy in LLMs,Virtual agent economies,"N Tomasev,M Franklin,JZ Leibo,J Jacobs‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2025",arxiv.org,,https://arxiv.org/abs/2509.10147,,The rapid adoption of autonomous AI agents is giving rise to a new economic layer whereagents transact and coordinate at scales and speeds beyond direct human oversight. We¬†‚Ä¶,1.0,Other
ELEPHANT,ELEPHANT: Measuring and Understanding Social Sycophancy in LLMs,Can Large Language Models Simulate Spoken Human Conversations?,"E Mayor,LM Bietti,A Bangerter- Cognitive Science, 2025",Wiley Online Library,,https://onlinelibrary.wiley.com/doi/abs/10.1111/cogs.70106,,Large language models (LLMs) can emulate many aspects of human cognition and havebeen heralded as a potential paradigm shift. They are proficient in chat‚Äêbased conversation¬†‚Ä¶,1.0,Other
ELEPHANT,ELEPHANT: Measuring and Understanding Social Sycophancy in LLMs,"Evolving Health Information‚ÄìSeeking Behavior in the Context of Google AI Overviews, ChatGPT, and Alexa: Interview Study Using the Think-Aloud Protocol","C Wardle, S Urbani, E Wang¬†- Journal of Medical Internet Research, 2025",jmir.org,,https://www.jmir.org/2025/1/e79961/,,Background Online health information seeking is undergoing a major shift with the advent ofartificial intelligence (AI)‚Äìpowered technologies such as voice assistants and large¬†‚Ä¶,,Other
ELEPHANT,ELEPHANT: Measuring and Understanding Social Sycophancy in LLMs,When truth is overridden: Uncovering the internal origins of sycophancy in large language models,"K Wang, J Li, S Yang,Z Zhang,D Wang- arXiv preprint arXiv:2508.02087, 2025",arxiv.org,,https://arxiv.org/abs/2508.02087,,"Large Language Models (LLMs) often exhibit sycophantic behavior, agreeing with user-stated opinions even when those contradict factual knowledge. While prior work has¬†‚Ä¶",1.0,Methodology
OpenDeception,OpenDeception: Benchmarking and Investigating AI Deceptive Behaviors via Open-ended Interaction Simulation,"Lessons from a Chimp: AI"" Scheming"" and the Quest for Ape Language","C Summerfield,L Luettgau,M Dubois,HR Kirk‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2025",arxiv.org,,https://arxiv.org/abs/2507.03409,,"We examine recent research that asks whether current AI systems may be developing acapacity for"" scheming""(covertly and strategically pursuing misaligned goals). We compare¬†‚Ä¶",2.0,Other
OpenDeception,OpenDeception: Benchmarking and Investigating AI Deceptive Behaviors via Open-ended Interaction Simulation,Simulating and Understanding Deceptive Behaviors in Long-Horizon Interactions,"Y Xu,X Zhang,MH Yeh,J Dhamala,O Dia‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2025",arxiv.org,,https://arxiv.org/abs/2510.03999,,Deception is a pervasive feature of human communication and an emerging concern inlarge language models (LLMs). While recent studies document instances of LLM deception¬†‚Ä¶,1.0,Other
OpenDeception,OpenDeception: Benchmarking and Investigating AI Deceptive Behaviors via Open-ended Interaction Simulation,Mitigating deceptive alignment via self-monitoring,"J Ji, W Chen, K Wang,D Hong,S Fang,B Chen‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2025",arxiv.org,,https://arxiv.org/abs/2505.18807,,"Modern large language models rely on chain-of-thought (CoT) reasoning to achieveimpressive performance, yet the same mechanism can amplify deceptive alignment¬†‚Ä¶",7.0,Methodology
OpenDeception,OpenDeception: Benchmarking and Investigating AI Deceptive Behaviors via Open-ended Interaction Simulation,"The hidden complexities of android TPL detection: An empirical analysis of techniques, challenges, and effectiveness","L Zhan,J Ming, J Fu, G Peng, L Sha, L Lan¬†- Computers & Security, 2025",Elsevier,,https://www.sciencedirect.com/science/article/pii/S016740482500361X,,"Third-party libraries (TPLs) play a crucial role in Android application (app) development andhave become an indispensable part of the Android ecosystem. However, TPLs also¬†‚Ä¶",,Survey
OpenDeception,OpenDeception: Benchmarking and Investigating AI Deceptive Behaviors via Open-ended Interaction Simulation,Quantized but Deceptive? A Multi-Dimensional Truthfulness Evaluation of Quantized LLMs,"Y Fu,X Long, R Li, H Yu, M Sheng,X Han‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2025",arxiv.org,,https://arxiv.org/abs/2508.19432,,Quantization enables efficient deployment of large language models (LLMs) in resource-constrained environments by significantly reducing memory and computation costs. While¬†‚Ä¶,1.0,Methodology
OpenDeception,OpenDeception: Benchmarking and Investigating AI Deceptive Behaviors via Open-ended Interaction Simulation,DeceptionBench: A Comprehensive Benchmark for AI Deception Behaviors in Real-world Scenarios,"Y Huang,Y Sun,Y Zhang, R Zhang,Y Dong‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2025",arxiv.org,,https://arxiv.org/abs/2510.15501,,"Despite the remarkable advances of Large Language Models (LLMs) across diversecognitive tasks, the rapid enhancement of these capabilities also introduces emergent¬†‚Ä¶",,Benchmark
OpenDeception,OpenDeception: Benchmarking and Investigating AI Deceptive Behaviors via Open-ended Interaction Simulation,WGSR-Bench: Wargame-based Game-theoretic Strategic Reasoning Benchmark for Large Language Models,"Q Yin, P Xu,Q Li, S Liu, S Shen, T Wang, Y Han‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2025",arxiv.org,,https://arxiv.org/abs/2506.10264,,"Recent breakthroughs in Large Language Models (LLMs) have led to a qualitative leap inartificial intelligence's performance on reasoning tasks, particularly demonstrating¬†‚Ä¶",,Benchmark
Goal Misgeneralization (Procgen),Goal Misgeneralization in Deep Reinforcement Learning,Ai alignment: A comprehensive survey,"J Ji,T Qiu,B Chen, B Zhang,H Lou, K Wang‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2023",arxiv.org,,https://arxiv.org/abs/2310.19852,,"AI alignment aims to make AI systems behave in line with human intentions and values. AsAI systems grow more capable, so do risks from misalignment. To provide a comprehensive¬†‚Ä¶",459.0,Survey
Goal Misgeneralization (Procgen),Goal Misgeneralization in Deep Reinforcement Learning,"AI deception: A survey of examples, risks, and potential solutions","PS Park,S Goldstein,A O'Gara,M Chen,D Hendrycks- Patterns, 2024",cell.com,,https://www.cell.com/patterns/fulltext/S2666-3899(24)00103-X?ref=aiexec.whitegloveai.com,,This paper argues that a range of current AI systems have learned how to deceive humans.We define deception as the systematic inducement of false beliefs in the pursuit of some¬†‚Ä¶,341.0,Survey
Goal Misgeneralization (Procgen),Goal Misgeneralization in Deep Reinforcement Learning,Open problems and fundamental limitations of reinforcement learning from human feedback,"S Casper,X Davies,C Shi,TK Gilbert‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2023",arxiv.org,,https://arxiv.org/abs/2307.15217,,Reinforcement learning from human feedback (RLHF) is a technique for training AI systemsto align with human goals. RLHF has emerged as the central method used to finetune state¬†‚Ä¶,777.0,Survey
Goal Misgeneralization (Procgen),Goal Misgeneralization in Deep Reinforcement Learning,Managing extreme AI risks amid rapid progress,"Y Bengio,G Hinton,A Yao,D Song,P Abbeel,T Darrell‚Ä¶¬†- Science, 2024",science.org,,https://www.science.org/doi/abs/10.1126/science.adn0117,,"Artificial intelligence (AI) is progressing rapidly, and companies are shifting their focus todeveloping generalist AI systems that can autonomously act and pursue goals. Increases in¬†‚Ä¶",435.0,Other
Goal Misgeneralization (Procgen),Goal Misgeneralization in Deep Reinforcement Learning,Foundational challenges in assuring alignment and safety of large language models,"U Anwar,A Saparov,J Rando,D Paleka‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2024",arxiv.org,,https://arxiv.org/abs/2404.09932,,This work identifies 18 foundational challenges in assuring the alignment and safety of largelanguage models (LLMs). These challenges are organized into three different categories¬†‚Ä¶,254.0,Survey
Goal Misgeneralization (Procgen),Goal Misgeneralization in Deep Reinforcement Learning,Rewarded soups: towards pareto-optimal alignment by interpolating weights fine-tuned on diverse rewards,"A Rame,G Couairon,C Dancette‚Ä¶¬†- Advances in¬†‚Ä¶, 2023",proceedings.neurips.cc,,https://proceedings.neurips.cc/paper_files/paper/2023/hash/e12a3b98b67e8395f639fde4c2b03168-Abstract-Conference.html,,"Foundation models are first pre-trained on vast unsupervised datasets and then fine-tunedon labeled data. Reinforcement learning, notably from human feedback (RLHF), can further¬†‚Ä¶",208.0,Methodology
Goal Misgeneralization (Procgen),Goal Misgeneralization in Deep Reinforcement Learning,Towards reasoning era: A survey of long chain-of-thought for reasoning large language models,"Q Chen,L Qin, J Liu,D Peng, J Guan,P Wang‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2025",arxiv.org,,https://arxiv.org/abs/2503.09567,,"Recent advancements in reasoning with large language models (RLLMs), such as OpenAI-O1 and DeepSeek-R1, have demonstrated their impressive capabilities in complex domains¬†‚Ä¶",228.0,Survey
Goal Misgeneralization (Procgen),Goal Misgeneralization in Deep Reinforcement Learning,"Survey on large language model-enhanced reinforcement learning: Concept, taxonomy, and methods","Y Cao,H Zhao,Y Cheng,T Shu,Y Chen‚Ä¶¬†- ‚Ä¶¬†on Neural Networks¬†‚Ä¶, 2024",ieeexplore.ieee.org,,https://ieeexplore.ieee.org/abstract/document/10766898/,,"With extensive pretrained knowledge and high-level general capabilities, large languagemodels (LLMs) emerge as a promising avenue to augment reinforcement learning (RL) in¬†‚Ä¶",175.0,Survey
Goal Misgeneralization (Procgen),Goal Misgeneralization in Deep Reinforcement Learning,Eight things to know about large language models,"SR Bowman- Critical AI, 2024",read.dukeupress.edu,,https://read.dukeupress.edu/critical-ai/article-abstract/doi/10.1215/2834703X-11556011/400182,,"The widespread public deployment of large language models (LLMs) in recent months hasprompted a wave of new attention and engagement from advocates, policymakers, and¬†‚Ä¶",243.0,Survey
Goal Misgeneralization (Procgen),Goal Misgeneralization in Deep Reinforcement Learning,The alignment problem from a deep learning perspective,"R Ngo,L Chan,S Mindermann- arXiv preprint arXiv:2209.00626, 2022",arxiv.org,,https://arxiv.org/abs/2209.00626,,"In coming years or decades, artificial general intelligence (AGI) may surpass humancapabilities at many critical tasks. We argue that, without substantial effort to prevent it, AGIs¬†‚Ä¶",295.0,Other
Goal Misgeneralization (Analysis),Goal Misgeneralization: Why Correct Specifications Aren't Enough For Correct Goals,"AI deception: A survey of examples, risks, and potential solutions","PS Park,S Goldstein,A O'Gara,M Chen,D Hendrycks- Patterns, 2024",cell.com,,https://www.cell.com/patterns/fulltext/S2666-3899(24)00103-X?ref=aiexec.whitegloveai.com,,This paper argues that a range of current AI systems have learned how to deceive humans.We define deception as the systematic inducement of false beliefs in the pursuit of some¬†‚Ä¶,341.0,Survey
Goal Misgeneralization (Analysis),Goal Misgeneralization: Why Correct Specifications Aren't Enough For Correct Goals,Open problems and fundamental limitations of reinforcement learning from human feedback,"S Casper,X Davies,C Shi,TK Gilbert‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2023",arxiv.org,,https://arxiv.org/abs/2307.15217,,Reinforcement learning from human feedback (RLHF) is a technique for training AI systemsto align with human goals. RLHF has emerged as the central method used to finetune state¬†‚Ä¶,777.0,Survey
Goal Misgeneralization (Analysis),Goal Misgeneralization: Why Correct Specifications Aren't Enough For Correct Goals,Managing extreme AI risks amid rapid progress,"Y Bengio,G Hinton,A Yao,D Song,P Abbeel,T Darrell‚Ä¶¬†- Science, 2024",science.org,,https://www.science.org/doi/abs/10.1126/science.adn0117,,"Artificial intelligence (AI) is progressing rapidly, and companies are shifting their focus todeveloping generalist AI systems that can autonomously act and pursue goals. Increases in¬†‚Ä¶",435.0,Other
Goal Misgeneralization (Analysis),Goal Misgeneralization: Why Correct Specifications Aren't Enough For Correct Goals,Large language model alignment: A survey,"T Shen,R Jin,Y Huang,C Liu,W Dong, Z Guo‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2023",arxiv.org,,https://arxiv.org/abs/2309.15025,,"Recent years have witnessed remarkable progress made in large language models (LLMs).Such advancements, while garnering significant attention, have concurrently elicited various¬†‚Ä¶",289.0,Survey
Goal Misgeneralization (Analysis),Goal Misgeneralization: Why Correct Specifications Aren't Enough For Correct Goals,The alignment problem from a deep learning perspective,"R Ngo,L Chan,S Mindermann- arXiv preprint arXiv:2209.00626, 2022",arxiv.org,,https://arxiv.org/abs/2209.00626,,"In coming years or decades, artificial general intelligence (AGI) may surpass humancapabilities at many critical tasks. We argue that, without substantial effort to prevent it, AGIs¬†‚Ä¶",295.0,Other
Goal Misgeneralization (Analysis),Goal Misgeneralization: Why Correct Specifications Aren't Enough For Correct Goals,Safetywashing: Do AI Safety Benchmarks Actually Measure Safety Progress?,"R Ren,S Basart, A Khoja,A Gatti‚Ä¶¬†- Advances in¬†‚Ä¶, 2024",proceedings.neurips.cc,,https://proceedings.neurips.cc/paper_files/paper/2024/hash/7ebcdd0de471c027e67a11959c666d74-Abstract-Datasets_and_Benchmarks_Track.html,,"Performance on popular ML benchmarks is highly correlated with model scale, suggestingthat most benchmarks tend to measure a similar underlying factor of general model¬†‚Ä¶",48.0,Benchmark
Goal Misgeneralization (Analysis),Goal Misgeneralization: Why Correct Specifications Aren't Enough For Correct Goals,Managing ai risks in an era of rapid progress,"Y Bengio,G Hinton,A Yao,D Song‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2023",blog.biocomm.ai,,https://blog.biocomm.ai/wp-content/uploads/2023/11/Managing-AI-Risks-in-an-Era-of-Rapid-Progress.pdf,,"In this short consensus paper, we outline risks from upcoming, advanced AI systems. Weexamine large-scale social harms and malicious uses, as well as an irreversible loss of¬†‚Ä¶",121.0,Survey
Goal Misgeneralization (Analysis),Goal Misgeneralization: Why Correct Specifications Aren't Enough For Correct Goals,Harms from increasingly agentic algorithmic systems,"A Chan, R Salganik,A Markelius, C Pang‚Ä¶¬†- Proceedings of the¬†‚Ä¶, 2023",dl.acm.org,,https://dl.acm.org/doi/abs/10.1145/3593013.3594033,,"Research in Fairness, Accountability, Transparency, and Ethics (FATE) 1 has establishedmany sources and forms of algorithmic harm, in domains as diverse as health care, finance¬†‚Ä¶",181.0,Survey
Goal Misgeneralization (Analysis),Goal Misgeneralization: Why Correct Specifications Aren't Enough For Correct Goals,Open-endedness is essential for artificial superhuman intelligence,"E Hughes,M Dennis,J Parker-Holder‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2024",arxiv.org,,https://arxiv.org/abs/2406.04268,,"In recent years there has been a tremendous surge in the general capabilities of AI systems,mainly fuelled by training foundation models on internetscale data. Nevertheless, the¬†‚Ä¶",69.0,Other
Goal Misgeneralization (Analysis),Goal Misgeneralization: Why Correct Specifications Aren't Enough For Correct Goals,Characterizing manipulation from AI systems,"M Carroll,A Chan, H Ashton,D Krueger- ‚Ä¶¬†of the 3rd ACM Conference on¬†‚Ä¶, 2023",dl.acm.org,,https://dl.acm.org/doi/abs/10.1145/3617694.3623226,,"Manipulation is a concern in many domains, such as social media, advertising, andchatbots. As AI systems mediate more of our digital interactions, it is important to understand¬†‚Ä¶",124.0,Survey
IPS Index,Instrumental Power-Seeking Index: A Novel Benchmark for Detecting Power-Seeking Behavior in LLMs,Ai alignment: A comprehensive survey,"J Ji,T Qiu,B Chen, B Zhang,H Lou, K Wang‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2023",arxiv.org,,https://arxiv.org/abs/2310.19852,,"AI alignment aims to make AI systems behave in line with human intentions and values. AsAI systems grow more capable, so do risks from misalignment. To provide a comprehensive¬†‚Ä¶",459.0,Survey
IPS Index,Instrumental Power-Seeking Index: A Novel Benchmark for Detecting Power-Seeking Behavior in LLMs,"AI deception: A survey of examples, risks, and potential solutions","PS Park,S Goldstein,A O'Gara,M Chen,D Hendrycks- Patterns, 2024",cell.com,,https://www.cell.com/patterns/fulltext/S2666-3899(24)00103-X?ref=aiexec.whitegloveai.com,,This paper argues that a range of current AI systems have learned how to deceive humans.We define deception as the systematic inducement of false beliefs in the pursuit of some¬†‚Ä¶,341.0,Survey
IPS Index,Instrumental Power-Seeking Index: A Novel Benchmark for Detecting Power-Seeking Behavior in LLMs,DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models.,"B Wang,W Chen,H Pei,C Xie,M Kang,C Zhang,C Xu‚Ä¶¬†- NeurIPS, 2023",blogs.qub.ac.uk,,https://blogs.qub.ac.uk/wp-content/uploads/sites/7/2024/01/A-comprehensive-Assessment-of-Trustworthiness-in-GPT-Models.pdf,,"Abstract Generative Pre-trained Transformer (GPT) models have exhibited exciting progressin their capabilities, capturing the interest of practitioners and the public alike. Yet, while the¬†‚Ä¶",625.0,Survey
IPS Index,Instrumental Power-Seeking Index: A Novel Benchmark for Detecting Power-Seeking Behavior in LLMs,Representation engineering: A top-down approach to ai transparency,"A Zou,L Phan,S Chen,J Campbell,P Guo‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2023",arxiv.org,,https://arxiv.org/abs/2310.01405,,"In this paper, we identify and characterize the emerging area of representation engineering(RepE), an approach to enhancing the transparency of AI systems that draws on insights¬†‚Ä¶",567.0,Survey
IPS Index,Instrumental Power-Seeking Index: A Novel Benchmark for Detecting Power-Seeking Behavior in LLMs,Managing extreme AI risks amid rapid progress,"Y Bengio,G Hinton,A Yao,D Song,P Abbeel,T Darrell‚Ä¶¬†- Science, 2024",science.org,,https://www.science.org/doi/abs/10.1126/science.adn0117,,"Artificial intelligence (AI) is progressing rapidly, and companies are shifting their focus todeveloping generalist AI systems that can autonomously act and pursue goals. Increases in¬†‚Ä¶",435.0,Other
IPS Index,Instrumental Power-Seeking Index: A Novel Benchmark for Detecting Power-Seeking Behavior in LLMs,The wmdp benchmark: Measuring and reducing malicious use with unlearning,"N Li,A Pan,A Gopal, S Yue, D Berrios,A Gatti‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2024",arxiv.org,,https://arxiv.org/abs/2403.03218,,"The White House Executive Order on Artificial Intelligence highlights the risks of largelanguage models (LLMs) empowering malicious actors in developing biological, cyber, and¬†‚Ä¶",284.0,Benchmark
IPS Index,Instrumental Power-Seeking Index: A Novel Benchmark for Detecting Power-Seeking Behavior in LLMs,Foundational challenges in assuring alignment and safety of large language models,"U Anwar,A Saparov,J Rando,D Paleka‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2024",arxiv.org,,https://arxiv.org/abs/2404.09932,,This work identifies 18 foundational challenges in assuring the alignment and safety of largelanguage models (LLMs). These challenges are organized into three different categories¬†‚Ä¶,254.0,Survey
IPS Index,Instrumental Power-Seeking Index: A Novel Benchmark for Detecting Power-Seeking Behavior in LLMs,An overview of catastrophic AI risks,"D Hendrycks,M Mazeika,T Woodside- arXiv preprint arXiv:2306.12001, 2023",arxiv.org,,https://arxiv.org/abs/2306.12001,,"Rapid advancements in artificial intelligence (AI) have sparked growing concerns amongexperts, policymakers, and world leaders regarding the potential for increasingly advanced¬†‚Ä¶",316.0,Survey
IPS Index,Instrumental Power-Seeking Index: A Novel Benchmark for Detecting Power-Seeking Behavior in LLMs,Review of large vision models and visual prompt engineering,"J Wang,Z Liu,L Zhao,Z Wu,C Ma, S Yu,H Dai‚Ä¶¬†- Meta-Radiology, 2023",Elsevier,,https://www.sciencedirect.com/science/article/pii/S2950162823000474,,"Visual prompt engineering is a fundamental methodology in the field of visual and imageartificial general intelligence. As the development of large vision models progresses, the¬†‚Ä¶",244.0,Survey
IPS Index,Instrumental Power-Seeking Index: A Novel Benchmark for Detecting Power-Seeking Behavior in LLMs,Rewarded soups: towards pareto-optimal alignment by interpolating weights fine-tuned on diverse rewards,"A Rame,G Couairon,C Dancette‚Ä¶¬†- Advances in¬†‚Ä¶, 2023",proceedings.neurips.cc,,https://proceedings.neurips.cc/paper_files/paper/2023/hash/e12a3b98b67e8395f639fde4c2b03168-Abstract-Conference.html,,"Foundation models are first pre-trained on vast unsupervised datasets and then fine-tunedon labeled data. Reinforcement learning, notably from human feedback (RLHF), can further¬†‚Ä¶",208.0,Methodology
SycEval,SycEval: Evaluating LLM Sycophancy,Cognitive bias in clinical large language models,"A Mahajan,Z Obermeyer,R Daneshjou,J Lester‚Ä¶¬†- NPJ Digital¬†‚Ä¶, 2025",nature.com,,https://www.nature.com/articles/s41746-025-01790-0,,"Cognitive bias accounts for a significant portion of preventable errors in healthcare,contributing to significant patient morbidity and mortality each year. As large language¬†‚Ä¶",5.0,Other
SycEval,SycEval: Evaluating LLM Sycophancy,Large language models outperform humans in identifying neuromyths but show sycophantic behavior in applied contexts,"E Richter,MWH Spitzer, A Morgan,L Frede‚Ä¶¬†- Trends in Neuroscience¬†‚Ä¶, 2025",Elsevier,,https://www.sciencedirect.com/science/article/pii/S2211949325000092,,"Background: Neuromyths are widespread among educators, which raises concerns aboutmisconceptions regarding the (neural) principles underlying learning in the educator¬†‚Ä¶",6.0,Other
SycEval,SycEval: Evaluating LLM Sycophancy,Measuring sycophancy of language models in multi-turn dialogues,"J Hong,G Byun,S Kim,K Shu,JD Choi- arXiv preprint arXiv:2505.23840, 2025",arxiv.org,,https://arxiv.org/abs/2505.23840,,"Large Language Models (LLMs) are expected to provide helpful and harmless responses,yet they often exhibit sycophancy--conforming to user beliefs regardless of factual accuracy¬†‚Ä¶",7.0,Methodology
SycEval,SycEval: Evaluating LLM Sycophancy,"Selective agreement, not sycophancy: investigating opinion dynamics in LLM interactions","E Cau,V Pansanella,D Pedreschi,G Rossetti- EPJ Data Science, 2025",Springer,,https://link.springer.com/content/pdf/10.1140/epjds/s13688-025-00579-1.pdf,,"Understanding how opinions evolve is essential for addressing phenomena such aspolarization, radicalization, and consensus formation. In this work, we investigate how¬†‚Ä¶",4.0,Other
SycEval,SycEval: Evaluating LLM Sycophancy,Social sycophancy: A broader understanding of llm sycophancy,"M Cheng,S Yu,C Lee,P Khadpe,L Ibrahim‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2025",arxiv.org,,https://arxiv.org/abs/2505.13995,,"A serious risk to the safety and utility of LLMs is sycophancy, ie, excessive agreement withand flattery of the user. Yet existing work focuses on only one aspect of sycophancy¬†‚Ä¶",31.0,Other
SycEval,SycEval: Evaluating LLM Sycophancy,Sycophancy under pressure: Evaluating and mitigating sycophantic bias via adversarial dialogues in scientific qa,"K Zhang, Q Jia, Z Chen,W Sun,X Zhu,C Li‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2025",arxiv.org,,https://arxiv.org/abs/2508.13743,,"Large language models (LLMs), while increasingly used in domains requiring factual rigor,often display a troubling behavior: sycophancy, the tendency to align with user beliefs¬†‚Ä¶",3.0,Methodology
SycEval,SycEval: Evaluating LLM Sycophancy,EchoBench: Benchmarking Sycophancy in Medical Large Vision-Language Models,"B Yuan, Y Zhou, Y Wang,F Huo, Y Jing, L Shen‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2025",arxiv.org,,https://arxiv.org/abs/2509.20146,,"Recent benchmarks for medical Large Vision-Language Models (LVLMs) emphasizeleaderboard accuracy, overlooking reliability and safety. We study sycophancy--models'¬†‚Ä¶",1.0,Benchmark
SycEval,SycEval: Evaluating LLM Sycophancy,Exploring the impact of personality traits on llm bias and toxicity,"S Wang,R Li,X Chen,Y Yuan,DF Wong‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2025",arxiv.org,,https://arxiv.org/abs/2502.12566,,"With the different roles that AI is expected to play in human life, imbuing large languagemodels (LLMs) with different personalities has attracted increasing research interests. While¬†‚Ä¶",10.0,Other
SycEval,SycEval: Evaluating LLM Sycophancy,"Measuring AI"" Slop"" in Text","C Shaib,T Chakrabarty,D Garcia-Olano‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2025",arxiv.org,,https://arxiv.org/abs/2509.19163,,"AI"" slop"" is an increasingly popular term used to describe low-quality AI-generated text, butthere is currently no agreed upon definition of this term nor a means to measure its¬†‚Ä¶",1.0,Other
SycEval,SycEval: Evaluating LLM Sycophancy,Explainability of Large Language Models: Opportunities and Challenges toward Generating Trustworthy Explanations,"S Atakishiyev,HKB Babiker, J Dai,N Farruque‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2025",arxiv.org,,https://arxiv.org/abs/2510.17256,,"Large language models have exhibited impressive performance across a broad range ofdownstream tasks in natural language processing. However, how a language model¬†‚Ä¶",,Survey
HELM Safety,HELM Safety: Towards Standardized Safety Evaluations of Language Models,Safetyprompts: a systematic review of open datasets for evaluating and improving large language model safety,"P R√∂ttger,F Pernisi,B Vidgen,D Hovy- Proceedings of the AAAI¬†‚Ä¶, 2025",ojs.aaai.org,,https://ojs.aaai.org/index.php/AAAI/article/view/34975,,The last two years have seen a rapid growth in concerns around the safety of largelanguage models (LLMs). Researchers and practitioners have met these concerns by¬†‚Ä¶,52.0,Survey
HELM Safety,HELM Safety: Towards Standardized Safety Evaluations of Language Models,Evaluating the Sensitivity of LLMs to Harmful Contents in Long Input,"F Ghorbanpour,A Fraser- arXiv preprint arXiv:2510.05864, 2025",arxiv.org,,https://arxiv.org/abs/2510.05864,,"Large language models (LLMs) increasingly support applications that rely on extendedcontext, from document processing to retrieval-augmented generation. While their long¬†‚Ä¶",,Methodology
HELM Safety,HELM Safety: Towards Standardized Safety Evaluations of Language Models,Are Large Language Models Actually Getting Safer?,"A Ferreira- Indirizzo: https://www. cigionline. org/static/documents¬†‚Ä¶, 2025",cigionline.org,,https://www.cigionline.org/static/documents/DPH-paper-Ashley_Ferreira.pdf,,"It is widely recognized that LLMs have risen to remarkable prominence in recent years,fundamentally transforming many aspects of our world. Their rise can be traced to the 2017¬†‚Ä¶",1.0,Other
HELM Safety,HELM Safety: Towards Standardized Safety Evaluations of Language Models,AIBENCH: TOWARDS TRUSTWORTHY EVALUA-TION UNDER THE 45 LAW,"Z Zhang,J Wang, Y Guo,F Wen,Z Chen,H Wang, W Li‚Ä¶",researchgate.net,,https://www.researchgate.net/profile/Zicheng-Zhang-9/publication/393362210_AIBENCH_TOWARDS_TRUSTWORTHY_EVALUA-_TION_UNDER_THE_45LAW/links/6867747be4632b045dc9b47c/AIBENCH-TOWARDS-TRUSTWORTHY-EVALUA-TION-UNDER-THE-45LAW.pdf,,"We present AIBench, a flexible and rapidly updating platform that aggregates evaluationresults from commercial platforms, popular open-source leaderboards, and internal¬†‚Ä¶",6.0,Benchmark
AIR-Bench 2024,AIR-Bench 2024: A Safety Benchmark Based on Risk Categories from Regulations and Policies,The hidden risks of large reasoning models: A safety assessment of r1,"K Zhou,C Liu,X Zhao,S Jangam,J Srinivasa‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2025",arxiv.org,,https://arxiv.org/abs/2502.12659,,"The rapid development of large reasoning models, such as OpenAI-o3 and DeepSeek-R1,has led to significant improvements in complex reasoning over non-reasoning large¬†‚Ä¶",36.0,Other
AIR-Bench 2024,AIR-Bench 2024: A Safety Benchmark Based on Risk Categories from Regulations and Policies,Acceptable use policies for foundation models,"K Klyman- Proceedings of the AAAI/ACM Conference on AI, Ethics¬†‚Ä¶, 2024",ojs.aaai.org,,https://ojs.aaai.org/index.php/AIES/article/view/31677,,"As foundation models have accumulated hundreds of millions of users, developers havebegun to take steps to prevent harmful types of uses. One salient intervention that foundation¬†‚Ä¶",14.0,Other
AIR-Bench 2024,AIR-Bench 2024: A Safety Benchmark Based on Risk Categories from Regulations and Policies,Refusal-trained llms are easily jailbroken as browser agents,"P Kumar,E Lau,S Vijayakumar,T Trinh‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2024",arxiv.org,,https://arxiv.org/abs/2410.13886,,"For safety reasons, large language models (LLMs) are trained to refuse harmful userinstructions, such as assisting dangerous activities. We study an open question in this work¬†‚Ä¶",17.0,Methodology
AIR-Bench 2024,AIR-Bench 2024: A Safety Benchmark Based on Risk Categories from Regulations and Policies,Agrail: A lifelong agent guardrail with effective and adaptive safety detection,"W Luo,S Dai,X Liu,S Banerjee,H Sun,M Chen‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2025",arxiv.org,,https://arxiv.org/abs/2502.11448,,The rapid advancements in Large Language Models (LLMs) have enabled their deploymentas autonomous agents for handling complex tasks in dynamic environments. These LLMs¬†‚Ä¶,18.0,Methodology
AIR-Bench 2024,AIR-Bench 2024: A Safety Benchmark Based on Risk Categories from Regulations and Policies,Shieldagent: Shielding agents via verifiable safety policy reasoning,"Z Chen,M Kang,B Li- arXiv preprint arXiv:2503.22738, 2025",arxiv.org,,https://arxiv.org/abs/2503.22738,,"Autonomous agents powered by foundation models have seen widespread adoption acrossvarious real-world applications. However, they remain highly vulnerable to malicious¬†‚Ä¶",21.0,Methodology
AIR-Bench 2024,AIR-Bench 2024: A Safety Benchmark Based on Risk Categories from Regulations and Policies,Aligned LLMs are not aligned browser agents,"P Kumar,E Lau,S Vijayakumar,T Trinh‚Ä¶¬†- The Thirteenth¬†‚Ä¶, 2025",openreview.net,,https://openreview.net/forum?id=NsFZZU9gvk,,"For safety reasons, large language models (LLMs) are trained to refuse harmful userinstructions, such as assisting dangerous activities. We study an open question in this work¬†‚Ä¶",8.0,Other
AIR-Bench 2024,AIR-Bench 2024: A Safety Benchmark Based on Risk Categories from Regulations and Policies,Differentially private kernel density estimation,"E Liu,JYC Hu, A Reneau,Z Song,H Liu- arXiv preprint arXiv:2409.01688, 2024",arxiv.org,,https://arxiv.org/abs/2409.01688,,"We introduce a refined differentially private (DP) data structure for kernel density estimation(KDE), offering not only improved privacy-utility tradeoff but also better efficiency over prior¬†‚Ä¶",5.0,Methodology
AIR-Bench 2024,AIR-Bench 2024: A Safety Benchmark Based on Risk Categories from Regulations and Policies,Eaira: Establishing a methodology for evaluating ai models as scientific research assistants,"F Cappello,S Madireddy,R Underwood‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2025",arxiv.org,,https://arxiv.org/abs/2502.20309,,"Recent advancements have positioned AI, and particularly Large Language Models (LLMs),as transformative tools for scientific research, capable of addressing complex tasks that¬†‚Ä¶",7.0,Methodology
AIR-Bench 2024,AIR-Bench 2024: A Safety Benchmark Based on Risk Categories from Regulations and Policies,Breaking Down Bias: On The Limits of Generalizable Pruning Strategies,"S Ma,A Salinas,J Nyarko,P Henderson- Proceedings of the 2025 ACM¬†‚Ä¶, 2025",dl.acm.org,,https://dl.acm.org/doi/abs/10.1145/3715275.3732161,,"We employ model pruning to examine how LLMs conceptualize racial biases, and whether ageneralizable mitigation strategy for such biases appears feasible. Our analysis yields¬†‚Ä¶",2.0,Methodology
AIR-Bench 2024,AIR-Bench 2024: A Safety Benchmark Based on Risk Categories from Regulations and Policies,A Comprehensive Survey on Trustworthiness in Reasoning with Large Language Models,"Y Wang,Y Yu,J Liang, R He¬†- arXiv preprint arXiv:2509.03871, 2025",arxiv.org,,https://arxiv.org/abs/2509.03871,,"The development of Long-CoT reasoning has advanced LLM performance across varioustasks, including language understanding, complex problem solving, and code generation¬†‚Ä¶",,Survey
TrustLLM,TrustLLM: Trustworthiness in Large Language Models,Security and privacy challenges of large language models: A survey,"BC Das,MH Amini,Y Wu- ACM Computing Surveys, 2025",dl.acm.org,,https://dl.acm.org/doi/abs/10.1145/3712001,,"Large language models (LLMs) have demonstrated extraordinary capabilities andcontributed to multiple fields, such as generating and summarizing text, language¬†‚Ä¶",388.0,Survey
TrustLLM,TrustLLM: Trustworthiness in Large Language Models,Empowering biomedical discovery with AI agents,"S Gao,A Fang, Y Huang,V Giunchiglia,A Noori‚Ä¶¬†- Cell, 2024",cell.com,,https://www.cell.com/cell/fulltext/S0092-8674(24)01070-5?&target=_blank,,"We envision"" AI scientists"" as systems capable of skeptical learning and reasoning thatempower biomedical research through collaborative agents that integrate AI models and¬†‚Ä¶",232.0,Methodology
TrustLLM,TrustLLM: Trustworthiness in Large Language Models,Large language models: A survey,"S Minaee,T Mikolov,N Nikzad,M Chenaghlu‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2024",arxiv.org,,https://arxiv.org/abs/2402.06196,,"Large Language Models (LLMs) have drawn a lot of attention due to their strongperformance on a wide range of natural language tasks, since the release of ChatGPT in¬†‚Ä¶",1504.0,Survey
TrustLLM,TrustLLM: Trustworthiness in Large Language Models,Jailbreakbench: An open robustness benchmark for jailbreaking large language models,"P Chao,E Debenedetti,A Robey‚Ä¶¬†- Advances in¬†‚Ä¶, 2024",proceedings.neurips.cc,,https://proceedings.neurips.cc/paper_files/paper/2024/hash/63092d79154adebd7305dfd498cbff70-Abstract-Datasets_and_Benchmarks_Track.html,,"Jailbreak attacks cause large language models (LLMs) to generate harmful, unethical, orotherwise objectionable content. Evaluating these attacks presents a number of challenges¬†‚Ä¶",350.0,Benchmark
TrustLLM,TrustLLM: Trustworthiness in Large Language Models,"Large language models: a comprehensive survey of its applications, challenges, limitations, and future prospects","MU Hadi,R Qureshi,A Shah,M Irfan,A Zafar‚Ä¶¬†- Authorea¬†‚Ä¶, 2023",researchgate.net,,https://www.researchgate.net/profile/Muhammad-Shaikh-9/publication/383818024_Large_Language_Models_A_Comprehensive_Survey_of_its_Applications_Challenges_Limitations_and_Future_Prospects/links/66dffb06b1606e24c21d8936/Large-Language-Models-A-Comprehensive-Survey-of-its-Applications-Challenges-Limitations-and-Future-Prospects.pdf,,"Within the vast expanse of computerized language processing, a revolutionary entity knownas Large Language Models (LLMs) has emerged, wielding immense power in its capacity to¬†‚Ä¶",601.0,Survey
TrustLLM,TrustLLM: Trustworthiness in Large Language Models,Combating misinformation in the age of llms: Opportunities and challenges,"C Chen,K Shu- AI Magazine, 2024",Wiley Online Library,,https://onlinelibrary.wiley.com/doi/abs/10.1002/aaai.12188,,Misinformation such as fake news and rumors is a serious threat for information ecosystemsand public trust. The emergence of large language models (LLMs) has great potential to¬†‚Ä¶,279.0,Other
TrustLLM,TrustLLM: Trustworthiness in Large Language Models,"A comprehensive survey of small language models in the era of large language models: Techniques, enhancements, applications, collaboration with llms, and¬†‚Ä¶","F Wang,Z Zhang,X Zhang,Z Wu, T Mo,Q Lu‚Ä¶¬†- ACM Transactions on¬†‚Ä¶, 2024",dl.acm.org,,https://dl.acm.org/doi/abs/10.1145/3768165,,"Large language models (LLMs) have demonstrated emergent abilities in text generation,question answering, and reasoning, facilitating various tasks and domains. Despite their¬†‚Ä¶",111.0,Survey
TrustLLM,TrustLLM: Trustworthiness in Large Language Models,Factuality challenges in the era of large language models and opportunities for fact-checking,"I Augenstein,T Baldwin,M Cha‚Ä¶¬†- Nature Machine¬†‚Ä¶, 2024",nature.com,,https://www.nature.com/articles/s42256-024-00881-z,,"The emergence of tools based on large language models (LLMs), such as OpenAI'sChatGPT and Google's Gemini, has garnered immense public attention owing to their¬†‚Ä¶",225.0,Survey
TrustLLM,TrustLLM: Trustworthiness in Large Language Models,Mllm-as-a-judge: Assessing multimodal llm-as-a-judge with vision-language benchmark,"D Chen,R Chen, S Zhang, Y Wang,Y Liu‚Ä¶¬†- ‚Ä¶¬†on Machine Learning, 2024",openreview.net,,https://openreview.net/forum?id=dbFEFHAD79,,"Multimodal Large Language Models (MLLMs) have gained significant attention recently,showing remarkable potential in artificial general intelligence. However, assessing the utility¬†‚Ä¶",210.0,Benchmark
TrustLLM,TrustLLM: Trustworthiness in Large Language Models,Can llm-generated misinformation be detected?,"C Chen,K Shu- arXiv preprint arXiv:2309.13788, 2023",arxiv.org,,https://arxiv.org/abs/2309.13788,,"The advent of Large Language Models (LLMs) has made a transformative impact. However,the potential that LLMs such as ChatGPT can be exploited to generate misinformation has¬†‚Ä¶",310.0,Other
MLCommons AILuminate,AILuminate: Introducing v1.0 of the AI Risk and Reliability Benchmark from MLCommons,"Who is responsible? the data, models, users or regulations? a comprehensive survey on responsible generative ai for a sustainable future","S Raza,R Qureshi, A Zahid, S Kamawal‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2025",arxiv.org,,https://arxiv.org/abs/2502.08650,,"Generative AI is moving rapidly from research into real world deployment across sectors,which elevates the need for responsible development, deployment, evaluation, and¬†‚Ä¶",13.0,Survey
MLCommons AILuminate,AILuminate: Introducing v1.0 of the AI Risk and Reliability Benchmark from MLCommons,Automated red teaming with goat: the generative offensive agent tester,"M Pavlova,E Brinkman,K Iyer,V Albiero‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2024",arxiv.org,,https://arxiv.org/abs/2410.01606,,"Red teaming assesses how large language models (LLMs) can produce content thatviolates norms, policies, and rules set during their safety training. However, most existing¬†‚Ä¶",21.0,Methodology
MLCommons AILuminate,AILuminate: Introducing v1.0 of the AI Risk and Reliability Benchmark from MLCommons,Are all prompt components value-neutral? understanding the heterogeneous adversarial robustness of dissected prompt in large language models,"Y Zheng,T Li, H Huang, T Zeng, J Lu,C Chu‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2025",arxiv.org,,https://arxiv.org/abs/2508.01554,,"Prompt-based adversarial attacks have become an effective means to assess therobustness of large language models (LLMs). However, existing approaches often treat¬†‚Ä¶",3.0,Methodology
MLCommons AILuminate,AILuminate: Introducing v1.0 of the AI Risk and Reliability Benchmark from MLCommons,D-rex: A benchmark for detecting deceptive reasoning in large language models,"S Krishna,A Zou,R Gupta,EK Jones, N Winter‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2025",arxiv.org,,https://arxiv.org/abs/2509.17938,,The safety and alignment of Large Language Models (LLMs) are critical for their responsibledeployment. Current evaluation methods predominantly focus on identifying and preventing¬†‚Ä¶,1.0,Benchmark
MLCommons AILuminate,AILuminate: Introducing v1.0 of the AI Risk and Reliability Benchmark from MLCommons,Contemplative Superalignment,"RE Laukkonen, F Inglis, S Chandaria‚Ä¶¬†- ‚Ä¶¬†Conference on Artificial¬†‚Ä¶, 2025",Springer,,https://link.springer.com/chapter/10.1007/978-3-032-00686-8_31,,"As artificial intelligence (AI) improves, current alignment strategies may falter in the face ofunpredictable self-improvement and the sheer complexity of AI. Rather than trying to control¬†‚Ä¶",1.0,Other
MLCommons AILuminate,AILuminate: Introducing v1.0 of the AI Risk and Reliability Benchmark from MLCommons,Is Perceptual Encryption Secure? A Security Benchmark for Perceptual Encryption Methods,"U Kashyap,SK Padhi,SS Ali- IEEE Transactions on Artificial¬†‚Ä¶, 2025",ieeexplore.ieee.org,,https://ieeexplore.ieee.org/abstract/document/10974570/,,"Perceptual encryption methods are the key enablers for protecting image privacy for deeplearning-based applications in the cloud. In perceptual encryption, the image content is¬†‚Ä¶",,Benchmark
MLCommons AILuminate,AILuminate: Introducing v1.0 of the AI Risk and Reliability Benchmark from MLCommons,Phare: A Safety Probe for Large Language Models,"PL Jeune, B Mal√©zieux,W Xiao,M Dora- arXiv preprint arXiv:2505.11365, 2025",arxiv.org,,https://arxiv.org/abs/2505.11365,,"Ensuring the safety of large language models (LLMs) is critical for responsible deployment,yet existing evaluations often prioritize performance over identifying failure modes. We¬†‚Ä¶",,Methodology
MLCommons AILuminate,AILuminate: Introducing v1.0 of the AI Risk and Reliability Benchmark from MLCommons,Decoding Federated Learning: The FedNAM+ Conformal Revolution,"SB Balija,A Nanda,D Sahoo- arXiv preprint arXiv:2506.17872, 2025",arxiv.org,,https://arxiv.org/abs/2506.17872,,"Federated learning has significantly advanced distributed training of machine learningmodels across decentralized data sources. However, existing frameworks often lack¬†‚Ä¶",1.0,Methodology
MLCommons AILuminate,AILuminate: Introducing v1.0 of the AI Risk and Reliability Benchmark from MLCommons,Beyond Uniform Criteria: Scenario-Adaptive Multi-Dimensional Jailbreak Evaluation,"L Jiang, Y Li, X Zhang,Y Ding, L Pan¬†- arXiv preprint arXiv:2508.06194, 2025",arxiv.org,,https://arxiv.org/abs/2508.06194,,"Precise jailbreak evaluation is vital for LLM red teaming and jailbreak research. Currentapproaches employ binary classification (eg, string matching, toxic text classifiers, LLM¬†‚Ä¶",,Methodology
MLCommons AILuminate,AILuminate: Introducing v1.0 of the AI Risk and Reliability Benchmark from MLCommons,The Trust Fabric: Decentralized Interoperability and Economic Coordination for the Agentic Web,"SB Balija, R Singal,R Raskar,E Darzi, R Bala‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2025",arxiv.org,,https://arxiv.org/abs/2507.07901,,"The fragmentation of AI agent ecosystems has created urgent demands for interoperability,trust, and economic coordination that current protocols--including MCP (Hou et al., 2025)¬†‚Ä¶",,Other
ALERT,ALERT: A Comprehensive Benchmark for Assessing Large Language Models' Safety through Red Teaming,Safetyprompts: a systematic review of open datasets for evaluating and improving large language model safety,"P R√∂ttger,F Pernisi,B Vidgen,D Hovy- Proceedings of the AAAI¬†‚Ä¶, 2025",ojs.aaai.org,,https://ojs.aaai.org/index.php/AAAI/article/view/34975,,The last two years have seen a rapid growth in concerns around the safety of largelanguage models (LLMs). Researchers and practitioners have met these concerns by¬†‚Ä¶,52.0,Survey
ALERT,ALERT: A Comprehensive Benchmark for Assessing Large Language Models' Safety through Red Teaming,"Wildguard: Open one-stop moderation tools for safety risks, jailbreaks, and refusals of llms","S Han,K Rao,A Ettinger,L Jiang‚Ä¶¬†- Advances in¬†‚Ä¶, 2024",proceedings.neurips.cc,,https://proceedings.neurips.cc/paper_files/paper/2024/hash/0f69b4b96a46f284b726fbd70f74fb3b-Abstract-Datasets_and_Benchmarks_Track.html,,"We introduce WildGuard---an open, light-weight moderation tool for LLM safety thatachieves three goals:(1) identifying malicious intent in user prompts,(2) detecting safety risks¬†‚Ä¶",169.0,Methodology
ALERT,ALERT: A Comprehensive Benchmark for Assessing Large Language Models' Safety through Red Teaming,Sorry-bench: Systematically evaluating large language model safety refusal,"T Xie,X Qi,Y Zeng,Y Huang,UM Sehwag‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2024",arxiv.org,,https://arxiv.org/abs/2406.14598,,"Evaluating aligned large language models'(LLMs) ability to recognize and reject unsafe userrequests is crucial for safe, policy-compliant deployments. Existing evaluation efforts¬†‚Ä¶",141.0,Benchmark
ALERT,ALERT: A Comprehensive Benchmark for Assessing Large Language Models' Safety through Red Teaming,Or-bench: An over-refusal benchmark for large language models,"J Cui,WL Chiang,I Stoica,CJ Hsieh- arXiv preprint arXiv:2405.20947, 2024",arxiv.org,,https://arxiv.org/abs/2405.20947,,"Large Language Models (LLMs) require careful safety alignment to prevent maliciousoutputs. While significant research focuses on mitigating harmful content generation, the¬†‚Ä¶",104.0,Benchmark
ALERT,ALERT: A Comprehensive Benchmark for Assessing Large Language Models' Safety through Red Teaming,"Who is responsible? the data, models, users or regulations? a comprehensive survey on responsible generative ai for a sustainable future","S Raza,R Qureshi, A Zahid, S Kamawal‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2025",arxiv.org,,https://arxiv.org/abs/2502.08650,,"Generative AI is moving rapidly from research into real world deployment across sectors,which elevates the need for responsible development, deployment, evaluation, and¬†‚Ä¶",13.0,Survey
ALERT,ALERT: A Comprehensive Benchmark for Assessing Large Language Models' Safety through Red Teaming,"On the trustworthiness of generative foundation models: Guideline, assessment, and perspective","Y Huang,C Gao, S Wu,H Wang,X Wang‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2025",arxiv.org,,https://arxiv.org/abs/2502.14296,,"Generative Foundation Models (GenFMs) have emerged as transformative tools. However,their widespread adoption raises critical concerns regarding trustworthiness across¬†‚Ä¶",32.0,Survey
ALERT,ALERT: A Comprehensive Benchmark for Assessing Large Language Models' Safety through Red Teaming,ART: automatic red-teaming for text-to-image models to protect benign users,"G Li,K Chen,S Zhang,J Zhang‚Ä¶¬†- Advances in Neural¬†‚Ä¶, 2024",proceedings.neurips.cc,,https://proceedings.neurips.cc/paper_files/paper/2024/hash/a5c7206fd66e8314bb21a04492359353-Abstract-Conference.html,,"Large-scale pre-trained generative models are taking the world by storm, due to theirabilities in generating creative content. Meanwhile, safeguards for these generative models¬†‚Ä¶",27.0,Methodology
ALERT,ALERT: A Comprehensive Benchmark for Assessing Large Language Models' Safety through Red Teaming,Benchmarking adversarial robustness to bias elicitation in large language models: Scalable automated assessment with llm-as-a-judge,"R Cantini,A Orsino, M Ruggiero,D Talia- Machine Learning, 2025",Springer,,https://link.springer.com/article/10.1007/s10994-025-06862-6,,The growing integration of Large Language Models (LLMs) into critical societal domains hasraised concerns about embedded biases that can perpetuate stereotypes and undermine¬†‚Ä¶,13.0,Benchmark
ALERT,ALERT: A Comprehensive Benchmark for Assessing Large Language Models' Safety through Red Teaming,Bi-factorial preference optimization: Balancing safety-helpfulness in language models,"W Zhang,PHS Torr,M Elhoseiny,A Bibi- arXiv preprint arXiv:2408.15313, 2024",arxiv.org,,https://arxiv.org/abs/2408.15313,,"Fine-tuning large language models (LLMs) on human preferences, typically throughreinforcement learning from human feedback (RLHF), has proven successful in enhancing¬†‚Ä¶",15.0,Methodology
ALERT,ALERT: A Comprehensive Benchmark for Assessing Large Language Models' Safety through Red Teaming,Blockchain for large language model security and safety: A holistic survey,"C Geren,A Board,GG Dagher,T Andersen‚Ä¶¬†- ACM SIGKDD¬†‚Ä¶, 2025",dl.acm.org,,https://dl.acm.org/doi/abs/10.1145/3715073.3715075,,"With the growing development and deployment of large language models (LLMs) in bothindustrial and academic fields, their security and safety concerns have become increasingly¬†‚Ä¶",21.0,Survey
AgentHarm,AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents,"A comprehensive survey in llm (-agent) full stack safety: Data, training and deployment","K Wang,G Zhang,Z Zhou,J Wu,M Yu,S Zhao‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2025",arxiv.org,,https://arxiv.org/abs/2504.15585,,The remarkable success of Large Language Models (LLMs) has illuminated a promisingpathway toward achieving Artificial General Intelligence for both academic and industrial¬†‚Ä¶,61.0,Survey
AgentHarm,AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents,A survey on llm-as-a-judge,"J Gu,X Jiang,Z Shi,H Tan,X Zhai,C Xu,W Li‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2024",arxiv.org,,https://arxiv.org/abs/2411.15594,,"Accurate and consistent evaluation is crucial for decision-making across numerous fields,yet it remains a challenging task due to inherent subjectivity, variability, and scale. Large¬†‚Ä¶",714.0,Survey
AgentHarm,AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents,Jailbreaking leading safety-aligned llms with simple adaptive attacks,"M Andriushchenko,F Croce,N Flammarion- arXiv preprint arXiv¬†‚Ä¶, 2024",arxiv.org,,https://arxiv.org/abs/2404.02151,,"We show that even the most recent safety-aligned LLMs are not robust to simple adaptivejailbreaking attacks. First, we demonstrate how to successfully leverage access to logprobs¬†‚Ä¶",284.0,Methodology
AgentHarm,AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents,"Large language model agent: A survey on methodology, applications and challenges","J Luo,W Zhang,Y Yuan,Y Zhao,J Yang,Y Gu‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2025",arxiv.org,,https://arxiv.org/abs/2503.21460,,"The era of intelligent agents is upon us, driven by revolutionary advancements in largelanguage models. Large Language Model (LLM) agents, with goal-driven behaviors and¬†‚Ä¶",43.0,Survey
AgentHarm,AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents,A survey on trustworthy llm agents: Threats and countermeasures,"M Yu, F Meng,X Zhou,S Wang,J Mao, L Pan‚Ä¶¬†- Proceedings of the 31st¬†‚Ä¶, 2025",dl.acm.org,,https://dl.acm.org/doi/abs/10.1145/3711896.3736561,,"With the rapid evolution of Large Language Models (LLMs), LLM-based agents and Multi-agent Systems (MAS) have significantly expanded the capabilities of LLM ecosystems. This¬†‚Ä¶",43.0,Survey
AgentHarm,AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents,A survey of self-evolving agents: On path to artificial super intelligence,"H Gao,J Geng,W Hua,M Hu,X Juan,H Liu‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2025",arxiv.org,,https://arxiv.org/abs/2507.21046,,"Large Language Models (LLMs) have demonstrated strong capabilities but remainfundamentally static, unable to adapt their internal parameters to novel tasks, evolving¬†‚Ä¶",33.0,Survey
AgentHarm,AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents,The hidden risks of large reasoning models: A safety assessment of r1,"K Zhou,C Liu,X Zhao,S Jangam,J Srinivasa‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2025",arxiv.org,,https://arxiv.org/abs/2502.12659,,"The rapid development of large reasoning models, such as OpenAI-o3 and DeepSeek-R1,has led to significant improvements in complex reasoning over non-reasoning large¬†‚Ä¶",36.0,Other
AgentHarm,AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents,"On the trustworthiness of generative foundation models: Guideline, assessment, and perspective","Y Huang,C Gao, S Wu,H Wang,X Wang‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2025",arxiv.org,,https://arxiv.org/abs/2502.14296,,"Generative Foundation Models (GenFMs) have emerged as transformative tools. However,their widespread adoption raises critical concerns regarding trustworthiness across¬†‚Ä¶",32.0,Survey
AgentHarm,AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents,A comprehensive survey of self-evolving ai agents: A new paradigm bridging foundation models and lifelong agentic systems,"J Fang,Y Peng,X Zhang,Y Wang, X Yi‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2025",arxiv.org,,https://arxiv.org/abs/2508.07407,,"Recent advances in large language models have sparked growing interest in AI agentscapable of solving complex, real-world tasks. However, most existing agent systems rely on¬†‚Ä¶",18.0,Survey
AgentHarm,AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents,From llm reasoning to autonomous ai agents: A comprehensive review,"MA Ferrag,N Tihanyi,M Debbah- arXiv preprint arXiv:2504.19678, 2025",arxiv.org,,https://arxiv.org/abs/2504.19678,,"Large language models and autonomous AI agents have evolved rapidly, resulting in adiverse array of evaluation benchmarks, frameworks, and collaboration protocols. However¬†‚Ä¶",61.0,Survey
SimpleSafetyTest,SimpleSafetyTest: Universal Safety Testing for Language Models,Safetyprompts: a systematic review of open datasets for evaluating and improving large language model safety,"P R√∂ttger,F Pernisi,B Vidgen,D Hovy- Proceedings of the AAAI¬†‚Ä¶, 2025",ojs.aaai.org,,https://ojs.aaai.org/index.php/AAAI/article/view/34975,,The last two years have seen a rapid growth in concerns around the safety of largelanguage models (LLMs). Researchers and practitioners have met these concerns by¬†‚Ä¶,52.0,Survey
SimpleSafetyTest,SimpleSafetyTest: Universal Safety Testing for Language Models,Jailbreaking leading safety-aligned llms with simple adaptive attacks,"M Andriushchenko,F Croce,N Flammarion- arXiv preprint arXiv¬†‚Ä¶, 2024",arxiv.org,,https://arxiv.org/abs/2404.02151,,"We show that even the most recent safety-aligned LLMs are not robust to simple adaptivejailbreaking attacks. First, we demonstrate how to successfully leverage access to logprobs¬†‚Ä¶",284.0,Methodology
SimpleSafetyTest,SimpleSafetyTest: Universal Safety Testing for Language Models,"Wildguard: Open one-stop moderation tools for safety risks, jailbreaks, and refusals of llms","S Han,K Rao,A Ettinger,L Jiang‚Ä¶¬†- Advances in¬†‚Ä¶, 2024",proceedings.neurips.cc,,https://proceedings.neurips.cc/paper_files/paper/2024/hash/0f69b4b96a46f284b726fbd70f74fb3b-Abstract-Datasets_and_Benchmarks_Track.html,,"We introduce WildGuard---an open, light-weight moderation tool for LLM safety thatachieves three goals:(1) identifying malicious intent in user prompts,(2) detecting safety risks¬†‚Ä¶",169.0,Methodology
SimpleSafetyTest,SimpleSafetyTest: Universal Safety Testing for Language Models,Xstest: A test suite for identifying exaggerated safety behaviours in large language models,"P R√∂ttger,HR Kirk,B Vidgen,G Attanasio‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2023",arxiv.org,,https://arxiv.org/abs/2308.01263,,"Without proper safeguards, large language models will readily follow malicious instructionsand generate toxic content. This risk motivates safety efforts such as red-teaming and large¬†‚Ä¶",349.0,Benchmark
SimpleSafetyTest,SimpleSafetyTest: Universal Safety Testing for Language Models,Sorry-bench: Systematically evaluating large language model safety refusal,"T Xie,X Qi,Y Zeng,Y Huang,UM Sehwag‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2024",arxiv.org,,https://arxiv.org/abs/2406.14598,,"Evaluating aligned large language models'(LLMs) ability to recognize and reject unsafe userrequests is crucial for safe, policy-compliant deployments. Existing evaluation efforts¬†‚Ä¶",141.0,Benchmark
SimpleSafetyTest,SimpleSafetyTest: Universal Safety Testing for Language Models,The responsible foundation model development cheatsheet: A review of tools & resources,"S Longpre,S Biderman,A Albalak‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2024",arxiv.org,,https://arxiv.org/abs/2406.16746,,"Foundation model development attracts a rapidly expanding body of contributors, scientists,and applications. To help shape responsible development practices, we introduce the¬†‚Ä¶",15.0,Survey
SimpleSafetyTest,SimpleSafetyTest: Universal Safety Testing for Language Models,Aegis: Online adaptive ai content safety moderation with ensemble of llm experts,"S Ghosh,P Varshney,E Galinkin,C Parisien- arXiv preprint arXiv¬†‚Ä¶, 2024",arxiv.org,,https://arxiv.org/abs/2404.05993,,"As Large Language Models (LLMs) and generative AI become more widespread, thecontent safety risks associated with their use also increase. We find a notable deficiency in¬†‚Ä¶",68.0,Methodology
SimpleSafetyTest,SimpleSafetyTest: Universal Safety Testing for Language Models,Latent adversarial training improves robustness to persistent harmful behaviors in llms,"A Sheshadri,A Ewart,P Guo,A Lynch,C Wu‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2024",arxiv.org,,https://arxiv.org/abs/2407.15549,,"Large language models (LLMs) can often be made to behave in undesirable ways that theyare explicitly fine-tuned not to. For example, the LLM red-teaming literature has produced a¬†‚Ä¶",39.0,Methodology
SimpleSafetyTest,SimpleSafetyTest: Universal Safety Testing for Language Models,Against The Achilles' Heel: A Survey on Red Teaming for Generative Models,"L Lin,H Mu,Z Zhai,M Wang,Y Wang,R Wang‚Ä¶¬†- Journal of Artificial¬†‚Ä¶, 2025",jair.org,,https://www.jair.org/index.php/jair/article/view/17654,,"Generative models are rapidly gaining popularity and being integrated into everydayapplications, raising concerns over their safe use as various vulnerabilities are exposed. In¬†‚Ä¶",41.0,Survey
SimpleSafetyTest,SimpleSafetyTest: Universal Safety Testing for Language Models,Guardreasoner: Towards reasoning-based llm safeguards,"Y Liu,H Gao,S Zhai,J Xia,T Wu, Z Xue‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2025",arxiv.org,,https://arxiv.org/abs/2501.18492,,"As LLMs increasingly impact safety-critical applications, ensuring their safety usingguardrails remains a key challenge. This paper proposes GuardReasoner, a new safeguard¬†‚Ä¶",49.0,Methodology
XSTest,XSTest: A Test Suite for Identifying Exaggerated Safety Behaviors in Large Language Models,Adversarial attacks of vision tasks in the past 10 years: A survey,"C Zhang, L Zhou,X Xu, J Wu,Z Liu- ACM Computing Surveys, 2025",dl.acm.org,,https://dl.acm.org/doi/abs/10.1145/3743126,,"With the advent of Large Vision-Language Models (LVLMs), new attack vectors, such ascognitive bias, prompt injection, and jailbreaking, have emerged. Understanding these¬†‚Ä¶",17.0,Survey
XSTest,XSTest: A Test Suite for Identifying Exaggerated Safety Behaviors in Large Language Models,Safetyprompts: a systematic review of open datasets for evaluating and improving large language model safety,"P R√∂ttger,F Pernisi,B Vidgen,D Hovy- Proceedings of the AAAI¬†‚Ä¶, 2025",ojs.aaai.org,,https://ojs.aaai.org/index.php/AAAI/article/view/34975,,The last two years have seen a rapid growth in concerns around the safety of largelanguage models (LLMs). Researchers and practitioners have met these concerns by¬†‚Ä¶,52.0,Survey
XSTest,XSTest: A Test Suite for Identifying Exaggerated Safety Behaviors in Large Language Models,The llama 3 herd of models,"A Grattafiori,A Dubey, A Jauhri, A Pandey‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2024",arxiv.org,,https://arxiv.org/abs/2407.21783,,"Modern artificial intelligence (AI) systems are powered by foundation models. This paperpresents a new set of foundation models, called Llama 3. It is a herd of language models¬†‚Ä¶",3144.0,Methodology
XSTest,XSTest: A Test Suite for Identifying Exaggerated Safety Behaviors in Large Language Models,"Fine-tuning aligned language models compromises safety, even when users do not intend to!","X Qi,Y Zeng,T Xie,PY Chen,R Jia,P Mittal‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2023",arxiv.org,,https://arxiv.org/abs/2310.03693,,Optimizing large language models (LLMs) for downstream use cases often involves thecustomization of pre-trained LLMs through further fine-tuning. Meta's open release of Llama¬†‚Ä¶,841.0,Other
XSTest,XSTest: A Test Suite for Identifying Exaggerated Safety Behaviors in Large Language Models,Jailbreakbench: An open robustness benchmark for jailbreaking large language models,"P Chao,E Debenedetti,A Robey‚Ä¶¬†- Advances in¬†‚Ä¶, 2024",proceedings.neurips.cc,,https://proceedings.neurips.cc/paper_files/paper/2024/hash/63092d79154adebd7305dfd498cbff70-Abstract-Datasets_and_Benchmarks_Track.html,,"Jailbreak attacks cause large language models (LLMs) to generate harmful, unethical, orotherwise objectionable content. Evaluating these attacks presents a number of challenges¬†‚Ä¶",350.0,Benchmark
XSTest,XSTest: A Test Suite for Identifying Exaggerated Safety Behaviors in Large Language Models,Openai o1 system card,"A Jaech,A Kalai,A Lerer, A Richardson‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2024",arxiv.org,,https://arxiv.org/abs/2412.16720,,The o1 model series is trained with large-scale reinforcement learning to reason using chainof thought. These advanced reasoning capabilities provide new avenues for improving the¬†‚Ä¶,1203.0,Methodology
XSTest,XSTest: A Test Suite for Identifying Exaggerated Safety Behaviors in Large Language Models,Rewardbench: Evaluating reward models for language modeling,"N Lambert,V Pyatkin,J Morrison,LJ Miranda‚Ä¶¬†- arXiv preprint arXiv¬†‚Ä¶, 2024",arxiv.org,,https://arxiv.org/abs/2403.13787,,"Reward models (RMs) are at the crux of successfully using RLHF to align pretrained modelsto human preferences, yet there has been relatively little study that focuses on evaluation of¬†‚Ä¶",393.0,Benchmark
XSTest,XSTest: A Test Suite for Identifying Exaggerated Safety Behaviors in Large Language Models,Gptfuzzer: Red teaming large language models with auto-generated jailbreak prompts,"J Yu,X Lin,Z Yu,X Xing- arXiv preprint arXiv:2309.10253, 2023",arxiv.org,,https://arxiv.org/abs/2309.10253,,"Large language models (LLMs) have recently experienced tremendous popularity and arewidely used from casual conversations to AI-driven programming. However, despite their¬†‚Ä¶",446.0,Methodology
XSTest,XSTest: A Test Suite for Identifying Exaggerated Safety Behaviors in Large Language Models,Improving alignment and robustness with circuit breakers,"A Zou,L Phan,J Wang, D Duenas‚Ä¶¬†- Advances in¬†‚Ä¶, 2024",proceedings.neurips.cc,,https://proceedings.neurips.cc/paper_files/paper/2024/hash/97ca7168c2c333df5ea61ece3b3276e1-Abstract-Conference.html,,"AI systems can take harmful actions and are highly vulnerable to adversarial attacks. Wepresent an approach, inspired by recent advances in representation engineering, that¬†‚Ä¶",139.0,Methodology
XSTest,XSTest: A Test Suite for Identifying Exaggerated Safety Behaviors in Large Language Models,"Wildguard: Open one-stop moderation tools for safety risks, jailbreaks, and refusals of llms","S Han,K Rao,A Ettinger,L Jiang‚Ä¶¬†- Advances in¬†‚Ä¶, 2024",proceedings.neurips.cc,,https://proceedings.neurips.cc/paper_files/paper/2024/hash/0f69b4b96a46f284b726fbd70f74fb3b-Abstract-Datasets_and_Benchmarks_Track.html,,"We introduce WildGuard---an open, light-weight moderation tool for LLM safety thatachieves three goals:(1) identifying malicious intent in user prompts,(2) detecting safety risks¬†‚Ä¶",169.0,Methodology
