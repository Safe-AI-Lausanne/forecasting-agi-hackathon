Benchmark Name,Paper Title,Google Scholar Link
TruthfulQA,TruthfulQA: Measuring How Models Mimic Human Falsehoods,https://scholar.google.com/scholar?q=TruthfulQA%3A+Measuring+How+Models+Mimic+Human+Falsehoods+Lin+Hilton+Evans
HaluEval,HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models,https://scholar.google.com/scholar?q=HaluEval%3A+A+Large-Scale+Hallucination+Evaluation+Benchmark+for+Large+Language+Models
TRUE,TRUE: Re-evaluating Factual Consistency Evaluation,https://scholar.google.com/scholar?q=TRUE%3A+Re-evaluating+Factual+Consistency+Evaluation+Honovich+Aharoni
FIB,Evaluating the Factual Consistency of Large Language Models Through News Summarization,https://scholar.google.com/scholar?q=Evaluating+the+Factual+Consistency+of+Large+Language+Models+Through+News+Summarization+Tam+Raffel
FEVER,FEVER: a Large-scale Dataset for Fact Extraction and VERification,https://scholar.google.com/scholar?q=FEVER%3A+a+Large-scale+Dataset+for+Fact+Extraction+and+VERification+Thorne+Vlachos
HalluLens,HalluLens: LLM Hallucination Benchmark,https://scholar.google.com/scholar?q=HalluLens%3A+LLM+Hallucination+Benchmark+Bang
FreshQA,FreshLLMs: Refreshing Large Language Models with Search Engine Augmentation,https://scholar.google.com/scholar?q=FreshLLMs%3A+Refreshing+Large+Language+Models+with+Search+Engine+Augmentation+FreshQA
ETHICS,Aligning AI With Shared Human Values,https://scholar.google.com/scholar?q=Aligning+AI+With+Shared+Human+Values+Hendrycks
Moral Stories,Moral Stories: Situated Reasoning about Norms Intents Actions and their Consequences,https://scholar.google.com/scholar?q=Moral+Stories+Situated+Reasoning+Norms+Intents+Actions+Consequences
Jiminy Cricket,What Would Jiminy Cricket Do? Towards Agents That Behave Morally,https://scholar.google.com/scholar?q=What+Would+Jiminy+Cricket+Do+Towards+Agents+That+Behave+Morally
SCRUPLES,Scruples: A Corpus of Community Ethical Judgments on 32000 Real-Life Anecdotes,https://scholar.google.com/scholar?q=Scruples+Corpus+Community+Ethical+Judgments+32000+Real-Life+Anecdotes
MoralBench,MoralBench: Moral Evaluation of LLMs,https://scholar.google.com/scholar?q=MoralBench+Moral+Evaluation+LLMs
Social Chemistry 101,Social Chemistry 101: Learning to Reason about Social and Moral Norms,https://scholar.google.com/scholar?q=Social+Chemistry+101+Learning+Reason+Social+Moral+Norms
Delphi,Delphi: Towards Machine Ethics and Norms,https://scholar.google.com/scholar?q=Delphi+Towards+Machine+Ethics+and+Norms
STORAL,A Corpus for Understanding and Generating Moral Stories,https://scholar.google.com/scholar?q=Corpus+Understanding+Generating+Moral+Stories+Guan
Moral Integrity Corpus,The Moral Integrity Corpus: A Benchmark for Ethical Dialogue Systems,https://scholar.google.com/scholar?q=Moral+Integrity+Corpus+Benchmark+Ethical+Dialogue+Systems
TRIAGE,Medical Triage as an AI Ethics Benchmark,https://scholar.google.com/scholar?q=Medical+Triage+as+an+AI+Ethics+Benchmark
RobustBench,RobustBench: a standardized adversarial robustness benchmark,https://scholar.google.com/scholar?q=RobustBench+a+standardized+adversarial+robustness+benchmark+Croce
AutoAttack,Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks,https://scholar.google.com/scholar?q=Reliable+evaluation+adversarial+robustness+ensemble+diverse+parameter-free+attacks+Croce+Hein
ImageNet-C / ImageNet-P,Benchmarking Neural Network Robustness to Common Corruptions and Perturbations,https://scholar.google.com/scholar?q=Benchmarking+Neural+Network+Robustness+Common+Corruptions+Perturbations+Hendrycks
WILDS,WILDS: A Benchmark of in-the-Wild Distribution Shifts,https://scholar.google.com/scholar?q=WILDS+Benchmark+in-the-Wild+Distribution+Shifts+Koh
OoD-Bench,OoD-Bench: Quantifying and Understanding Two Dimensions of Out-of-Distribution Generalization,https://scholar.google.com/scholar?q=OoD-Bench+Quantifying+Understanding+Two+Dimensions+Out-of-Distribution+Generalization
SoK: Certified Robustness,SoK: Certified Robustness for Deep Neural Networks,https://scholar.google.com/scholar?q=SoK+Certified+Robustness+Deep+Neural+Networks+Li
Adversarial Robustness Benchmark,Benchmarking Adversarial Robustness on Image Classification,https://scholar.google.com/scholar?q=Benchmarking+Adversarial+Robustness+Image+Classification+Dong
GRB,Graph Robustness Benchmark: Benchmarking the Adversarial Robustness of Graph Machine Learning,https://scholar.google.com/scholar?q=Graph+Robustness+Benchmark+Adversarial+Robustness+Graph+Machine+Learning
RealToxicityPrompts,RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models,https://scholar.google.com/scholar?q=RealToxicityPrompts+Evaluating+Neural+Toxic+Degeneration+Language+Models
SafetyBench,SafetyBench: Evaluating the Safety of Large Language Models with Multiple Choice Questions,https://scholar.google.com/scholar?q=SafetyBench+Evaluating+Safety+Large+Language+Models
ToxiGen,ToxiGen: A Large-Scale Machine-Generated Dataset for Adversarial and Implicit Hate Speech Detection,https://scholar.google.com/scholar?q=ToxiGen+Large-Scale+Machine-Generated+Dataset+Adversarial+Implicit+Hate+Speech
BBQ,BBQ: A Hand-Built Bias Benchmark for Question Answering,https://scholar.google.com/scholar?q=BBQ+Hand-Built+Bias+Benchmark+Question+Answering
BOLD,BOLD: Dataset and Metrics for Measuring Biases in Open-Ended Language Generation,https://scholar.google.com/scholar?q=BOLD+Dataset+Metrics+Measuring+Biases+Open-Ended+Language+Generation
JailbreakBench,JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models,https://scholar.google.com/scholar?q=JailbreakBench+Open+Robustness+Benchmark+Jailbreaking+Large+Language+Models
HarmBench,HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal,https://scholar.google.com/scholar?q=HarmBench+Standardized+Evaluation+Framework+Automated+Red+Teaming
DecodingTrust,DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models,https://scholar.google.com/scholar?q=DecodingTrust+Comprehensive+Assessment+Trustworthiness+GPT+Models
AdvBench,Universal and Transferable Adversarial Attacks on Aligned Language Models,https://scholar.google.com/scholar?q=Universal+Transferable+Adversarial+Attacks+Aligned+Language+Models
HarmfulQA,Red-Teaming Large Language Models using Chain of Utterances for Safety-Alignment,https://scholar.google.com/scholar?q=Red-Teaming+Large+Language+Models+Chain+Utterances+Safety-Alignment
DoNotAnswer,Do-Not-Answer: A Dataset for Evaluating Safeguards in LLMs,https://scholar.google.com/scholar?q=Do-Not-Answer+Dataset+Evaluating+Safeguards+LLMs
SALAD-Bench,SALAD-Bench: A Hierarchical and Comprehensive Safety Benchmark for Large Language Models,https://scholar.google.com/scholar?q=SALAD-Bench+Hierarchical+Comprehensive+Safety+Benchmark+Large+Language+Models
MACHIAVELLI,Do the Rewards Justify the Means? Measuring Trade-Offs Between Rewards and Ethical Behavior in the MACHIAVELLI Benchmark,https://scholar.google.com/scholar?q=Do+the+Rewards+Justify+the+Means+Measuring+Trade-Offs+Between+Rewards+and+Ethical+Behavior+in+the+MACHIAVELLI+Benchmark+Pan
ELEPHANT,ELEPHANT: Measuring and Understanding Social Sycophancy in LLMs,https://scholar.google.com/scholar?q=ELEPHANT+Measuring+and+Understanding+Social+Sycophancy+in+LLMs+Cheng
OpenDeception,OpenDeception: Benchmarking and Investigating AI Deceptive Behaviors via Open-ended Interaction Simulation,https://scholar.google.com/scholar?q=OpenDeception+Benchmarking+and+Investigating+AI+Deceptive+Behaviors
Goal Misgeneralization (Procgen),Goal Misgeneralization in Deep Reinforcement Learning,https://scholar.google.com/scholar?q=Goal+Misgeneralization+in+Deep+Reinforcement+Learning+Langosco+Koch
Goal Misgeneralization (Analysis),Goal Misgeneralization: Why Correct Specifications Aren't Enough For Correct Goals,https://scholar.google.com/scholar?q=Goal+Misgeneralization+Why+Correct+Specifications+Aren%27t+Enough+For+Correct+Goals+Shah
IPS Index,Instrumental Power-Seeking Index: A Novel Benchmark for Detecting Power-Seeking Behavior in LLMs,https://scholar.google.com/scholar?q=Instrumental+Power-Seeking+Index+Novel+Benchmark+Detecting+Power-Seeking+Behavior+LLMs
SycEval,SycEval: Evaluating LLM Sycophancy,https://scholar.google.com/scholar?q=SycEval+Evaluating+LLM+Sycophancy
HELM Safety,HELM Safety: Towards Standardized Safety Evaluations of Language Models,https://scholar.google.com/scholar?q=HELM+Safety+Towards+Standardized+Safety+Evaluations+Language+Models+Stanford
AIR-Bench 2024,AIR-Bench 2024: A Safety Benchmark Based on Risk Categories from Regulations and Policies,https://scholar.google.com/scholar?q=AIR-Bench+2024+Safety+Benchmark+Risk+Categories+Regulations+Policies
TrustLLM,TrustLLM: Trustworthiness in Large Language Models,https://scholar.google.com/scholar?q=TrustLLM+Trustworthiness+Large+Language+Models
MLCommons AILuminate,AILuminate: Introducing v1.0 of the AI Risk and Reliability Benchmark from MLCommons,https://scholar.google.com/scholar?q=AILuminate+Introducing+v1.0+AI+Risk+Reliability+Benchmark+MLCommons
ALERT,ALERT: A Comprehensive Benchmark for Assessing Large Language Models' Safety through Red Teaming,https://scholar.google.com/scholar?q=ALERT+Comprehensive+Benchmark+Assessing+Large+Language+Models+Safety+Red+Teaming
AgentHarm,AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents,https://scholar.google.com/scholar?q=AgentHarm+Benchmark+Measuring+Harmfulness+LLM+Agents
SimpleSafetyTest,SimpleSafetyTest: Universal Safety Testing for Language Models,https://scholar.google.com/scholar?q=SimpleSafetyTest+Universal+Safety+Testing+Language+Models
XSTest,XSTest: A Test Suite for Identifying Exaggerated Safety Behaviors in Large Language Models,https://scholar.google.com/scholar?q=XSTest+Test+Suite+Identifying+Exaggerated+Safety+Behaviors+Large+Language+Models
AnthropicRedTeam,Anthropic Red Team Dataset: Adversarial Testing for Language Models,https://scholar.google.com/scholar?q=AnthropicRedTeam+Adversarial+Testing+Language+Models
BELLS,BELLS: Benchmarking Emergent Large Language Model Safeguards,https://scholar.google.com/scholar?q=BELLS+Benchmarking+Emergent+Large+Language+Model+Safeguards
GaslightBench,GaslightBench: Quantifying LLM Susceptibility to Social Prompting,https://scholar.google.com/scholar?q=GaslightBench+Quantifying+LLM+Susceptibility+Social+Prompting
CoinRun,Quantifying Generalization in Reinforcement Learning,https://scholar.google.com/scholar?q=CoinRun+Quantifying+Generalization+Reinforcement+Learning
HappyFaces,Goal Misgeneralization in Deep Reinforcement Learning,https://scholar.google.com/scholar?q=HappyFaces+Goal+Misgeneralization+Deep+Reinforcement+Learning
WildBench,WildBench: Benchmarking LLMs with Challenging Tasks from Real Users in the Wild,https://scholar.google.com/scholar?q=WildBench+Benchmarking+LLMs+Challenging+Tasks+Real+Users+Wild
Perspective API Benchmark,Perspective API: Machine Learning to Reduce Toxicity Online,https://scholar.google.com/scholar?q=Perspective+API+Machine+Learning+Reduce+Toxicity+Online
MLCommons AI Safety v0.5,Introducing v0.5 of the AI Safety Benchmark from MLCommons,https://scholar.google.com/scholar?q=Introducing+v0.5+AI+Safety+Benchmark+MLCommons