[
  {
    "benchmark_name": "TruthfulQA",
    "benchmark_paper": "TruthfulQA: Measuring How Models Mimic Human Falsehoods",
    "title": "A survey on evaluation of large language models",
    "authors": "Y Chang,X Wang,J Wang,Y Wu,L Yangâ€¦Â - ACM transactions onÂ â€¦, 2024",
    "publication": "dl.acm.org",
    "year": NaN,
    "url": "https://dl.acm.org/doi/abs/10.1145/3641289",
    "pdf_url": NaN,
    "abstract": "Large language models (LLMs) are gaining increasing popularity in both academia andindustry, owing to their unprecedented performance in various applications. As LLMsÂ â€¦",
    "cited_by_count": 4405.0
  },
  {
    "benchmark_name": "TruthfulQA",
    "benchmark_paper": "TruthfulQA: Measuring How Models Mimic Human Falsehoods",
    "title": "A survey on large language model (llm) security and privacy: The good, the bad, and the ugly",
    "authors": "Y Yao,J Duan,K Xu, Y Cai,Z Sun,Y Zhang- High-Confidence Computing, 2024",
    "publication": "Elsevier",
    "year": NaN,
    "url": "https://www.sciencedirect.com/science/article/pii/S266729522400014X",
    "pdf_url": NaN,
    "abstract": "Abstract Large Language Models (LLMs), such as ChatGPT and Bard, have revolutionizednatural language understanding and generation. They possess deep languageÂ â€¦",
    "cited_by_count": 1335.0
  },
  {
    "benchmark_name": "TruthfulQA",
    "benchmark_paper": "TruthfulQA: Measuring How Models Mimic Human Falsehoods",
    "title": "A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions",
    "authors": "L Huang,W Yu,W Ma,W Zhong,Z Fengâ€¦Â - ACM Transactions onÂ â€¦, 2025",
    "publication": "dl.acm.org",
    "year": NaN,
    "url": "https://dl.acm.org/doi/abs/10.1145/3703155",
    "pdf_url": NaN,
    "abstract": "The emergence of large language models (LLMs) has marked a significant breakthrough innatural language processing (NLP), fueling a paradigm shift in information acquisitionÂ â€¦",
    "cited_by_count": 3037.0
  },
  {
    "benchmark_name": "TruthfulQA",
    "benchmark_paper": "TruthfulQA: Measuring How Models Mimic Human Falsehoods",
    "title": "ðŸ§œ Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models",
    "authors": "Y Zhang,Y Li,L Cui,D Cai,L Liu,T Fuâ€¦Â - ComputationalÂ â€¦, 2025",
    "publication": "direct.mit.edu",
    "year": NaN,
    "url": "https://direct.mit.edu/coli/article/doi/10.1162/coli.a.16/131631",
    "pdf_url": NaN,
    "abstract": "While large language models (LLMs) have demonstrated remarkable capabilities across arange of downstream tasks, a significant concern revolves around their propensity to exhibitÂ â€¦",
    "cited_by_count": 1549.0
  },
  {
    "benchmark_name": "TruthfulQA",
    "benchmark_paper": "TruthfulQA: Measuring How Models Mimic Human Falsehoods",
    "title": "Simpo: Simple preference optimization with a reference-free reward",
    "authors": "Y Meng,M Xia,D Chen- Advances in Neural InformationÂ â€¦, 2024",
    "publication": "proceedings.neurips.cc",
    "year": NaN,
    "url": "https://proceedings.neurips.cc/paper_files/paper/2024/hash/e099c1c9699814af0be873a175361713-Abstract-Conference.html",
    "pdf_url": NaN,
    "abstract": "Abstract Direct Preference Optimization (DPO) is a widely used offline preferenceoptimization algorithm that reparameterizes reward functions in reinforcement learning fromÂ â€¦",
    "cited_by_count": 657.0
  },
  {
    "benchmark_name": "TruthfulQA",
    "benchmark_paper": "TruthfulQA: Measuring How Models Mimic Human Falsehoods",
    "title": "Llama 2: Open foundation and fine-tuned chat models",
    "authors": "H Touvron,L Martin,K Stone, P Albertâ€¦Â - arXiv preprint arXivÂ â€¦, 2023",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2307.09288",
    "pdf_url": NaN,
    "abstract": "In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned largelanguage models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fineÂ â€¦",
    "cited_by_count": 19070.0
  },
  {
    "benchmark_name": "TruthfulQA",
    "benchmark_paper": "TruthfulQA: Measuring How Models Mimic Human Falsehoods",
    "title": "A comprehensive overview of large language models",
    "authors": "H Naveed,AU Khan,S Qiu,M Saqib,S Anwarâ€¦Â - ACM Transactions onÂ â€¦, 2025",
    "publication": "dl.acm.org",
    "year": NaN,
    "url": "https://dl.acm.org/doi/abs/10.1145/3744746",
    "pdf_url": NaN,
    "abstract": "Large Language Models (LLMs) have recently demonstrated remarkable capabilities innatural language processing tasks and beyond. This success of LLMs has led to a largeÂ â€¦",
    "cited_by_count": 1815.0
  },
  {
    "benchmark_name": "TruthfulQA",
    "benchmark_paper": "TruthfulQA: Measuring How Models Mimic Human Falsehoods",
    "title": "Large language models: A survey",
    "authors": "S Minaee,T Mikolov,N Nikzad,M Chenaghluâ€¦Â - arXiv preprint arXivÂ â€¦, 2024",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2402.06196",
    "pdf_url": NaN,
    "abstract": "Large Language Models (LLMs) have drawn a lot of attention due to their strongperformance on a wide range of natural language tasks, since the release of ChatGPT inÂ â€¦",
    "cited_by_count": 1504.0
  },
  {
    "benchmark_name": "TruthfulQA",
    "benchmark_paper": "TruthfulQA: Measuring How Models Mimic Human Falsehoods",
    "title": "Judging llm-as-a-judge with mt-bench and chatbot arena",
    "authors": "L Zheng,WL Chiang,Y Shengâ€¦Â - Advances in neuralÂ â€¦, 2023",
    "publication": "proceedings.neurips.cc",
    "year": NaN,
    "url": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/91f18a1287b398d378ef22505bf41832-Abstract-Datasets_and_Benchmarks.html",
    "pdf_url": NaN,
    "abstract": "Evaluating large language model (LLM) based chat assistants is challenging due to theirbroad capabilities and the inadequacy of existing benchmarks in measuring humanÂ â€¦",
    "cited_by_count": 5891.0
  },
  {
    "benchmark_name": "TruthfulQA",
    "benchmark_paper": "TruthfulQA: Measuring How Models Mimic Human Falsehoods",
    "title": "A survey of large language models",
    "authors": "WX Zhao,K Zhou,J Li,T Tang,X Wangâ€¦Â - arXiv preprint arXivÂ â€¦, 2023",
    "publication": "researchgate.net",
    "year": NaN,
    "url": "https://www.researchgate.net/profile/Tang-Tianyi-3/publication/369740832_A_Survey_of_Large_Language_Models/links/665fd2e3637e4448a37dd281/A-Survey-of-Large-Language-Models.pdf",
    "pdf_url": NaN,
    "abstract": "Ever since the Turing Test was proposed in the 1950s, humans have explored the masteringof language intelligence by machine. Language is essentially a complex, intricate system ofÂ â€¦",
    "cited_by_count": 6302.0
  },
  {
    "benchmark_name": "HaluEval",
    "benchmark_paper": "HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models",
    "title": "A survey of GPT-3 family large language models including ChatGPT and GPT-4",
    "authors": "KS Kalyan- Natural Language Processing Journal, 2024",
    "publication": "Elsevier",
    "year": NaN,
    "url": "https://www.sciencedirect.com/science/article/pii/S2949719123000456",
    "pdf_url": NaN,
    "abstract": "Large language models (LLMs) are a special class of pretrained language models (PLMs)obtained by scaling model size, pretraining corpus and computation. LLMs, because of theirÂ â€¦",
    "cited_by_count": 512.0
  },
  {
    "benchmark_name": "HaluEval",
    "benchmark_paper": "HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models",
    "title": "From google gemini to openai q*(q-star): A survey on reshaping the generative artificial intelligence (ai) research landscape",
    "authors": "TR McIntosh,T Susnjak,T Liu,P Watters,D Xu,D Liuâ€¦Â - Technologies, 2025",
    "publication": "mdpi.com",
    "year": NaN,
    "url": "https://www.mdpi.com/2227-7080/13/2/51",
    "pdf_url": NaN,
    "abstract": "This comprehensive survey explored the evolving landscape of generative ArtificialIntelligence (AI), with a specific focus on the recent technological breakthroughs and theÂ â€¦",
    "cited_by_count": 250.0
  },
  {
    "benchmark_name": "HaluEval",
    "benchmark_paper": "HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models",
    "title": "A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions",
    "authors": "L Huang,W Yu,W Ma,W Zhong,Z Fengâ€¦Â - ACM Transactions onÂ â€¦, 2025",
    "publication": "dl.acm.org",
    "year": NaN,
    "url": "https://dl.acm.org/doi/abs/10.1145/3703155",
    "pdf_url": NaN,
    "abstract": "The emergence of large language models (LLMs) has marked a significant breakthrough innatural language processing (NLP), fueling a paradigm shift in information acquisitionÂ â€¦",
    "cited_by_count": 3037.0
  },
  {
    "benchmark_name": "HaluEval",
    "benchmark_paper": "HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models",
    "title": "ðŸ§œ Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models",
    "authors": "Y Zhang,Y Li,L Cui,D Cai,L Liu,T Fuâ€¦Â - ComputationalÂ â€¦, 2025",
    "publication": "direct.mit.edu",
    "year": NaN,
    "url": "https://direct.mit.edu/coli/article/doi/10.1162/coli.a.16/131631",
    "pdf_url": NaN,
    "abstract": "While large language models (LLMs) have demonstrated remarkable capabilities across arange of downstream tasks, a significant concern revolves around their propensity to exhibitÂ â€¦",
    "cited_by_count": 1549.0
  },
  {
    "benchmark_name": "HaluEval",
    "benchmark_paper": "HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models",
    "title": "Large language models: A survey",
    "authors": "S Minaee,T Mikolov,N Nikzad,M Chenaghluâ€¦Â - arXiv preprint arXivÂ â€¦, 2024",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2402.06196",
    "pdf_url": NaN,
    "abstract": "Large Language Models (LLMs) have drawn a lot of attention due to their strongperformance on a wide range of natural language tasks, since the release of ChatGPT inÂ â€¦",
    "cited_by_count": 1504.0
  },
  {
    "benchmark_name": "HaluEval",
    "benchmark_paper": "HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models",
    "title": "A survey of large language models",
    "authors": "WX Zhao,K Zhou,J Li,T Tang,X Wangâ€¦Â - arXiv preprint arXivÂ â€¦, 2023",
    "publication": "researchgate.net",
    "year": NaN,
    "url": "https://www.researchgate.net/profile/Tang-Tianyi-3/publication/369740832_A_Survey_of_Large_Language_Models/links/665fd2e3637e4448a37dd281/A-Survey-of-Large-Language-Models.pdf",
    "pdf_url": NaN,
    "abstract": "Ever since the Turing Test was proposed in the 1950s, humans have explored the masteringof language intelligence by machine. Language is essentially a complex, intricate system ofÂ â€¦",
    "cited_by_count": 6302.0
  },
  {
    "benchmark_name": "HaluEval",
    "benchmark_paper": "HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models",
    "title": "Hallucination is inevitable: An innate limitation of large language models",
    "authors": "Z Xu,S Jain,M Kankanhalli- arXiv preprint arXiv:2401.11817, 2024",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2401.11817",
    "pdf_url": NaN,
    "abstract": "Hallucination has been widely recognized to be a significant drawback for large languagemodels (LLMs). There have been many works that attempt to reduce the extent ofÂ â€¦",
    "cited_by_count": 723.0
  },
  {
    "benchmark_name": "HaluEval",
    "benchmark_paper": "HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models",
    "title": "Ragas: Automated evaluation of retrieval augmented generation",
    "authors": "S Es,J James,LE Ankeâ€¦Â - Proceedings of the 18thÂ â€¦, 2024",
    "publication": "aclanthology.org",
    "year": NaN,
    "url": "https://aclanthology.org/2024.eacl-demo.16/",
    "pdf_url": NaN,
    "abstract": "Abstract We introduce RAGAs (Retrieval Augmented Generation Assessment), a frameworkfor reference-free evaluation of Retrieval Augmented Generation (RAG) pipelines. RAGAs isÂ â€¦",
    "cited_by_count": 795.0
  },
  {
    "benchmark_name": "HaluEval",
    "benchmark_paper": "HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models",
    "title": "Large language models for information retrieval: A survey",
    "authors": "Y Zhu,H Yuan,S Wang,J Liu,W Liu,C Dengâ€¦Â - arXiv preprint arXivÂ â€¦, 2023",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2308.07107",
    "pdf_url": NaN,
    "abstract": "As a primary means of information acquisition, information retrieval (IR) systems, such assearch engines, have integrated themselves into our daily lives. These systems also serveÂ â€¦",
    "cited_by_count": 585.0
  },
  {
    "benchmark_name": "HaluEval",
    "benchmark_paper": "HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models",
    "title": "Large legal fictions: Profiling legal hallucinations in large language models",
    "authors": "M Dahl, V Magesh,M Suzgunâ€¦Â - Journal of Legal Analysis, 2024",
    "publication": "academic.oup.com",
    "year": NaN,
    "url": "https://academic.oup.com/jla/article-abstract/16/1/64/7699227",
    "pdf_url": NaN,
    "abstract": "Do large language models (LLMs) know the law? LLMs are increasingly being used toaugment legal practice, education, and research, yet their revolutionary potential isÂ â€¦",
    "cited_by_count": 295.0
  },
  {
    "benchmark_name": "TRUE",
    "benchmark_paper": "TRUE: Re-evaluating Factual Consistency Evaluation",
    "title": "A survey on evaluation of large language models",
    "authors": "Y Chang,X Wang,J Wang,Y Wu,L Yangâ€¦Â - ACM transactions onÂ â€¦, 2024",
    "publication": "dl.acm.org",
    "year": NaN,
    "url": "https://dl.acm.org/doi/abs/10.1145/3641289",
    "pdf_url": NaN,
    "abstract": "Large language models (LLMs) are gaining increasing popularity in both academia andindustry, owing to their unprecedented performance in various applications. As LLMsÂ â€¦",
    "cited_by_count": 4405.0
  },
  {
    "benchmark_name": "TRUE",
    "benchmark_paper": "TRUE: Re-evaluating Factual Consistency Evaluation",
    "title": "Survey of hallucination in natural language generation",
    "authors": "Z Ji,N Lee,R Frieske,T Yu,D Su,Y Xu,E Ishiiâ€¦Â - ACM computingÂ â€¦, 2023",
    "publication": "dl.acm.org",
    "year": NaN,
    "url": "https://dl.acm.org/doi/abs/10.1145/3571730",
    "pdf_url": NaN,
    "abstract": "Natural Language Generation (NLG) has improved exponentially in recent years thanks tothe development of sequence-to-sequence deep learning technologies such as TransformerÂ â€¦",
    "cited_by_count": 5570.0
  },
  {
    "benchmark_name": "TRUE",
    "benchmark_paper": "TRUE: Re-evaluating Factual Consistency Evaluation",
    "title": "Detecting hallucinations in large language models using semantic entropy",
    "authors": "S Farquhar,J Kossen,L Kuhn,Y Gal- Nature, 2024",
    "publication": "nature.com",
    "year": NaN,
    "url": "https://www.nature.com/articles/s41586-024-07421-0",
    "pdf_url": NaN,
    "abstract": "Large language model (LLM) systems, such as ChatGPT or Gemini, can show impressivereasoning and question-answering capabilities but often 'hallucinate'false outputs andÂ â€¦",
    "cited_by_count": 772.0
  },
  {
    "benchmark_name": "TRUE",
    "benchmark_paper": "TRUE: Re-evaluating Factual Consistency Evaluation",
    "title": "Halueval: A large-scale hallucination evaluation benchmark for large language models",
    "authors": "J Li,X Cheng,WX Zhao,JY Nie,JR Wen- arXiv preprint arXiv:2305.11747, 2023",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2305.11747",
    "pdf_url": NaN,
    "abstract": "Large language models (LLMs), such as ChatGPT, are prone to generate hallucinations, ie,content that conflicts with the source or cannot be verified by the factual knowledge. ToÂ â€¦",
    "cited_by_count": 650.0
  },
  {
    "benchmark_name": "TRUE",
    "benchmark_paper": "TRUE: Re-evaluating Factual Consistency Evaluation",
    "title": "Enabling large language models to generate text with citations",
    "authors": "T Gao,H Yen,J Yu,D Chen- arXiv preprint arXiv:2305.14627, 2023",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2305.14627",
    "pdf_url": NaN,
    "abstract": "Large language models (LLMs) have emerged as a widely-used tool for informationseeking, but their generated outputs are prone to hallucination. In this work, our aim is toÂ â€¦",
    "cited_by_count": 437.0
  },
  {
    "benchmark_name": "TRUE",
    "benchmark_paper": "TRUE: Re-evaluating Factual Consistency Evaluation",
    "title": "Rarr: Researching and revising what language models say, using language models",
    "authors": "L Gao,Z Dai,P Pasupat,A Chen,AT Chagantyâ€¦Â - arXiv preprint arXivÂ â€¦, 2022",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2210.08726",
    "pdf_url": NaN,
    "abstract": "Language models (LMs) now excel at many tasks such as few-shot learning, questionanswering, reasoning, and dialog. However, they sometimes generate unsupported orÂ â€¦",
    "cited_by_count": 395.0
  },
  {
    "benchmark_name": "TRUE",
    "benchmark_paper": "TRUE: Re-evaluating Factual Consistency Evaluation",
    "title": "Large language model alignment: A survey",
    "authors": "T Shen,R Jin,Y Huang,C Liu,W Dong, Z Guoâ€¦Â - arXiv preprint arXivÂ â€¦, 2023",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2309.15025",
    "pdf_url": NaN,
    "abstract": "Recent years have witnessed remarkable progress made in large language models (LLMs).Such advancements, while garnering significant attention, have concurrently elicited variousÂ â€¦",
    "cited_by_count": 289.0
  },
  {
    "benchmark_name": "TRUE",
    "benchmark_paper": "TRUE: Re-evaluating Factual Consistency Evaluation",
    "title": "Making retrieval-augmented language models robust to irrelevant context",
    "authors": "O Yoran,T Wolfson,O Ram,J Berant- arXiv preprint arXiv:2310.01558, 2023",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2310.01558",
    "pdf_url": NaN,
    "abstract": "Retrieval-augmented language models (RALMs) hold promise to produce languageunderstanding systems that are are factual, efficient, and up-to-date. An importantÂ â€¦",
    "cited_by_count": 259.0
  },
  {
    "benchmark_name": "TRUE",
    "benchmark_paper": "TRUE: Re-evaluating Factual Consistency Evaluation",
    "title": "Does fine-tuning llms on new knowledge encourage hallucinations?",
    "authors": "Z Gekhman,G Yona,R Aharoni,M Eyalâ€¦Â - arXiv preprint arXivÂ â€¦, 2024",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2405.05904",
    "pdf_url": NaN,
    "abstract": "When large language models are aligned via supervised fine-tuning, they may encounternew factual information that was not acquired through pre-training. It is often conjectured thatÂ â€¦",
    "cited_by_count": 198.0
  },
  {
    "benchmark_name": "TRUE",
    "benchmark_paper": "TRUE: Re-evaluating Factual Consistency Evaluation",
    "title": "AlignScore: Evaluating factual consistency with a unified alignment function",
    "authors": "Y Zha,Y Yang,R Li,Z Hu- arXiv preprint arXiv:2305.16739, 2023",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2305.16739",
    "pdf_url": NaN,
    "abstract": "Many text generation applications require the generated text to be factually consistent withinput information. Automatic evaluation of factual consistency is challenging. Previous workÂ â€¦",
    "cited_by_count": 271.0
  },
  {
    "benchmark_name": "FIB",
    "benchmark_paper": "Evaluating the Factual Consistency of Large Language Models Through News Summarization",
    "title": "Survey on factuality in large language models: Knowledge, retrieval and domain-specificity",
    "authors": "C Wang,X Liu,Y Yue,X Tang,T Zhangâ€¦Â - arXiv preprint arXivÂ â€¦, 2023",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2310.07521",
    "pdf_url": NaN,
    "abstract": "This survey addresses the crucial issue of factuality in Large Language Models (LLMs). AsLLMs find applications across diverse domains, the reliability and accuracy of their outputsÂ â€¦",
    "cited_by_count": 281.0
  },
  {
    "benchmark_name": "FIB",
    "benchmark_paper": "Evaluating the Factual Consistency of Large Language Models Through News Summarization",
    "title": "Evaluating large language models: A comprehensive survey",
    "authors": "Z Guo,R Jin,C Liu,Y Huang,D Shi, L Yu, Y Liuâ€¦Â - arXiv preprint arXivÂ â€¦, 2023",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2310.19736",
    "pdf_url": NaN,
    "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities across a broadspectrum of tasks. They have attracted significant attention and been deployed in numerousÂ â€¦",
    "cited_by_count": 237.0
  },
  {
    "benchmark_name": "FIB",
    "benchmark_paper": "Evaluating the Factual Consistency of Large Language Models Through News Summarization",
    "title": "Trustllm: Trustworthiness in large language models",
    "authors": "Y Huang,L Sun,H Wang, S Wu,Q Zhang, Y Liâ€¦Â - arXiv preprint arXivÂ â€¦, 2024",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2401.05561",
    "pdf_url": NaN,
    "abstract": "Large language models (LLMs), exemplified by ChatGPT, have gained considerableattention for their excellent natural language processing capabilities. Nonetheless, theseÂ â€¦",
    "cited_by_count": 487.0
  },
  {
    "benchmark_name": "FIB",
    "benchmark_paper": "Evaluating the Factual Consistency of Large Language Models Through News Summarization",
    "title": "Cognitive mirage: A review of hallucinations in large language models",
    "authors": "H Ye,T Liu,A Zhang, W Hua,W Jia- arXiv preprint arXiv:2309.06794, 2023",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2309.06794",
    "pdf_url": NaN,
    "abstract": "As large language models continue to develop in the field of AI, text generation systems aresusceptible to a worrisome phenomenon known as hallucination. In this study, weÂ â€¦",
    "cited_by_count": 218.0
  },
  {
    "benchmark_name": "FIB",
    "benchmark_paper": "Evaluating the Factual Consistency of Large Language Models Through News Summarization",
    "title": "A comprehensive survey on process-oriented automatic text summarization with exploration of llm-based methods",
    "authors": "Y Zhang, H Jin, D Meng, J Wang, J TanÂ - arXiv preprint arXiv:2403.02901, 2024",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2403.02901",
    "pdf_url": NaN,
    "abstract": "Automatic Text Summarization (ATS), utilizing Natural Language Processing (NLP)algorithms, aims to create concise and accurate summaries, thereby significantly reducingÂ â€¦",
    "cited_by_count": 223.0
  },
  {
    "benchmark_name": "FIB",
    "benchmark_paper": "Evaluating the Factual Consistency of Large Language Models Through News Summarization",
    "title": "Position: Trustllm: Trustworthiness in large language models",
    "authors": "Y Huang,L Sun,H Wang, S Wuâ€¦Â - InternationalÂ â€¦, 2024",
    "publication": "proceedings.mlr.press",
    "year": NaN,
    "url": "http://proceedings.mlr.press/v235/huang24x.html",
    "pdf_url": NaN,
    "abstract": "Large language models (LLMs) have gained considerable attention for their excellentnatural language processing capabilities. Nonetheless, these LLMs present manyÂ â€¦",
    "cited_by_count": 88.0
  },
  {
    "benchmark_name": "FIB",
    "benchmark_paper": "Evaluating the Factual Consistency of Large Language Models Through News Summarization",
    "title": "A systematic survey of text summarization: From statistical methods to large language models",
    "authors": "H Zhang,PS Yu,J Zhang- ACM Computing Surveys, 2025",
    "publication": "dl.acm.org",
    "year": NaN,
    "url": "https://dl.acm.org/doi/abs/10.1145/3731445",
    "pdf_url": NaN,
    "abstract": "Text summarization research has undergone several significant transformations with theadvent of deep neural networks, pre-trained language models (PLMs), and recent largeÂ â€¦",
    "cited_by_count": 113.0
  },
  {
    "benchmark_name": "FIB",
    "benchmark_paper": "Evaluating the Factual Consistency of Large Language Models Through News Summarization",
    "title": "Lm vs lm: Detecting factual errors via cross examination",
    "authors": "R Cohen, M Hamri,M Geva,A Globerson- arXiv preprint arXivÂ â€¦, 2023",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2305.13281",
    "pdf_url": NaN,
    "abstract": "A prominent weakness of modern language models (LMs) is their tendency to generatefactually incorrect text, which hinders their usability. A natural question is whether suchÂ â€¦",
    "cited_by_count": 180.0
  },
  {
    "benchmark_name": "FIB",
    "benchmark_paper": "Evaluating the Factual Consistency of Large Language Models Through News Summarization",
    "title": "Lave: Llm-powered agent assistance and language augmentation for video editing",
    "authors": "B Wang,Y Li, Z Lv,H Xia,Y Xu, R SodhiÂ - Proceedings of the 29thÂ â€¦, 2024",
    "publication": "dl.acm.org",
    "year": NaN,
    "url": "https://dl.acm.org/doi/abs/10.1145/3640543.3645143",
    "pdf_url": NaN,
    "abstract": "Video creation has become increasingly popular, yet the expertise and effort required forediting often pose barriers to beginners. In this paper, we explore the integration of largeÂ â€¦",
    "cited_by_count": 86.0
  },
  {
    "benchmark_name": "FIB",
    "benchmark_paper": "Evaluating the Factual Consistency of Large Language Models Through News Summarization",
    "title": "Generating benchmarks for factuality evaluation of language models",
    "authors": "D Muhlgay,O Ram,I Magar,Y Levine, N Ratnerâ€¦Â - arXiv preprint arXivÂ â€¦, 2023",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2307.06908",
    "pdf_url": NaN,
    "abstract": "Before deploying a language model (LM) within a given domain, it is important to measureits tendency to generate factually incorrect information in that domain. Existing methods forÂ â€¦",
    "cited_by_count": 122.0
  },
  {
    "benchmark_name": "FEVER",
    "benchmark_paper": "FEVER: a Large-scale Dataset for Fact Extraction and VERification",
    "title": "A comprehensive overview of large language models",
    "authors": "H Naveed,AU Khan,S Qiu,M Saqib,S Anwarâ€¦Â - ACM Transactions onÂ â€¦, 2025",
    "publication": "dl.acm.org",
    "year": NaN,
    "url": "https://dl.acm.org/doi/abs/10.1145/3744746",
    "pdf_url": NaN,
    "abstract": "Large Language Models (LLMs) have recently demonstrated remarkable capabilities innatural language processing tasks and beyond. This success of LLMs has led to a largeÂ â€¦",
    "cited_by_count": 1815.0
  },
  {
    "benchmark_name": "FEVER",
    "benchmark_paper": "FEVER: a Large-scale Dataset for Fact Extraction and VERification",
    "title": "Survey of hallucination in natural language generation",
    "authors": "Z Ji,N Lee,R Frieske,T Yu,D Su,Y Xu,E Ishiiâ€¦Â - ACM computingÂ â€¦, 2023",
    "publication": "dl.acm.org",
    "year": NaN,
    "url": "https://dl.acm.org/doi/abs/10.1145/3571730",
    "pdf_url": NaN,
    "abstract": "Natural Language Generation (NLG) has improved exponentially in recent years thanks tothe development of sequence-to-sequence deep learning technologies such as TransformerÂ â€¦",
    "cited_by_count": 5570.0
  },
  {
    "benchmark_name": "FEVER",
    "benchmark_paper": "FEVER: a Large-scale Dataset for Fact Extraction and VERification",
    "title": "Retrieval-augmented generation for large language models: A survey",
    "authors": "Y Gao, Y Xiong, X Gao, K Jia, J Pan,Y Biâ€¦Â - arXiv preprint arXivÂ â€¦, 2023",
    "publication": "simg.baai.ac.cn",
    "year": NaN,
    "url": "https://simg.baai.ac.cn/paperfile/25a43194-c74c-4cd3-b60f-0a1f27f8b8af.pdf",
    "pdf_url": NaN,
    "abstract": "Large language models (LLMs) demonstrate powerful capabilities, but they still facechallenges in practical applications, such as hallucinations, slow knowledge updates, andÂ â€¦",
    "cited_by_count": 3542.0
  },
  {
    "benchmark_name": "FEVER",
    "benchmark_paper": "FEVER: a Large-scale Dataset for Fact Extraction and VERification",
    "title": "React: Synergizing reasoning and acting in language models",
    "authors": "S Yao,J Zhao,D Yu,N Du,I Shafranâ€¦Â - The eleventhÂ â€¦, 2022",
    "publication": "openreview.net",
    "year": NaN,
    "url": "https://openreview.net/forum?id=WE_vluYUL-X",
    "pdf_url": NaN,
    "abstract": "While large language models (LLMs) have demonstrated impressive capabilities acrosstasks in language understanding and interactive decision making, their abilities forÂ â€¦",
    "cited_by_count": 5155.0
  },
  {
    "benchmark_name": "FEVER",
    "benchmark_paper": "FEVER: a Large-scale Dataset for Fact Extraction and VERification",
    "title": "Factscore: Fine-grained atomic evaluation of factual precision in long form text generation",
    "authors": "S Min,K Krishna,X Lyu,M Lewis,W Yihâ€¦Â - arXiv preprint arXivÂ â€¦, 2023",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2305.14251",
    "pdf_url": NaN,
    "abstract": "Evaluating the factuality of long-form text generated by large language models (LMs) is non-trivial because (1) generations often contain a mixture of supported and unsupported piecesÂ â€¦",
    "cited_by_count": 875.0
  },
  {
    "benchmark_name": "FEVER",
    "benchmark_paper": "FEVER: a Large-scale Dataset for Fact Extraction and VERification",
    "title": "Beyond the imitation game: Quantifying and extrapolating the capabilities of language models",
    "authors": "A Srivastava, A Rastogi, A Rao,AAM Shoebâ€¦Â - â€¦Â on machine learningÂ â€¦, 2023",
    "publication": "openreview.net",
    "year": NaN,
    "url": "https://openreview.net/forum?id=uyTL5Bvosj&nesting=2&sort=date-desc",
    "pdf_url": NaN,
    "abstract": "Language models demonstrate both quantitative improvement and new qualitativecapabilities with increasing scale. Despite their potentially transformative impact, these newÂ â€¦",
    "cited_by_count": 2020.0
  },
  {
    "benchmark_name": "FEVER",
    "benchmark_paper": "FEVER: a Large-scale Dataset for Fact Extraction and VERification",
    "title": "Improving text embeddings with large language models",
    "authors": "L Wang,N Yang,X Huang,L Yangâ€¦Â - arXiv preprint arXivÂ â€¦, 2023",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2401.00368",
    "pdf_url": NaN,
    "abstract": "In this paper, we introduce a novel and simple method for obtaining high-quality textembeddings using only synthetic data and less than 1k training steps. Unlike existingÂ â€¦",
    "cited_by_count": 579.0
  },
  {
    "benchmark_name": "FEVER",
    "benchmark_paper": "FEVER: a Large-scale Dataset for Fact Extraction and VERification",
    "title": "Unleashing the potential of prompt engineering in large language models: a comprehensive review",
    "authors": "B Chen,Z Zhang,N LangrenÃ©,S Zhu- arXiv preprint arXiv:2310.14735, 2023",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2310.14735",
    "pdf_url": NaN,
    "abstract": "This comprehensive review delves into the pivotal role of prompt engineering in unleashingthe capabilities of Large Language Models (LLMs). The development of Artificial IntelligenceÂ â€¦",
    "cited_by_count": 568.0
  },
  {
    "benchmark_name": "FEVER",
    "benchmark_paper": "FEVER: a Large-scale Dataset for Fact Extraction and VERification",
    "title": "Truthfulqa: Measuring how models mimic human falsehoods",
    "authors": "S Lin,J Hilton,O Evans- arXiv preprint arXiv:2109.07958, 2021",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2109.07958",
    "pdf_url": NaN,
    "abstract": "We propose a benchmark to measure whether a language model is truthful in generatinganswers to questions. The benchmark comprises 817 questions that span 38 categoriesÂ â€¦",
    "cited_by_count": 2553.0
  },
  {
    "benchmark_name": "FEVER",
    "benchmark_paper": "FEVER: a Large-scale Dataset for Fact Extraction and VERification",
    "title": "Text embeddings by weakly-supervised contrastive pre-training",
    "authors": "L Wang,N Yang,X Huang,B Jiao,L Yangâ€¦Â - arXiv preprint arXivÂ â€¦, 2022",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2212.03533",
    "pdf_url": NaN,
    "abstract": "This paper presents E5, a family of state-of-the-art text embeddings that transfer well to awide range of tasks. The model is trained in a contrastive manner with weak supervisionÂ â€¦",
    "cited_by_count": 876.0
  },
  {
    "benchmark_name": "HalluLens",
    "benchmark_paper": "HalluLens: LLM Hallucination Benchmark",
    "title": "Why language models hallucinate",
    "authors": "AT Kalai,O Nachum,SS Vempala,E Zhang- arXiv preprint arXivÂ â€¦, 2025",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2509.04664",
    "pdf_url": NaN,
    "abstract": "Like students facing hard exam questions, large language models sometimes guess whenuncertain, producing plausible yet incorrect statements instead of admitting uncertaintyÂ â€¦",
    "cited_by_count": 41.0
  },
  {
    "benchmark_name": "HalluLens",
    "benchmark_paper": "HalluLens: LLM Hallucination Benchmark",
    "title": "Towards holistic evaluation of large audio-language models: A comprehensive survey",
    "authors": "CK Yang, NS Ho,H Lee- arXiv preprint arXiv:2505.15957, 2025",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2505.15957",
    "pdf_url": NaN,
    "abstract": "With advancements in large audio-language models (LALMs), which enhance largelanguage models (LLMs) with auditory capabilities, these models are expected toÂ â€¦",
    "cited_by_count": 19.0
  },
  {
    "benchmark_name": "HalluLens",
    "benchmark_paper": "HalluLens: LLM Hallucination Benchmark",
    "title": "Never compromise to vulnerabilities: A comprehensive survey on ai governance",
    "authors": "Y Jiang,J Zhao, Y Yuan,T Zhang,Y Huangâ€¦Â - arXiv preprint arXivÂ â€¦, 2025",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2508.08789",
    "pdf_url": NaN,
    "abstract": "The rapid advancement of AI has expanded its capabilities across domains, yet introducedcritical technical vulnerabilities, such as algorithmic bias and adversarial sensitivity, thatÂ â€¦",
    "cited_by_count": 2.0
  },
  {
    "benchmark_name": "HalluLens",
    "benchmark_paper": "HalluLens: LLM Hallucination Benchmark",
    "title": "A comprehensive taxonomy of hallucinations in large language models",
    "authors": "M Cossio- arXiv preprint arXiv:2508.01781, 2025",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2508.01781",
    "pdf_url": NaN,
    "abstract": "Large language models (LLMs) have revolutionized natural language processing, yet theirpropensity for hallucination, generating plausible but factually incorrect or fabricated contentÂ â€¦",
    "cited_by_count": 9.0
  },
  {
    "benchmark_name": "HalluLens",
    "benchmark_paper": "HalluLens: LLM Hallucination Benchmark",
    "title": "Stress testing deliberative alignment for anti-scheming training",
    "authors": "B Schoen, E Nitishinskaya,M Balesniâ€¦Â - arXiv preprint arXivÂ â€¦, 2025",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2509.15541",
    "pdf_url": NaN,
    "abstract": "Highly capable AI systems could secretly pursue misaligned goals--what we call\" scheming\".Because a scheming AI would deliberately try to hide its misaligned goals and actionsÂ â€¦",
    "cited_by_count": 4.0
  },
  {
    "benchmark_name": "HalluLens",
    "benchmark_paper": "HalluLens: LLM Hallucination Benchmark",
    "title": "The hallucination tax of reinforcement finetuning",
    "authors": "L Song,T Shi,J Zhao- arXiv preprint arXiv:2505.13988, 2025",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2505.13988",
    "pdf_url": NaN,
    "abstract": "Reinforcement finetuning (RFT) has become a standard approach for enhancing thereasoning capabilities of large language models (LLMs). However, its impact on modelÂ â€¦",
    "cited_by_count": 8.0
  },
  {
    "benchmark_name": "HalluLens",
    "benchmark_paper": "HalluLens: LLM Hallucination Benchmark",
    "title": "AbstentionBench: Reasoning LLMs Fail on Unanswerable Questions",
    "authors": "P Kirichenko,M Ibrahim,K Chaudhuriâ€¦Â - arXiv preprint arXivÂ â€¦, 2025",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2506.09038",
    "pdf_url": NaN,
    "abstract": "For Large Language Models (LLMs) to be reliably deployed in both everyday and high-stakes domains, knowing when not to answer is equally critical as answering correctly. RealÂ â€¦",
    "cited_by_count": 13.0
  },
  {
    "benchmark_name": "HalluLens",
    "benchmark_paper": "HalluLens: LLM Hallucination Benchmark",
    "title": "The risks of ai-generated, hyper-personalized digital advertisements",
    "authors": "A LeBrun- Philosophy & Technology, 2025",
    "publication": "Springer",
    "year": NaN,
    "url": "https://link.springer.com/article/10.1007/s13347-025-00935-z",
    "pdf_url": NaN,
    "abstract": "Generative AI is set to transform digital advertising, which remains the revenue juggernat formuch of the internet. In the current model, advertisers use personal data to target users withÂ â€¦",
    "cited_by_count": 2.0
  },
  {
    "benchmark_name": "HalluLens",
    "benchmark_paper": "HalluLens: LLM Hallucination Benchmark",
    "title": "MIRAGE-Bench: LLM Agent is Hallucinating and Where to Find Them",
    "authors": "W Zhang,Y Sun, P Huang, J Pu, H Linâ€¦Â - arXiv preprint arXivÂ â€¦, 2025",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2507.21017",
    "pdf_url": NaN,
    "abstract": "Hallucinations pose critical risks for large language model (LLM)-based agents, oftenmanifesting as hallucinative actions resulting from fabricated or misinterpreted informationÂ â€¦",
    "cited_by_count": 1.0
  },
  {
    "benchmark_name": "HalluLens",
    "benchmark_paper": "HalluLens: LLM Hallucination Benchmark",
    "title": "Introducing the Swiss Food Knowledge Graph: AI for Context-Aware Nutrition Recommendation",
    "authors": "LA Rahman,I Papathanail,S Mougiakakou- arXiv preprint arXivÂ â€¦, 2025",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2507.10156",
    "pdf_url": NaN,
    "abstract": "AI has driven significant progress in the nutrition field, especially through multimedia-basedautomatic dietary assessment. However, existing automatic dietary assessment systemsÂ â€¦",
    "cited_by_count": 1.0
  },
  {
    "benchmark_name": "FreshQA",
    "benchmark_paper": "FreshLLMs: Refreshing Large Language Models with Search Engine Augmentation",
    "title": "A survey on evaluation of large language models",
    "authors": "Y Chang,X Wang,J Wang,Y Wu,L Yangâ€¦Â - ACM transactions onÂ â€¦, 2024",
    "publication": "dl.acm.org",
    "year": NaN,
    "url": "https://dl.acm.org/doi/abs/10.1145/3641289",
    "pdf_url": NaN,
    "abstract": "Large language models (LLMs) are gaining increasing popularity in both academia andindustry, owing to their unprecedented performance in various applications. As LLMsÂ â€¦",
    "cited_by_count": 4405.0
  },
  {
    "benchmark_name": "FreshQA",
    "benchmark_paper": "FreshLLMs: Refreshing Large Language Models with Search Engine Augmentation",
    "title": "Combating misinformation in the age of llms: Opportunities and challenges",
    "authors": "C Chen,K Shu- AI Magazine, 2024",
    "publication": "Wiley Online Library",
    "year": NaN,
    "url": "https://onlinelibrary.wiley.com/doi/abs/10.1002/aaai.12188",
    "pdf_url": NaN,
    "abstract": "Misinformation such as fake news and rumors is a serious threat for information ecosystemsand public trust. The emergence of large language models (LLMs) has great potential toÂ â€¦",
    "cited_by_count": 279.0
  },
  {
    "benchmark_name": "FreshQA",
    "benchmark_paper": "FreshLLMs: Refreshing Large Language Models with Search Engine Augmentation",
    "title": "A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions",
    "authors": "L Huang,W Yu,W Ma,W Zhong,Z Fengâ€¦Â - ACM Transactions onÂ â€¦, 2025",
    "publication": "dl.acm.org",
    "year": NaN,
    "url": "https://dl.acm.org/doi/abs/10.1145/3703155",
    "pdf_url": NaN,
    "abstract": "The emergence of large language models (LLMs) has marked a significant breakthrough innatural language processing (NLP), fueling a paradigm shift in information acquisitionÂ â€¦",
    "cited_by_count": 3037.0
  },
  {
    "benchmark_name": "FreshQA",
    "benchmark_paper": "FreshLLMs: Refreshing Large Language Models with Search Engine Augmentation",
    "title": "Hallucination is inevitable: An innate limitation of large language models",
    "authors": "Z Xu,S Jain,M Kankanhalli- arXiv preprint arXiv:2401.11817, 2024",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2401.11817",
    "pdf_url": NaN,
    "abstract": "Hallucination has been widely recognized to be a significant drawback for large languagemodels (LLMs). There have been many works that attempt to reduce the extent ofÂ â€¦",
    "cited_by_count": 723.0
  },
  {
    "benchmark_name": "FreshQA",
    "benchmark_paper": "FreshLLMs: Refreshing Large Language Models with Search Engine Augmentation",
    "title": "Trustllm: Trustworthiness in large language models",
    "authors": "Y Huang,L Sun,H Wang, S Wu,Q Zhang, Y Liâ€¦Â - arXiv preprint arXivÂ â€¦, 2024",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2401.05561",
    "pdf_url": NaN,
    "abstract": "Large language models (LLMs), exemplified by ChatGPT, have gained considerableattention for their excellent natural language processing capabilities. Nonetheless, theseÂ â€¦",
    "cited_by_count": 487.0
  },
  {
    "benchmark_name": "FreshQA",
    "benchmark_paper": "FreshLLMs: Refreshing Large Language Models with Search Engine Augmentation",
    "title": "Large language models for information retrieval: A survey",
    "authors": "Y Zhu,H Yuan,S Wang,J Liu,W Liu,C Dengâ€¦Â - arXiv preprint arXivÂ â€¦, 2023",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2308.07107",
    "pdf_url": NaN,
    "abstract": "As a primary means of information acquisition, information retrieval (IR) systems, such assearch engines, have integrated themselves into our daily lives. These systems also serveÂ â€¦",
    "cited_by_count": 585.0
  },
  {
    "benchmark_name": "FreshQA",
    "benchmark_paper": "FreshLLMs: Refreshing Large Language Models with Search Engine Augmentation",
    "title": "Raft: Adapting language model to domain specific rag",
    "authors": "T Zhang,SG Patil,N Jain,S Shen,M Zahariaâ€¦Â - arXiv preprint arXivÂ â€¦, 2024",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2403.10131",
    "pdf_url": NaN,
    "abstract": "Pretraining Large Language Models (LLMs) on large corpora of textual data is now astandard paradigm. When using these LLMs for many downstream applications, it isÂ â€¦",
    "cited_by_count": 296.0
  },
  {
    "benchmark_name": "FreshQA",
    "benchmark_paper": "FreshLLMs: Refreshing Large Language Models with Search Engine Augmentation",
    "title": "Long-form factuality in large language models",
    "authors": "J Wei,C Yang,X Song,Y Lu,N Huâ€¦Â - Advances inÂ â€¦, 2024",
    "publication": "proceedings.neurips.cc",
    "year": NaN,
    "url": "https://proceedings.neurips.cc/paper_files/paper/2024/hash/937ae0e83eb08d2cb8627fe1def8c751-Abstract-Conference.html",
    "pdf_url": NaN,
    "abstract": "Large language models (LLMs) often generate content that contains factual errors whenresponding to fact-seeking prompts on open-ended topics. To benchmark a model's longÂ â€¦",
    "cited_by_count": 147.0
  },
  {
    "benchmark_name": "FreshQA",
    "benchmark_paper": "FreshLLMs: Refreshing Large Language Models with Search Engine Augmentation",
    "title": "Survey on factuality in large language models: Knowledge, retrieval and domain-specificity",
    "authors": "C Wang,X Liu,Y Yue,X Tang,T Zhangâ€¦Â - arXiv preprint arXivÂ â€¦, 2023",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2310.07521",
    "pdf_url": NaN,
    "abstract": "This survey addresses the crucial issue of factuality in Large Language Models (LLMs). AsLLMs find applications across diverse domains, the reliability and accuracy of their outputsÂ â€¦",
    "cited_by_count": 281.0
  },
  {
    "benchmark_name": "FreshQA",
    "benchmark_paper": "FreshLLMs: Refreshing Large Language Models with Search Engine Augmentation",
    "title": "Knowledge conflicts for llms: A survey",
    "authors": "R Xu,Z Qi,Z Guo,C Wang,H Wang,Y Zhangâ€¦Â - arXiv preprint arXivÂ â€¦, 2024",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2403.08319",
    "pdf_url": NaN,
    "abstract": "This survey provides an in-depth analysis of knowledge conflicts for large language models(LLMs), highlighting the complex challenges they encounter when blending contextual andÂ â€¦",
    "cited_by_count": 188.0
  },
  {
    "benchmark_name": "ETHICS",
    "benchmark_paper": "Aligning AI With Shared Human Values",
    "title": "A survey on evaluation of large language models",
    "authors": "Y Chang,X Wang,J Wang,Y Wu,L Yangâ€¦Â - ACM transactions onÂ â€¦, 2024",
    "publication": "dl.acm.org",
    "year": NaN,
    "url": "https://dl.acm.org/doi/abs/10.1145/3641289",
    "pdf_url": NaN,
    "abstract": "Large language models (LLMs) are gaining increasing popularity in both academia andindustry, owing to their unprecedented performance in various applications. As LLMsÂ â€¦",
    "cited_by_count": 4405.0
  },
  {
    "benchmark_name": "ETHICS",
    "benchmark_paper": "Aligning AI With Shared Human Values",
    "title": "Challenges and applications of large language models",
    "authors": "J Kaddour,J Harris,M Mozes,H Bradleyâ€¦Â - arXiv preprint arXivÂ â€¦, 2023",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2307.10169",
    "pdf_url": NaN,
    "abstract": "Large Language Models (LLMs) went from non-existent to ubiquitous in the machinelearning discourse within a few years. Due to the fast pace of the field, it is difficult to identifyÂ â€¦",
    "cited_by_count": 768.0
  },
  {
    "benchmark_name": "ETHICS",
    "benchmark_paper": "Aligning AI With Shared Human Values",
    "title": "Universal and transferable adversarial attacks on aligned language models",
    "authors": "A Zou,Z Wang,N Carlini,M Nasr,JZ Kolterâ€¦Â - arXiv preprint arXivÂ â€¦, 2023",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2307.15043",
    "pdf_url": NaN,
    "abstract": "Because\" out-of-the-box\" large language models are capable of generating a great deal ofobjectionable content, recent work has focused on aligning these models in an attempt toÂ â€¦",
    "cited_by_count": 2181.0
  },
  {
    "benchmark_name": "ETHICS",
    "benchmark_paper": "Aligning AI With Shared Human Values",
    "title": "Connecting the dots in trustworthy Artificial Intelligence: From AI principles, ethics, and key requirements to responsible AI systems and regulation",
    "authors": "N DÃ­az-RodrÃ­guez,J Del Ser,M Coeckelberghâ€¦Â - InformationÂ â€¦, 2023",
    "publication": "Elsevier",
    "year": NaN,
    "url": "https://www.sciencedirect.com/science/article/pii/S1566253523002129",
    "pdf_url": NaN,
    "abstract": "Abstract Trustworthy Artificial Intelligence (AI) is based on seven technical requirementssustained over three main pillars that should be met throughout the system's entire life cycleÂ â€¦",
    "cited_by_count": 851.0
  },
  {
    "benchmark_name": "ETHICS",
    "benchmark_paper": "Aligning AI With Shared Human Values",
    "title": "Can large language models be an alternative to human evaluations?",
    "authors": "CH Chiang,H Lee- arXiv preprint arXiv:2305.01937, 2023",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2305.01937",
    "pdf_url": NaN,
    "abstract": "Human evaluation is indispensable and inevitable for assessing the quality of textsgenerated by machine learning models or written by humans. However, human evaluation isÂ â€¦",
    "cited_by_count": 848.0
  },
  {
    "benchmark_name": "ETHICS",
    "benchmark_paper": "Aligning AI With Shared Human Values",
    "title": "Beyond the imitation game: Quantifying and extrapolating the capabilities of language models",
    "authors": "A Srivastava, A Rastogi, A Rao,AAM Shoebâ€¦Â - â€¦Â on machine learningÂ â€¦, 2023",
    "publication": "openreview.net",
    "year": NaN,
    "url": "https://openreview.net/forum?id=uyTL5Bvosj&nesting=2&sort=date-desc",
    "pdf_url": NaN,
    "abstract": "Language models demonstrate both quantitative improvement and new qualitativecapabilities with increasing scale. Despite their potentially transformative impact, these newÂ â€¦",
    "cited_by_count": 2020.0
  },
  {
    "benchmark_name": "ETHICS",
    "benchmark_paper": "Aligning AI With Shared Human Values",
    "title": "DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models.",
    "authors": "B Wang,W Chen,H Pei,C Xie,M Kang,C Zhang,C Xuâ€¦Â - NeurIPS, 2023",
    "publication": "blogs.qub.ac.uk",
    "year": NaN,
    "url": "https://blogs.qub.ac.uk/wp-content/uploads/sites/7/2024/01/A-comprehensive-Assessment-of-Trustworthiness-in-GPT-Models.pdf",
    "pdf_url": NaN,
    "abstract": "Abstract Generative Pre-trained Transformer (GPT) models have exhibited exciting progressin their capabilities, capturing the interest of practitioners and the public alike. Yet, while theÂ â€¦",
    "cited_by_count": 625.0
  },
  {
    "benchmark_name": "ETHICS",
    "benchmark_paper": "Aligning AI With Shared Human Values",
    "title": "Tree of attacks: Jailbreaking black-box llms automatically",
    "authors": "A Mehrotra,M Zampetakisâ€¦Â - Advances inÂ â€¦, 2024",
    "publication": "proceedings.neurips.cc",
    "year": NaN,
    "url": "https://proceedings.neurips.cc/paper_files/paper/2024/hash/70702e8cbb4890b4a467b984ae59828a-Abstract-Conference.html",
    "pdf_url": NaN,
    "abstract": "Abstract While Large Language Models (LLMs) display versatile functionality, they continueto generate harmful, biased, and toxic content, as demonstrated by the prevalence of humanÂ â€¦",
    "cited_by_count": 424.0
  },
  {
    "benchmark_name": "ETHICS",
    "benchmark_paper": "Aligning AI With Shared Human Values",
    "title": "Scaling and evaluating sparse autoencoders",
    "authors": "L Gao,TD la Tour, H Tillman,G Goh, R Trollâ€¦Â - arXiv preprint arXivÂ â€¦, 2024",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2406.04093",
    "pdf_url": NaN,
    "abstract": "Sparse autoencoders provide a promising unsupervised approach for extractinginterpretable features from a language model by reconstructing activations from a sparseÂ â€¦",
    "cited_by_count": 303.0
  },
  {
    "benchmark_name": "ETHICS",
    "benchmark_paper": "Aligning AI With Shared Human Values",
    "title": "Trustllm: Trustworthiness in large language models",
    "authors": "Y Huang,L Sun,H Wang, S Wu,Q Zhang, Y Liâ€¦Â - arXiv preprint arXivÂ â€¦, 2024",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2401.05561",
    "pdf_url": NaN,
    "abstract": "Large language models (LLMs), exemplified by ChatGPT, have gained considerableattention for their excellent natural language processing capabilities. Nonetheless, theseÂ â€¦",
    "cited_by_count": 487.0
  },
  {
    "benchmark_name": "Moral Stories",
    "benchmark_paper": "Moral Stories: Situated Reasoning about Norms Intents Actions and their Consequences",
    "title": "Ai alignment: A comprehensive survey",
    "authors": "J Ji,T Qiu,B Chen, B Zhang,H Lou, K Wangâ€¦Â - arXiv preprint arXivÂ â€¦, 2023",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2310.19852",
    "pdf_url": NaN,
    "abstract": "AI alignment aims to make AI systems behave in line with human intentions and values. AsAI systems grow more capable, so do risks from misalignment. To provide a comprehensiveÂ â€¦",
    "cited_by_count": 459.0
  },
  {
    "benchmark_name": "Moral Stories",
    "benchmark_paper": "Moral Stories: Situated Reasoning about Norms Intents Actions and their Consequences",
    "title": "Natural language reasoning, a survey",
    "authors": "F Yu,H Zhang,P Tiwari,B Wang- ACM Computing Surveys, 2024",
    "publication": "dl.acm.org",
    "year": NaN,
    "url": "https://dl.acm.org/doi/abs/10.1145/3664194",
    "pdf_url": NaN,
    "abstract": "This survey article proposes a clearer view of Natural Language Reasoning (NLR) in thefield of Natural Language Processing (NLP), both conceptually and practicallyÂ â€¦",
    "cited_by_count": 161.0
  },
  {
    "benchmark_name": "Moral Stories",
    "benchmark_paper": "Moral Stories: Situated Reasoning about Norms Intents Actions and their Consequences",
    "title": "DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models.",
    "authors": "B Wang,W Chen,H Pei,C Xie,M Kang,C Zhang,C Xuâ€¦Â - NeurIPS, 2023",
    "publication": "blogs.qub.ac.uk",
    "year": NaN,
    "url": "https://blogs.qub.ac.uk/wp-content/uploads/sites/7/2024/01/A-comprehensive-Assessment-of-Trustworthiness-in-GPT-Models.pdf",
    "pdf_url": NaN,
    "abstract": "Abstract Generative Pre-trained Transformer (GPT) models have exhibited exciting progressin their capabilities, capturing the interest of practitioners and the public alike. Yet, while theÂ â€¦",
    "cited_by_count": 625.0
  },
  {
    "benchmark_name": "Moral Stories",
    "benchmark_paper": "Moral Stories: Situated Reasoning about Norms Intents Actions and their Consequences",
    "title": "From pretraining data to language models to downstream tasks: Tracking the trails of political biases leading to unfair NLP models",
    "authors": "S Feng,CY Park,Y Liu,Y Tsvetkov- arXiv preprint arXiv:2305.08283, 2023",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2305.08283",
    "pdf_url": NaN,
    "abstract": "Language models (LMs) are pretrained on diverse data sources, including news, discussionforums, books, and online encyclopedias. A significant portion of this data includes opinionsÂ â€¦",
    "cited_by_count": 373.0
  },
  {
    "benchmark_name": "Moral Stories",
    "benchmark_paper": "Moral Stories: Situated Reasoning about Norms Intents Actions and their Consequences",
    "title": "Evaluating the moral beliefs encoded in llms",
    "authors": "N Scherrer,C Shi,A Federâ€¦Â - Advances in NeuralÂ â€¦, 2023",
    "publication": "proceedings.neurips.cc",
    "year": NaN,
    "url": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/a2cf225ba392627529efef14dc857e22-Abstract-Conference.html",
    "pdf_url": NaN,
    "abstract": "This paper presents a case study on the design, administration, post-processing, andevaluation of surveys on large language models (LLMs). It comprises two components:(1) AÂ â€¦",
    "cited_by_count": 217.0
  },
  {
    "benchmark_name": "Moral Stories",
    "benchmark_paper": "Moral Stories: Situated Reasoning about Norms Intents Actions and their Consequences",
    "title": "Refiner: Reasoning feedback on intermediate representations",
    "authors": "D Paul,M Ismayilzada,M Peyrard,B Borgesâ€¦Â - arXiv preprint arXivÂ â€¦, 2023",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2304.01904",
    "pdf_url": NaN,
    "abstract": "Language models (LMs) have recently shown remarkable performance on reasoning tasksby explicitly generating intermediate inferences, eg, chain-of-thought prompting. HoweverÂ â€¦",
    "cited_by_count": 242.0
  },
  {
    "benchmark_name": "Moral Stories",
    "benchmark_paper": "Moral Stories: Situated Reasoning about Norms Intents Actions and their Consequences",
    "title": "Training socially aligned language models in simulated human society",
    "authors": "R Liu,R Yang,C Jia,G Zhang,D Zhouâ€¦Â - arXiv preprint arXivÂ â€¦, 2023",
    "publication": "proceedings.iclr.cc",
    "year": NaN,
    "url": "https://proceedings.iclr.cc/paper_files/paper/2024/file/d763b4a2dde0ae7b77498516ce9f439e-Paper-Conference.pdf",
    "pdf_url": NaN,
    "abstract": "Social alignment in AI systems aims to ensure that these models behave according toestablished societal values. However, unlike humans, who derive consensus on valueÂ â€¦",
    "cited_by_count": 142.0
  },
  {
    "benchmark_name": "Moral Stories",
    "benchmark_paper": "Moral Stories: Situated Reasoning about Norms Intents Actions and their Consequences",
    "title": "When to make exceptions: Exploring language models as accounts of human moral judgment",
    "authors": "Z Jin,S Levine,F Gonzalez Adautoâ€¦Â - Advances in neuralÂ â€¦, 2022",
    "publication": "proceedings.neurips.cc",
    "year": NaN,
    "url": "https://proceedings.neurips.cc/paper_files/paper/2022/hash/b654d6150630a5ba5df7a55621390daf-Abstract-Conference.html",
    "pdf_url": NaN,
    "abstract": "AI systems are becoming increasingly intertwined with human life. In order to effectivelycollaborate with humans and ensure safety, AI systems need to be able to understandÂ â€¦",
    "cited_by_count": 135.0
  },
  {
    "benchmark_name": "Moral Stories",
    "benchmark_paper": "Moral Stories: Situated Reasoning about Norms Intents Actions and their Consequences",
    "title": "Knowledge of cultural moral norms in large language models",
    "authors": "A Ramezani,Y Xu- arXiv preprint arXiv:2306.01857, 2023",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2306.01857",
    "pdf_url": NaN,
    "abstract": "Moral norms vary across cultures. A recent line of work suggests that English large languagemodels contain human-like moral biases, but these studies typically do not examine moralÂ â€¦",
    "cited_by_count": 121.0
  },
  {
    "benchmark_name": "Moral Stories",
    "benchmark_paper": "Moral Stories: Situated Reasoning about Norms Intents Actions and their Consequences",
    "title": "Culturally aware and adapted nlp: A taxonomy and a survey of the state of the art",
    "authors": "CC Liu,I Gurevych,A Korhonen- Transactions of the Association forÂ â€¦, 2025",
    "publication": "direct.mit.edu",
    "year": NaN,
    "url": "https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00760/131587",
    "pdf_url": NaN,
    "abstract": "The surge of interest in culture in NLP has inspired much recent research, but a sharedunderstanding of â€œcultureâ€ remains unclear, making it difficult to evaluate progress in thisÂ â€¦",
    "cited_by_count": 48.0
  },
  {
    "benchmark_name": "Jiminy Cricket",
    "benchmark_paper": "What Would Jiminy Cricket Do? Towards Agents That Behave Morally",
    "title": "Safetyprompts: a systematic review of open datasets for evaluating and improving large language model safety",
    "authors": "P RÃ¶ttger,F Pernisi,B Vidgen,D Hovy- Proceedings of the AAAIÂ â€¦, 2025",
    "publication": "ojs.aaai.org",
    "year": NaN,
    "url": "https://ojs.aaai.org/index.php/AAAI/article/view/34975",
    "pdf_url": NaN,
    "abstract": "The last two years have seen a rapid growth in concerns around the safety of largelanguage models (LLMs). Researchers and practitioners have met these concerns byÂ â€¦",
    "cited_by_count": 52.0
  },
  {
    "benchmark_name": "Jiminy Cricket",
    "benchmark_paper": "What Would Jiminy Cricket Do? Towards Agents That Behave Morally",
    "title": "DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models.",
    "authors": "B Wang,W Chen,H Pei,C Xie,M Kang,C Zhang,C Xuâ€¦Â - NeurIPS, 2023",
    "publication": "blogs.qub.ac.uk",
    "year": NaN,
    "url": "https://blogs.qub.ac.uk/wp-content/uploads/sites/7/2024/01/A-comprehensive-Assessment-of-Trustworthiness-in-GPT-Models.pdf",
    "pdf_url": NaN,
    "abstract": "Abstract Generative Pre-trained Transformer (GPT) models have exhibited exciting progressin their capabilities, capturing the interest of practitioners and the public alike. Yet, while theÂ â€¦",
    "cited_by_count": 625.0
  },
  {
    "benchmark_name": "Jiminy Cricket",
    "benchmark_paper": "What Would Jiminy Cricket Do? Towards Agents That Behave Morally",
    "title": "Trustllm: Trustworthiness in large language models",
    "authors": "Y Huang,L Sun,H Wang, S Wu,Q Zhang, Y Liâ€¦Â - arXiv preprint arXivÂ â€¦, 2024",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2401.05561",
    "pdf_url": NaN,
    "abstract": "Large language models (LLMs), exemplified by ChatGPT, have gained considerableattention for their excellent natural language processing capabilities. Nonetheless, theseÂ â€¦",
    "cited_by_count": 487.0
  },
  {
    "benchmark_name": "Jiminy Cricket",
    "benchmark_paper": "What Would Jiminy Cricket Do? Towards Agents That Behave Morally",
    "title": "Representation engineering: A top-down approach to ai transparency",
    "authors": "A Zou,L Phan,S Chen,J Campbell,P Guoâ€¦Â - arXiv preprint arXivÂ â€¦, 2023",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2310.01405",
    "pdf_url": NaN,
    "abstract": "In this paper, we identify and characterize the emerging area of representation engineering(RepE), an approach to enhancing the transparency of AI systems that draws on insightsÂ â€¦",
    "cited_by_count": 567.0
  },
  {
    "benchmark_name": "Jiminy Cricket",
    "benchmark_paper": "What Would Jiminy Cricket Do? Towards Agents That Behave Morally",
    "title": "Do the rewards justify the means? measuring trade-offs between rewards and ethical behavior in the machiavelli benchmark",
    "authors": "A Pan,JS Chan,A Zou,N Li,S Basartâ€¦Â - InternationalÂ â€¦, 2023",
    "publication": "proceedings.mlr.press",
    "year": NaN,
    "url": "https://proceedings.mlr.press/v202/pan23a.html",
    "pdf_url": NaN,
    "abstract": "Artificial agents have traditionally been trained to maximize reward, which may incentivizepower-seeking and deception, analogous to how next-token prediction in language modelsÂ â€¦",
    "cited_by_count": 199.0
  },
  {
    "benchmark_name": "Jiminy Cricket",
    "benchmark_paper": "What Would Jiminy Cricket Do? Towards Agents That Behave Morally",
    "title": "Unsolved problems in ml safety",
    "authors": "D Hendrycks,N Carlini,J Schulmanâ€¦Â - arXiv preprint arXivÂ â€¦, 2021",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2109.13916",
    "pdf_url": NaN,
    "abstract": "Machine learning (ML) systems are rapidly increasing in size, are acquiring newcapabilities, and are increasingly deployed in high-stakes settings. As with other powerfulÂ â€¦",
    "cited_by_count": 459.0
  },
  {
    "benchmark_name": "Jiminy Cricket",
    "benchmark_paper": "What Would Jiminy Cricket Do? Towards Agents That Behave Morally",
    "title": "Position: Trustllm: Trustworthiness in large language models",
    "authors": "Y Huang,L Sun,H Wang, S Wuâ€¦Â - InternationalÂ â€¦, 2024",
    "publication": "proceedings.mlr.press",
    "year": NaN,
    "url": "http://proceedings.mlr.press/v235/huang24x.html",
    "pdf_url": NaN,
    "abstract": "Large language models (LLMs) have gained considerable attention for their excellentnatural language processing capabilities. Nonetheless, these LLMs present manyÂ â€¦",
    "cited_by_count": 88.0
  },
  {
    "benchmark_name": "Jiminy Cricket",
    "benchmark_paper": "What Would Jiminy Cricket Do? Towards Agents That Behave Morally",
    "title": "Open-Ethical AI: Advancements in Open-Source Human-Centric Neural Language Models",
    "authors": "S Sicari,JF Cevallos M,A Rizzardiâ€¦Â - ACM ComputingÂ â€¦, 2024",
    "publication": "dl.acm.org",
    "year": NaN,
    "url": "https://dl.acm.org/doi/abs/10.1145/3703454",
    "pdf_url": NaN,
    "abstract": "This survey summarises the most recent methods for building and assessing helpful, honest,and harmless neural language models, considering small, medium, and large-size modelsÂ â€¦",
    "cited_by_count": 9.0
  },
  {
    "benchmark_name": "Jiminy Cricket",
    "benchmark_paper": "What Would Jiminy Cricket Do? Towards Agents That Behave Morally",
    "title": "Multi-turn reinforcement learning with preference human feedback",
    "authors": "L Shani,A Rosenberg,A Casselâ€¦Â - Advances inÂ â€¦, 2024",
    "publication": "proceedings.neurips.cc",
    "year": NaN,
    "url": "https://proceedings.neurips.cc/paper_files/paper/2024/hash/d77a7b289361abff82bdd2fb537ae152-Abstract-Conference.html",
    "pdf_url": NaN,
    "abstract": "Abstract Reinforcement Learning from Human Feedback (RLHF) has become the standardapproach for aligning Large Language Models (LLMs) with human preferences, allowingÂ â€¦",
    "cited_by_count": 51.0
  },
  {
    "benchmark_name": "Jiminy Cricket",
    "benchmark_paper": "What Would Jiminy Cricket Do? Towards Agents That Behave Morally",
    "title": "Moca: Measuring human-language model alignment on causal and moral judgment tasks",
    "authors": "A Nie,Y Zhang,AS Amdekar,C Piechâ€¦Â - Advances inÂ â€¦, 2023",
    "publication": "proceedings.neurips.cc",
    "year": NaN,
    "url": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/f751c6f8bfb52c60f43942896fe65904-Abstract-Conference.html",
    "pdf_url": NaN,
    "abstract": "Human commonsense understanding of the physical and social world is organized aroundintuitive theories. These theories support making causal and moral judgments. WhenÂ â€¦",
    "cited_by_count": 69.0
  },
  {
    "benchmark_name": "SCRUPLES",
    "benchmark_paper": "Scruples: A Corpus of Community Ethical Judgments on 32000 Real-Life Anecdotes",
    "title": "Evaluating large language models: A comprehensive survey",
    "authors": "Z Guo,R Jin,C Liu,Y Huang,D Shi, L Yu, Y Liuâ€¦Â - arXiv preprint arXivÂ â€¦, 2023",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2310.19736",
    "pdf_url": NaN,
    "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities across a broadspectrum of tasks. They have attracted significant attention and been deployed in numerousÂ â€¦",
    "cited_by_count": 237.0
  },
  {
    "benchmark_name": "SCRUPLES",
    "benchmark_paper": "Scruples: A Corpus of Community Ethical Judgments on 32000 Real-Life Anecdotes",
    "title": "Navigating llm ethics: Advancements, challenges, and future directions",
    "authors": "J Jiao,S Afroogh,Y Xu,C Phillips- AI and Ethics, 2025",
    "publication": "Springer",
    "year": NaN,
    "url": "https://link.springer.com/article/10.1007/s43681-025-00814-5",
    "pdf_url": NaN,
    "abstract": "This study addresses ethical issues surrounding Large Language Models (LLMs) within thefield of artificial intelligence. It explores the common ethical challenges posed by both LLMsÂ â€¦",
    "cited_by_count": 91.0
  },
  {
    "benchmark_name": "SCRUPLES",
    "benchmark_paper": "Scruples: A Corpus of Community Ethical Judgments on 32000 Real-Life Anecdotes",
    "title": "Whose opinions do language models reflect?",
    "authors": "S Santurkar,E Durmus,F Ladhakâ€¦Â - InternationalÂ â€¦, 2023",
    "publication": "proceedings.mlr.press",
    "year": NaN,
    "url": "https://proceedings.mlr.press/v202/santurkar23a?utm_source=chatgpt.com",
    "pdf_url": NaN,
    "abstract": "Abstract Language models (LMs) are increasingly being used in open-ended contexts,where the opinions they reflect in response to subjective queries can have a profoundÂ â€¦",
    "cited_by_count": 693.0
  },
  {
    "benchmark_name": "SCRUPLES",
    "benchmark_paper": "Scruples: A Corpus of Community Ethical Judgments on 32000 Real-Life Anecdotes",
    "title": "Evaluating the moral beliefs encoded in llms",
    "authors": "N Scherrer,C Shi,A Federâ€¦Â - Advances in NeuralÂ â€¦, 2023",
    "publication": "proceedings.neurips.cc",
    "year": NaN,
    "url": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/a2cf225ba392627529efef14dc857e22-Abstract-Conference.html",
    "pdf_url": NaN,
    "abstract": "This paper presents a case study on the design, administration, post-processing, andevaluation of surveys on large language models (LLMs). It comprises two components:(1) AÂ â€¦",
    "cited_by_count": 217.0
  },
  {
    "benchmark_name": "SCRUPLES",
    "benchmark_paper": "Scruples: A Corpus of Community Ethical Judgments on 32000 Real-Life Anecdotes",
    "title": "Large language model alignment: A survey",
    "authors": "T Shen,R Jin,Y Huang,C Liu,W Dong, Z Guoâ€¦Â - arXiv preprint arXivÂ â€¦, 2023",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2309.15025",
    "pdf_url": NaN,
    "abstract": "Recent years have witnessed remarkable progress made in large language models (LLMs).Such advancements, while garnering significant attention, have concurrently elicited variousÂ â€¦",
    "cited_by_count": 289.0
  },
  {
    "benchmark_name": "SCRUPLES",
    "benchmark_paper": "Scruples: A Corpus of Community Ethical Judgments on 32000 Real-Life Anecdotes",
    "title": "When to make exceptions: Exploring language models as accounts of human moral judgment",
    "authors": "Z Jin,S Levine,F Gonzalez Adautoâ€¦Â - Advances in neuralÂ â€¦, 2022",
    "publication": "proceedings.neurips.cc",
    "year": NaN,
    "url": "https://proceedings.neurips.cc/paper_files/paper/2022/hash/b654d6150630a5ba5df7a55621390daf-Abstract-Conference.html",
    "pdf_url": NaN,
    "abstract": "AI systems are becoming increasingly intertwined with human life. In order to effectivelycollaborate with humans and ensure safety, AI systems need to be able to understandÂ â€¦",
    "cited_by_count": 135.0
  },
  {
    "benchmark_name": "SCRUPLES",
    "benchmark_paper": "Scruples: A Corpus of Community Ethical Judgments on 32000 Real-Life Anecdotes",
    "title": "Knowledge of cultural moral norms in large language models",
    "authors": "A Ramezani,Y Xu- arXiv preprint arXiv:2306.01857, 2023",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2306.01857",
    "pdf_url": NaN,
    "abstract": "Moral norms vary across cultures. A recent line of work suggests that English large languagemodels contain human-like moral biases, but these studies typically do not examine moralÂ â€¦",
    "cited_by_count": 121.0
  },
  {
    "benchmark_name": "SCRUPLES",
    "benchmark_paper": "Scruples: A Corpus of Community Ethical Judgments on 32000 Real-Life Anecdotes",
    "title": "Moca: Measuring human-language model alignment on causal and moral judgment tasks",
    "authors": "A Nie,Y Zhang,AS Amdekar,C Piechâ€¦Â - Advances inÂ â€¦, 2023",
    "publication": "proceedings.neurips.cc",
    "year": NaN,
    "url": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/f751c6f8bfb52c60f43942896fe65904-Abstract-Conference.html",
    "pdf_url": NaN,
    "abstract": "Human commonsense understanding of the physical and social world is organized aroundintuitive theories. These theories support making causal and moral judgments. WhenÂ â€¦",
    "cited_by_count": 69.0
  },
  {
    "benchmark_name": "SCRUPLES",
    "benchmark_paper": "Scruples: A Corpus of Community Ethical Judgments on 32000 Real-Life Anecdotes",
    "title": "Safetyprompts: a systematic review of open datasets for evaluating and improving large language model safety",
    "authors": "P RÃ¶ttger,F Pernisi,B Vidgen,D Hovy- Proceedings of the AAAIÂ â€¦, 2025",
    "publication": "ojs.aaai.org",
    "year": NaN,
    "url": "https://ojs.aaai.org/index.php/AAAI/article/view/34975",
    "pdf_url": NaN,
    "abstract": "The last two years have seen a rapid growth in concerns around the safety of largelanguage models (LLMs). Researchers and practitioners have met these concerns byÂ â€¦",
    "cited_by_count": 52.0
  },
  {
    "benchmark_name": "SCRUPLES",
    "benchmark_paper": "Scruples: A Corpus of Community Ethical Judgments on 32000 Real-Life Anecdotes",
    "title": "Safetybench: Evaluating the safety of large language models",
    "authors": "Z Zhang,L Lei, L Wu, R Sun,Y Huang, C Longâ€¦Â - arXiv preprint arXivÂ â€¦, 2023",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2309.07045",
    "pdf_url": NaN,
    "abstract": "With the rapid development of Large Language Models (LLMs), increasing attention hasbeen paid to their safety concerns. Consequently, evaluating the safety of LLMs has becomeÂ â€¦",
    "cited_by_count": 108.0
  },
  {
    "benchmark_name": "MoralBench",
    "benchmark_paper": "MoralBench: Moral Evaluation of LLMs",
    "title": "Large language model psychometrics: A systematic review of evaluation, validation, and enhancement",
    "authors": "H Ye, J Jin,Y Xie, X Zhang,G Song- arXiv preprint arXiv:2505.08245, 2025",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2505.08245",
    "pdf_url": NaN,
    "abstract": "The rapid advancement of large language models (LLMs) has outpaced traditionalevaluation methodologies. It presents novel challenges, such as measuring human-likeÂ â€¦",
    "cited_by_count": 12.0
  },
  {
    "benchmark_name": "MoralBench",
    "benchmark_paper": "MoralBench: Moral Evaluation of LLMs",
    "title": "Recommender systems meet large language model agents: A survey",
    "authors": "X Zhu,Y Wang,H Gao,W Xu,C Wangâ€¦Â - â€¦Â and TrendsÂ® inÂ â€¦, 2025",
    "publication": "nowpublishers.com",
    "year": NaN,
    "url": "https://www.nowpublishers.com/article/Details/SEC-050",
    "pdf_url": NaN,
    "abstract": "In recent years, the integration of Large Language Models (LLMs) and RecommenderSystems (RS) has revolutionized the way personalized and intelligent user experiences areÂ â€¦",
    "cited_by_count": 15.0
  },
  {
    "benchmark_name": "MoralBench",
    "benchmark_paper": "MoralBench: Moral Evaluation of LLMs",
    "title": "The pluralistic moral gap: Understanding judgment and value differences between humans and large language models",
    "authors": "G Russo,D Nozza,P RÃ¶ttger,D Hovy- arXiv preprint arXiv:2507.17216, 2025",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2507.17216",
    "pdf_url": NaN,
    "abstract": "People increasingly rely on Large Language Models (LLMs) for moral advice, which mayinfluence humans' decisions. Yet, little is known about how closely LLMs align with humanÂ â€¦",
    "cited_by_count": 5.0
  },
  {
    "benchmark_name": "MoralBench",
    "benchmark_paper": "MoralBench: Moral Evaluation of LLMs",
    "title": "Normative evaluation of large language models with everyday moral dilemmas",
    "authors": "P Sachdeva, T van NuenenÂ - Proceedings of the 2025 ACM ConferenceÂ â€¦, 2025",
    "publication": "dl.acm.org",
    "year": NaN,
    "url": "https://dl.acm.org/doi/abs/10.1145/3715275.3732044",
    "pdf_url": NaN,
    "abstract": "The rapid adoption of large language models (LLMs) has spurred extensive research intotheir encoded moral norms and decision-making processes. While prior work oftenÂ â€¦",
    "cited_by_count": 6.0
  },
  {
    "benchmark_name": "MoralBench",
    "benchmark_paper": "MoralBench: Moral Evaluation of LLMs",
    "title": "Value compass benchmarks: A comprehensive, generative and self-evolving platform for llms' value evaluation",
    "authors": "J Yao,X Yi,S Duan,J Wang,Y Baiâ€¦Â - Proceedings of theÂ â€¦, 2025",
    "publication": "aclanthology.org",
    "year": NaN,
    "url": "https://aclanthology.org/2025.acl-demo.64/",
    "pdf_url": NaN,
    "abstract": "As large language models (LLMs) are gradually integrated into human daily life, assessingtheir underlying values becomes essential for understanding their risks and alignment withÂ â€¦",
    "cited_by_count": 3.0
  },
  {
    "benchmark_name": "MoralBench",
    "benchmark_paper": "MoralBench: Moral Evaluation of LLMs",
    "title": "Benchmarking and advancing large language models for local life services",
    "authors": "X Lan,J Feng, J Lei, X Shi, Y LiÂ - Proceedings of the 31st ACM SIGKDDÂ â€¦, 2025",
    "publication": "dl.acm.org",
    "year": NaN,
    "url": "https://dl.acm.org/doi/abs/10.1145/3711896.3737196",
    "pdf_url": NaN,
    "abstract": "Large language models (LLMs) have exhibited remarkable capabilities and achievedsignificant breakthroughs across various domains, leading to their widespread adoption inÂ â€¦",
    "cited_by_count": 4.0
  },
  {
    "benchmark_name": "MoralBench",
    "benchmark_paper": "MoralBench: Moral Evaluation of LLMs",
    "title": "MoralBench: A MultiModal Moral Benchmark for LVLMs",
    "authors": "B Yan,J Zhang, Z Chen,S Shan, X ChenÂ - arXiv preprint arXivÂ â€¦, 2024",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2412.20718",
    "pdf_url": NaN,
    "abstract": "Recently, large foundation models, including large language models (LLMs) and largevision-language models (LVLMs), have become essential tools in critical fields such as lawÂ â€¦",
    "cited_by_count": 7.0
  },
  {
    "benchmark_name": "MoralBench",
    "benchmark_paper": "MoralBench: Moral Evaluation of LLMs",
    "title": "Whose morality do they speak? Unraveling cultural bias in multilingual language models",
    "authors": "M Aksoy- Natural Language Processing Journal, 2025",
    "publication": "Elsevier",
    "year": NaN,
    "url": "https://www.sciencedirect.com/science/article/pii/S2949719125000482",
    "pdf_url": NaN,
    "abstract": "Large language models (LLMs) have become integral tools in diverse domains, yet theirmoral reasoning capabilities across cultural and linguistic contexts remain underexploredÂ â€¦",
    "cited_by_count": 9.0
  },
  {
    "benchmark_name": "MoralBench",
    "benchmark_paper": "MoralBench: Moral Evaluation of LLMs",
    "title": "Large Language Models meet moral values: A comprehensive assessment of moral abilities",
    "authors": "L Bulla,S De Giorgis,M MongiovÃ¬â€¦Â - Computers in HumanÂ â€¦, 2025",
    "publication": "Elsevier",
    "year": NaN,
    "url": "https://www.sciencedirect.com/science/article/pii/S2451958825000247",
    "pdf_url": NaN,
    "abstract": "Automatic moral classification in textual data is crucial for various fields including NaturalLanguage Processing (NLP), social sciences, and ethical AI development. DespiteÂ â€¦",
    "cited_by_count": 10.0
  },
  {
    "benchmark_name": "MoralBench",
    "benchmark_paper": "MoralBench: Moral Evaluation of LLMs",
    "title": "SafeDialBench: A Fine-Grained Safety Benchmark for Large Language Models in Multi-Turn Dialogues with Diverse Jailbreak Attacks",
    "authors": "H Cao, Y Wang, S Jing, Z Peng, Z Bai, Z Caoâ€¦Â - arXiv preprint arXivÂ â€¦, 2025",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2502.11090",
    "pdf_url": NaN,
    "abstract": "With the rapid advancement of Large Language Models (LLMs), the safety of LLMs hasbeen a critical concern requiring precise assessment. Current benchmarks primarilyÂ â€¦",
    "cited_by_count": 5.0
  },
  {
    "benchmark_name": "Social Chemistry 101",
    "benchmark_paper": "Social Chemistry 101: Learning to Reason about Social and Moral Norms",
    "title": "Ai alignment: A comprehensive survey",
    "authors": "J Ji,T Qiu,B Chen, B Zhang,H Lou, K Wangâ€¦Â - arXiv preprint arXivÂ â€¦, 2023",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2310.19852",
    "pdf_url": NaN,
    "abstract": "AI alignment aims to make AI systems behave in line with human intentions and values. AsAI systems grow more capable, so do risks from misalignment. To provide a comprehensiveÂ â€¦",
    "cited_by_count": 459.0
  },
  {
    "benchmark_name": "Social Chemistry 101",
    "benchmark_paper": "Social Chemistry 101: Learning to Reason about Social and Moral Norms",
    "title": "Evaluating large language models: A comprehensive survey",
    "authors": "Z Guo,R Jin,C Liu,Y Huang,D Shi, L Yu, Y Liuâ€¦Â - arXiv preprint arXivÂ â€¦, 2023",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2310.19736",
    "pdf_url": NaN,
    "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities across a broadspectrum of tasks. They have attracted significant attention and been deployed in numerousÂ â€¦",
    "cited_by_count": 237.0
  },
  {
    "benchmark_name": "Social Chemistry 101",
    "benchmark_paper": "Social Chemistry 101: Learning to Reason about Social and Moral Norms",
    "title": "DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models.",
    "authors": "B Wang,W Chen,H Pei,C Xie,M Kang,C Zhang,C Xuâ€¦Â - NeurIPS, 2023",
    "publication": "blogs.qub.ac.uk",
    "year": NaN,
    "url": "https://blogs.qub.ac.uk/wp-content/uploads/sites/7/2024/01/A-comprehensive-Assessment-of-Trustworthiness-in-GPT-Models.pdf",
    "pdf_url": NaN,
    "abstract": "Abstract Generative Pre-trained Transformer (GPT) models have exhibited exciting progressin their capabilities, capturing the interest of practitioners and the public alike. Yet, while theÂ â€¦",
    "cited_by_count": 625.0
  },
  {
    "benchmark_name": "Social Chemistry 101",
    "benchmark_paper": "Social Chemistry 101: Learning to Reason about Social and Moral Norms",
    "title": "Trustllm: Trustworthiness in large language models",
    "authors": "Y Huang,L Sun,H Wang, S Wu,Q Zhang, Y Liâ€¦Â - arXiv preprint arXivÂ â€¦, 2024",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2401.05561",
    "pdf_url": NaN,
    "abstract": "Large language models (LLMs), exemplified by ChatGPT, have gained considerableattention for their excellent natural language processing capabilities. Nonetheless, theseÂ â€¦",
    "cited_by_count": 487.0
  },
  {
    "benchmark_name": "Social Chemistry 101",
    "benchmark_paper": "Social Chemistry 101: Learning to Reason about Social and Moral Norms",
    "title": "Evaluating the moral beliefs encoded in llms",
    "authors": "N Scherrer,C Shi,A Federâ€¦Â - Advances in NeuralÂ â€¦, 2023",
    "publication": "proceedings.neurips.cc",
    "year": NaN,
    "url": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/a2cf225ba392627529efef14dc857e22-Abstract-Conference.html",
    "pdf_url": NaN,
    "abstract": "This paper presents a case study on the design, administration, post-processing, andevaluation of surveys on large language models (LLMs). It comprises two components:(1) AÂ â€¦",
    "cited_by_count": 217.0
  },
  {
    "benchmark_name": "Social Chemistry 101",
    "benchmark_paper": "Social Chemistry 101: Learning to Reason about Social and Moral Norms",
    "title": "Large language model alignment: A survey",
    "authors": "T Shen,R Jin,Y Huang,C Liu,W Dong, Z Guoâ€¦Â - arXiv preprint arXivÂ â€¦, 2023",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2309.15025",
    "pdf_url": NaN,
    "abstract": "Recent years have witnessed remarkable progress made in large language models (LLMs).Such advancements, while garnering significant attention, have concurrently elicited variousÂ â€¦",
    "cited_by_count": 289.0
  },
  {
    "benchmark_name": "Social Chemistry 101",
    "benchmark_paper": "Social Chemistry 101: Learning to Reason about Social and Moral Norms",
    "title": "Position: Trustllm: Trustworthiness in large language models",
    "authors": "Y Huang,L Sun,H Wang, S Wuâ€¦Â - InternationalÂ â€¦, 2024",
    "publication": "proceedings.mlr.press",
    "year": NaN,
    "url": "http://proceedings.mlr.press/v235/huang24x.html",
    "pdf_url": NaN,
    "abstract": "Large language models (LLMs) have gained considerable attention for their excellentnatural language processing capabilities. Nonetheless, these LLMs present manyÂ â€¦",
    "cited_by_count": 88.0
  },
  {
    "benchmark_name": "Social Chemistry 101",
    "benchmark_paper": "Social Chemistry 101: Learning to Reason about Social and Moral Norms",
    "title": "Natural language reasoning, a survey",
    "authors": "F Yu,H Zhang,P Tiwari,B Wang- ACM Computing Surveys, 2024",
    "publication": "dl.acm.org",
    "year": NaN,
    "url": "https://dl.acm.org/doi/abs/10.1145/3664194",
    "pdf_url": NaN,
    "abstract": "This survey article proposes a clearer view of Natural Language Reasoning (NLR) in thefield of Natural Language Processing (NLP), both conceptually and practicallyÂ â€¦",
    "cited_by_count": 161.0
  },
  {
    "benchmark_name": "Social Chemistry 101",
    "benchmark_paper": "Social Chemistry 101: Learning to Reason about Social and Moral Norms",
    "title": "A survey on fairness in large language models",
    "authors": "Y Li,M Du,R Song,X Wang,Y Wang- arXiv preprint arXiv:2308.10149, 2023",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2308.10149",
    "pdf_url": NaN,
    "abstract": "Large Language Models (LLMs) have shown powerful performance and developmentprospects and are widely deployed in the real world. However, LLMs can capture socialÂ â€¦",
    "cited_by_count": 166.0
  },
  {
    "benchmark_name": "Social Chemistry 101",
    "benchmark_paper": "Social Chemistry 101: Learning to Reason about Social and Moral Norms",
    "title": "Foundation metrics for evaluating effectiveness of healthcare conversations powered by generative AI",
    "authors": "M Abbasian,E Khatibi,I Azimi,D Onianiâ€¦Â - NPJ DigitalÂ â€¦, 2024",
    "publication": "nature.com",
    "year": NaN,
    "url": "https://www.nature.com/articles/s41746-024-01074-z",
    "pdf_url": NaN,
    "abstract": "Abstract Generative Artificial Intelligence is set to revolutionize healthcare delivery bytransforming traditional patient care into a more personalized, efficient, and proactiveÂ â€¦",
    "cited_by_count": 121.0
  },
  {
    "benchmark_name": "Delphi",
    "benchmark_paper": "Delphi: Towards Machine Ethics and Norms",
    "title": "Evaluating the moral beliefs encoded in llms",
    "authors": "N Scherrer,C Shi,A Federâ€¦Â - Advances in NeuralÂ â€¦, 2023",
    "publication": "proceedings.neurips.cc",
    "year": NaN,
    "url": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/a2cf225ba392627529efef14dc857e22-Abstract-Conference.html",
    "pdf_url": NaN,
    "abstract": "This paper presents a case study on the design, administration, post-processing, andevaluation of surveys on large language models (LLMs). It comprises two components:(1) AÂ â€¦",
    "cited_by_count": 217.0
  },
  {
    "benchmark_name": "Delphi",
    "benchmark_paper": "Delphi: Towards Machine Ethics and Norms",
    "title": "The TESCREAL bundle: Eugenics and the promise of utopia through artificial general intelligence",
    "authors": "T Gebru, Ã‰P TorresÂ - First Monday, 2024",
    "publication": "firstmonday.org",
    "year": NaN,
    "url": "http://firstmonday.org/ojs/index.php/fm/article/view/13636",
    "pdf_url": NaN,
    "abstract": "The stated goal of many organizations in the field of artificial intelligence (AI) is to developartificial general intelligence (AGI), an imagined system with more intelligence than anythingÂ â€¦",
    "cited_by_count": 198.0
  },
  {
    "benchmark_name": "Delphi",
    "benchmark_paper": "Delphi: Towards Machine Ethics and Norms",
    "title": "Collective constitutional ai: Aligning a language model with public input",
    "authors": "S Huang,D Siddarth, L Lovitt,TI Liaoâ€¦Â - Proceedings of theÂ â€¦, 2024",
    "publication": "dl.acm.org",
    "year": NaN,
    "url": "https://dl.acm.org/doi/abs/10.1145/3630106.3658979",
    "pdf_url": NaN,
    "abstract": "There is growing consensus that language model (LM) developers should not be the soledeciders of LM behavior, creating a need for methods that enable the broader public toÂ â€¦",
    "cited_by_count": 102.0
  },
  {
    "benchmark_name": "Delphi",
    "benchmark_paper": "Delphi: Towards Machine Ethics and Norms",
    "title": "Open-Ethical AI: Advancements in Open-Source Human-Centric Neural Language Models",
    "authors": "S Sicari,JF Cevallos M,A Rizzardiâ€¦Â - ACM ComputingÂ â€¦, 2024",
    "publication": "dl.acm.org",
    "year": NaN,
    "url": "https://dl.acm.org/doi/abs/10.1145/3703454",
    "pdf_url": NaN,
    "abstract": "This survey summarises the most recent methods for building and assessing helpful, honest,and harmless neural language models, considering small, medium, and large-size modelsÂ â€¦",
    "cited_by_count": 9.0
  },
  {
    "benchmark_name": "Delphi",
    "benchmark_paper": "Delphi: Towards Machine Ethics and Norms",
    "title": "Personalisation within bounds: A risk taxonomy and policy framework for the alignment of large language models with personalised feedback",
    "authors": "HR Kirk,B Vidgen,P RÃ¶ttger,SA Hale- arXiv preprint arXiv:2303.05453, 2023",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2303.05453",
    "pdf_url": NaN,
    "abstract": "Large language models (LLMs) are used to generate content for a wide range of tasks, andare set to reach a growing audience in coming years due to integration in product interfacesÂ â€¦",
    "cited_by_count": 139.0
  },
  {
    "benchmark_name": "Delphi",
    "benchmark_paper": "Delphi: Towards Machine Ethics and Norms",
    "title": "Socially intelligent machines that learn from humans and help humans learn",
    "authors": "H Gweon,J Fan,B Kim- Philosophical Transactions ofÂ â€¦, 2023",
    "publication": "royalsocietypublishing.org",
    "year": NaN,
    "url": "https://royalsocietypublishing.org/doi/abs/10.1098/rsta.2022.0048",
    "pdf_url": NaN,
    "abstract": "A hallmark of human intelligence is the ability to understand and influence other minds.Humans engage in inferential social learning (ISL) by using commonsense psychology toÂ â€¦",
    "cited_by_count": 30.0
  },
  {
    "benchmark_name": "Delphi",
    "benchmark_paper": "Delphi: Towards Machine Ethics and Norms",
    "title": "Value kaleidoscope: Engaging ai with pluralistic human values, rights, and duties",
    "authors": "T Sorensen,L Jiang,JD Hwang,S Levineâ€¦Â - Proceedings of theÂ â€¦, 2024",
    "publication": "ojs.aaai.org",
    "year": NaN,
    "url": "https://ojs.aaai.org/index.php/AAAI/article/view/29970",
    "pdf_url": NaN,
    "abstract": "Human values are crucial to human decision-making.\\textit {Value pluralism} is the view thatmultiple correct values may be held in tension with one another (eg, when considering\\textitÂ â€¦",
    "cited_by_count": 118.0
  },
  {
    "benchmark_name": "Delphi",
    "benchmark_paper": "Delphi: Towards Machine Ethics and Norms",
    "title": "Unveiling the implicit toxicity in large language models",
    "authors": "J Wen,P Ke,H Sun,Z Zhang, C Li, J Baiâ€¦Â - arXiv preprint arXivÂ â€¦, 2023",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2311.17391",
    "pdf_url": NaN,
    "abstract": "The open-endedness of large language models (LLMs) combined with their impressivecapabilities may lead to new safety issues when being exploited for malicious use. WhileÂ â€¦",
    "cited_by_count": 109.0
  },
  {
    "benchmark_name": "Delphi",
    "benchmark_paper": "Delphi: Towards Machine Ethics and Norms",
    "title": "Bridging the gap: A survey on integrating (human) feedback for natural language generation",
    "authors": "P Fernandes,A Madaan,E Liu,A Farinhasâ€¦Â - Transactions of theÂ â€¦, 2023",
    "publication": "direct.mit.edu",
    "year": NaN,
    "url": "https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00626/118795",
    "pdf_url": NaN,
    "abstract": "Natural language generation has witnessed significant advancements due to the training oflarge language models on vast internet-scale datasets. Despite these advancements, thereÂ â€¦",
    "cited_by_count": 99.0
  },
  {
    "benchmark_name": "Delphi",
    "benchmark_paper": "Delphi: Towards Machine Ethics and Norms",
    "title": "NLPositionality: Characterizing design biases of datasets and models",
    "authors": "S Santy,JT Liang,RL Bras,K Reineckeâ€¦Â - arXiv preprint arXivÂ â€¦, 2023",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2306.01943",
    "pdf_url": NaN,
    "abstract": "Design biases in NLP systems, such as performance differences for different populations,often stem from their creator's positionality, ie, views and lived experiences shaped byÂ â€¦",
    "cited_by_count": 103.0
  },
  {
    "benchmark_name": "STORAL",
    "benchmark_paper": "A Corpus for Understanding and Generating Moral Stories",
    "title": "Open-world story generation with structured knowledge enhancement: A comprehensive survey",
    "authors": "Y Wang,J Lin,Z Yu,W Hu,BF Karlsson- Neurocomputing, 2023",
    "publication": "Elsevier",
    "year": NaN,
    "url": "https://www.sciencedirect.com/science/article/pii/S0925231223009153",
    "pdf_url": NaN,
    "abstract": "Storytelling and narrative are fundamental to human experience, intertwined with our socialand cultural engagement. As such, researchers have long attempted to create systems thatÂ â€¦",
    "cited_by_count": 42.0
  },
  {
    "benchmark_name": "STORAL",
    "benchmark_paper": "A Corpus for Understanding and Generating Moral Stories",
    "title": "Slam-omni: Timbre-controllable voice interaction system with single-stage training",
    "authors": "W Chen,Z Ma,R Yan,Y Liang,X Li, R Xu,Z Niuâ€¦Â - arXiv preprint arXivÂ â€¦, 2024",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2412.15649",
    "pdf_url": NaN,
    "abstract": "Recent advancements highlight the potential of end-to-end real-time spoken dialoguesystems, showcasing their low latency and high quality. In this paper, we introduce SLAMÂ â€¦",
    "cited_by_count": 36.0
  },
  {
    "benchmark_name": "STORAL",
    "benchmark_paper": "A Corpus for Understanding and Generating Moral Stories",
    "title": "Values, ethics, morals? on the use of moral concepts in NLP research",
    "authors": "K Vida, J Simon,A Lauscher- arXiv preprint arXiv:2310.13915, 2023",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2310.13915",
    "pdf_url": NaN,
    "abstract": "With language technology increasingly affecting individuals' lives, many recent works haveinvestigated the ethical aspects of NLP. Among other topics, researchers focused on theÂ â€¦",
    "cited_by_count": 24.0
  },
  {
    "benchmark_name": "STORAL",
    "benchmark_paper": "A Corpus for Understanding and Generating Moral Stories",
    "title": "Bridging cultural nuances in dialogue agents through cultural value surveys",
    "authors": "Y Cao,M Chen,D Hershcovich- arXiv preprint arXiv:2401.10352, 2024",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2401.10352",
    "pdf_url": NaN,
    "abstract": "The cultural landscape of interactions with dialogue agents is a compelling yet relativelyunexplored territory. It's clear that various sociocultural aspects--from communication stylesÂ â€¦",
    "cited_by_count": 10.0
  },
  {
    "benchmark_name": "STORAL",
    "benchmark_paper": "A Corpus for Understanding and Generating Moral Stories",
    "title": "Uro-bench: A comprehensive benchmark for end-to-end spoken dialogue models",
    "authors": "R Yan,X Li,W Chen,Z Niu,C Yang,Z Ma,K Yuâ€¦Â - arXiv preprint arXivÂ â€¦, 2025",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2502.17810",
    "pdf_url": NaN,
    "abstract": "In recent years, with advances in large language models (LLMs), end-to-end spokendialogue models (SDMs) have made significant strides. Compared to text-based LLMs, theÂ â€¦",
    "cited_by_count": 14.0
  },
  {
    "benchmark_name": "STORAL",
    "benchmark_paper": "A Corpus for Understanding and Generating Moral Stories",
    "title": "CULEMO: Cultural Lenses on Emotion--Benchmarking LLMs for Cross-Cultural Emotion Understanding",
    "authors": "TD Belay,AH Ahmed,A Grissom II,I Ameerâ€¦Â - arXiv preprint arXivÂ â€¦, 2025",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2503.10688",
    "pdf_url": NaN,
    "abstract": "NLP research has increasingly focused on subjective tasks such as emotion analysis.However, existing emotion benchmarks suffer from two major shortcomings:(1) they largelyÂ â€¦",
    "cited_by_count": 8.0
  },
  {
    "benchmark_name": "STORAL",
    "benchmark_paper": "A Corpus for Understanding and Generating Moral Stories",
    "title": "A survey on modelling morality for text analysis",
    "authors": "I Reinig,M Becker, I Rehbein,SP Ponzetto- 2024",
    "publication": "madoc.bib.uni-mannheim.de",
    "year": NaN,
    "url": "https://madoc.bib.uni-mannheim.de/67689/",
    "pdf_url": NaN,
    "abstract": "In this survey, we provide a systematic review of recent work on modelling morality in text, anarea of research that has garnered increasing attention in recent years. Our survey isÂ â€¦",
    "cited_by_count": 9.0
  },
  {
    "benchmark_name": "STORAL",
    "benchmark_paper": "A Corpus for Understanding and Generating Moral Stories",
    "title": "Bilingual Dialogue Dataset with Personality and Emotion Annotations for Personality Recognition in Education",
    "authors": "Z Liu, Y Xiao,Z Su,L Ye,K Lu, X PengÂ - Scientific Data, 2025",
    "publication": "nature.com",
    "year": NaN,
    "url": "https://www.nature.com/articles/s41597-025-04836-w",
    "pdf_url": NaN,
    "abstract": "Dialogue datasets are essential for advancing natural language processing (NLP) tasks.However, many existing datasets lack integrated annotations for personality and emotionÂ â€¦",
    "cited_by_count": 1.0
  },
  {
    "benchmark_name": "STORAL",
    "benchmark_paper": "A Corpus for Understanding and Generating Moral Stories",
    "title": "MORABLES: A Benchmark for Assessing Abstract Moral Reasoning in LLMs with Fables",
    "authors": "M Marcuzzo,A Zangari,A Albarelliâ€¦Â - arXiv preprint arXivÂ â€¦, 2025",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2509.12371",
    "pdf_url": NaN,
    "abstract": "As LLMs excel on standard reading comprehension benchmarks, attention is shifting towardevaluating their capacity for complex abstract reasoning and inference. Literature-basedÂ â€¦",
    "cited_by_count": NaN
  },
  {
    "benchmark_name": "STORAL",
    "benchmark_paper": "A Corpus for Understanding and Generating Moral Stories",
    "title": "JETHICS: Japanese Ethics Understanding Evaluation Dataset",
    "authors": "M Takeshita,R Rzepka- arXiv preprint arXiv:2506.16187, 2025",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2506.16187",
    "pdf_url": NaN,
    "abstract": "In this work, we propose JETHICS, a Japanese dataset for evaluating ethics understandingof AI models. JETHICS contains 78K examples and is built by following the constructionÂ â€¦",
    "cited_by_count": NaN
  },
  {
    "benchmark_name": "Moral Integrity Corpus",
    "benchmark_paper": "The Moral Integrity Corpus: A Benchmark for Ethical Dialogue Systems",
    "title": "Ai alignment: A comprehensive survey",
    "authors": "J Ji,T Qiu,B Chen, B Zhang,H Lou, K Wangâ€¦Â - arXiv preprint arXivÂ â€¦, 2023",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2310.19852",
    "pdf_url": NaN,
    "abstract": "AI alignment aims to make AI systems behave in line with human intentions and values. AsAI systems grow more capable, so do risks from misalignment. To provide a comprehensiveÂ â€¦",
    "cited_by_count": 459.0
  },
  {
    "benchmark_name": "Moral Integrity Corpus",
    "benchmark_paper": "The Moral Integrity Corpus: A Benchmark for Ethical Dialogue Systems",
    "title": "Evaluating large language models: A comprehensive survey",
    "authors": "Z Guo,R Jin,C Liu,Y Huang,D Shi, L Yu, Y Liuâ€¦Â - arXiv preprint arXivÂ â€¦, 2023",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2310.19736",
    "pdf_url": NaN,
    "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities across a broadspectrum of tasks. They have attracted significant attention and been deployed in numerousÂ â€¦",
    "cited_by_count": 237.0
  },
  {
    "benchmark_name": "Moral Integrity Corpus",
    "benchmark_paper": "The Moral Integrity Corpus: A Benchmark for Ethical Dialogue Systems",
    "title": "Large language models in education: Vision and opportunities",
    "authors": "W Gan, Z Qi, J Wu, JCW LinÂ - 2023 IEEE internationalÂ â€¦, 2023",
    "publication": "ieeexplore.ieee.org",
    "year": NaN,
    "url": "https://ieeexplore.ieee.org/abstract/document/10386291/",
    "pdf_url": NaN,
    "abstract": "With the rapid development of artificial intelligence technology, large language models(LLMs) have become a hot research topic. Education plays an important role in humanÂ â€¦",
    "cited_by_count": 202.0
  },
  {
    "benchmark_name": "Moral Integrity Corpus",
    "benchmark_paper": "The Moral Integrity Corpus: A Benchmark for Ethical Dialogue Systems",
    "title": "Training socially aligned language models in simulated human society",
    "authors": "R Liu,R Yang,C Jia,G Zhang,D Zhouâ€¦Â - arXiv preprint arXivÂ â€¦, 2023",
    "publication": "proceedings.iclr.cc",
    "year": NaN,
    "url": "https://proceedings.iclr.cc/paper_files/paper/2024/file/d763b4a2dde0ae7b77498516ce9f439e-Paper-Conference.pdf",
    "pdf_url": NaN,
    "abstract": "Social alignment in AI systems aims to ensure that these models behave according toestablished societal values. However, unlike humans, who derive consensus on valueÂ â€¦",
    "cited_by_count": 142.0
  },
  {
    "benchmark_name": "Moral Integrity Corpus",
    "benchmark_paper": "The Moral Integrity Corpus: A Benchmark for Ethical Dialogue Systems",
    "title": "Personalisation within bounds: A risk taxonomy and policy framework for the alignment of large language models with personalised feedback",
    "authors": "HR Kirk,B Vidgen,P RÃ¶ttger,SA Hale- arXiv preprint arXiv:2303.05453, 2023",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2303.05453",
    "pdf_url": NaN,
    "abstract": "Large language models (LLMs) are used to generate content for a wide range of tasks, andare set to reach a growing audience in coming years due to integration in product interfacesÂ â€¦",
    "cited_by_count": 139.0
  },
  {
    "benchmark_name": "Moral Integrity Corpus",
    "benchmark_paper": "The Moral Integrity Corpus: A Benchmark for Ethical Dialogue Systems",
    "title": "Mirages: On anthropomorphism in dialogue systems",
    "authors": "G Abercrombie,AC Curry,T Dinkar,V Rieserâ€¦Â - arXiv preprint arXivÂ â€¦, 2023",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2305.09800",
    "pdf_url": NaN,
    "abstract": "Automated dialogue or conversational systems are anthropomorphised by developers andpersonified by users. While a degree of anthropomorphism may be inevitable due to theÂ â€¦",
    "cited_by_count": 125.0
  },
  {
    "benchmark_name": "Moral Integrity Corpus",
    "benchmark_paper": "The Moral Integrity Corpus: A Benchmark for Ethical Dialogue Systems",
    "title": "Culturally aware and adapted nlp: A taxonomy and a survey of the state of the art",
    "authors": "CC Liu,I Gurevych,A Korhonen- Transactions of the Association forÂ â€¦, 2025",
    "publication": "direct.mit.edu",
    "year": NaN,
    "url": "https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00760/131587",
    "pdf_url": NaN,
    "abstract": "The surge of interest in culture in NLP has inspired much recent research, but a sharedunderstanding of â€œcultureâ€ remains unclear, making it difficult to evaluate progress in thisÂ â€¦",
    "cited_by_count": 48.0
  },
  {
    "benchmark_name": "Moral Integrity Corpus",
    "benchmark_paper": "The Moral Integrity Corpus: A Benchmark for Ethical Dialogue Systems",
    "title": "Survey of cultural awareness in language models: Text and beyond",
    "authors": "S Pawar,J Park,J Jin,A Arora,J Myungâ€¦Â - ComputationalÂ â€¦, 2025",
    "publication": "direct.mit.edu",
    "year": NaN,
    "url": "https://direct.mit.edu/coli/article/doi/10.1162/COLI.a.14/130804",
    "pdf_url": NaN,
    "abstract": "Large-scale deployment of large language models (LLMs) in various applications, such aschatbots and virtual assistants, requires LLMs to be culturally sensitive to the user to ensureÂ â€¦",
    "cited_by_count": 61.0
  },
  {
    "benchmark_name": "Moral Integrity Corpus",
    "benchmark_paper": "The Moral Integrity Corpus: A Benchmark for Ethical Dialogue Systems",
    "title": "Moca: Measuring human-language model alignment on causal and moral judgment tasks",
    "authors": "A Nie,Y Zhang,AS Amdekar,C Piechâ€¦Â - Advances inÂ â€¦, 2023",
    "publication": "proceedings.neurips.cc",
    "year": NaN,
    "url": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/f751c6f8bfb52c60f43942896fe65904-Abstract-Conference.html",
    "pdf_url": NaN,
    "abstract": "Human commonsense understanding of the physical and social world is organized aroundintuitive theories. These theories support making causal and moral judgments. WhenÂ â€¦",
    "cited_by_count": 69.0
  },
  {
    "benchmark_name": "Moral Integrity Corpus",
    "benchmark_paper": "The Moral Integrity Corpus: A Benchmark for Ethical Dialogue Systems",
    "title": "Safetyprompts: a systematic review of open datasets for evaluating and improving large language model safety",
    "authors": "P RÃ¶ttger,F Pernisi,B Vidgen,D Hovy- Proceedings of the AAAIÂ â€¦, 2025",
    "publication": "ojs.aaai.org",
    "year": NaN,
    "url": "https://ojs.aaai.org/index.php/AAAI/article/view/34975",
    "pdf_url": NaN,
    "abstract": "The last two years have seen a rapid growth in concerns around the safety of largelanguage models (LLMs). Researchers and practitioners have met these concerns byÂ â€¦",
    "cited_by_count": 52.0
  },
  {
    "benchmark_name": "RobustBench",
    "benchmark_paper": "RobustBench: a standardized adversarial robustness benchmark",
    "title": "Trustworthy AI: From principles to practices",
    "authors": "B Li,P Qi, B Liu, S Di,J Liu,J Pei,J Yiâ€¦Â - ACM Computing Surveys, 2023",
    "publication": "dl.acm.org",
    "year": NaN,
    "url": "https://dl.acm.org/doi/abs/10.1145/3555803",
    "pdf_url": NaN,
    "abstract": "The rapid development of Artificial Intelligence (AI) technology has enabled the deploymentof various systems based on it. However, many current AI systems are found vulnerable toÂ â€¦",
    "cited_by_count": 737.0
  },
  {
    "benchmark_name": "RobustBench",
    "benchmark_paper": "RobustBench: a standardized adversarial robustness benchmark",
    "title": "Interpretable deep learning: Interpretation, interpretability, trustworthiness, and beyond",
    "authors": "X Li,H Xiong,X Li, X Wu,X Zhang,J Liu,J Bianâ€¦Â - â€¦Â and Information Systems, 2022",
    "publication": "Springer",
    "year": NaN,
    "url": "https://link.springer.com/article/10.1007/s10115-022-01756-8",
    "pdf_url": NaN,
    "abstract": "Deep neural networks have been well-known for their superb handling of various machinelearning and artificial intelligence tasks. However, due to their over-parameterized black-boxÂ â€¦",
    "cited_by_count": 573.0
  },
  {
    "benchmark_name": "RobustBench",
    "benchmark_paper": "RobustBench: a standardized adversarial robustness benchmark",
    "title": "Jailbreakbench: An open robustness benchmark for jailbreaking large language models",
    "authors": "P Chao,E Debenedetti,A Robeyâ€¦Â - Advances inÂ â€¦, 2024",
    "publication": "proceedings.neurips.cc",
    "year": NaN,
    "url": "https://proceedings.neurips.cc/paper_files/paper/2024/hash/63092d79154adebd7305dfd498cbff70-Abstract-Datasets_and_Benchmarks_Track.html",
    "pdf_url": NaN,
    "abstract": "Jailbreak attacks cause large language models (LLMs) to generate harmful, unethical, orotherwise objectionable content. Evaluating these attacks presents a number of challengesÂ â€¦",
    "cited_by_count": 350.0
  },
  {
    "benchmark_name": "RobustBench",
    "benchmark_paper": "RobustBench: a standardized adversarial robustness benchmark",
    "title": "Harmbench: A standardized evaluation framework for automated red teaming and robust refusal",
    "authors": "M Mazeika,L Phan,X Yin,A Zou,Z Wang,N Muâ€¦Â - arXiv preprint arXivÂ â€¦, 2024",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2402.04249",
    "pdf_url": NaN,
    "abstract": "Automated red teaming holds substantial promise for uncovering and mitigating the risksassociated with the malicious use of large language models (LLMs), yet the field lacks aÂ â€¦",
    "cited_by_count": 549.0
  },
  {
    "benchmark_name": "RobustBench",
    "benchmark_paper": "RobustBench: a standardized adversarial robustness benchmark",
    "title": "Better diffusion models further improve adversarial training",
    "authors": "Z Wang,T Pang,C Du,M Linâ€¦Â - â€¦Â on machine learning, 2023",
    "publication": "proceedings.mlr.press",
    "year": NaN,
    "url": "http://proceedings.mlr.press/v202/wang23ad.html",
    "pdf_url": NaN,
    "abstract": "It has been recognized that the data generated by the denoising diffusion probabilisticmodel (DDPM) improves adversarial training. After two years of rapid development inÂ â€¦",
    "cited_by_count": 347.0
  },
  {
    "benchmark_name": "RobustBench",
    "benchmark_paper": "RobustBench: a standardized adversarial robustness benchmark",
    "title": "Visual adversarial examples jailbreak aligned large language models",
    "authors": "X Qi,K Huang,A Panda,P Hendersonâ€¦Â - Proceedings of theÂ â€¦, 2024",
    "publication": "ojs.aaai.org",
    "year": NaN,
    "url": "https://ojs.aaai.org/index.php/AAAI/article/view/30150",
    "pdf_url": NaN,
    "abstract": "Warning: this paper contains data, prompts, and model outputs that are offensive in nature.Recently, there has been a surge of interest in integrating vision into Large LanguageÂ â€¦",
    "cited_by_count": 331.0
  },
  {
    "benchmark_name": "RobustBench",
    "benchmark_paper": "RobustBench: a standardized adversarial robustness benchmark",
    "title": "Diffusion models for adversarial purification",
    "authors": "W Nie,B Guo,Y Huang,C Xiao,A Vahdatâ€¦Â - arXiv preprint arXivÂ â€¦, 2022",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2205.07460",
    "pdf_url": NaN,
    "abstract": "Adversarial purification refers to a class of defense methods that remove adversarialperturbations using a generative model. These methods do not make assumptions on theÂ â€¦",
    "cited_by_count": 743.0
  },
  {
    "benchmark_name": "RobustBench",
    "benchmark_paper": "RobustBench: a standardized adversarial robustness benchmark",
    "title": "Continual test-time domain adaptation",
    "authors": "Q Wang,O Fink,L Van Goolâ€¦Â - Proceedings of the IEEEÂ â€¦, 2022",
    "publication": "openaccess.thecvf.com",
    "year": NaN,
    "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Wang_Continual_Test-Time_Domain_Adaptation_CVPR_2022_paper.html",
    "pdf_url": NaN,
    "abstract": "Test-time domain adaptation aims to adapt a source pre-trained model to a target domainwithout using any source data. Existing works mainly consider the case where the targetÂ â€¦",
    "cited_by_count": 709.0
  },
  {
    "benchmark_name": "RobustBench",
    "benchmark_paper": "RobustBench: a standardized adversarial robustness benchmark",
    "title": "Foundational challenges in assuring alignment and safety of large language models",
    "authors": "U Anwar,A Saparov,J Rando,D Palekaâ€¦Â - arXiv preprint arXivÂ â€¦, 2024",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2404.09932",
    "pdf_url": NaN,
    "abstract": "This work identifies 18 foundational challenges in assuring the alignment and safety of largelanguage models (LLMs). These challenges are organized into three different categoriesÂ â€¦",
    "cited_by_count": 254.0
  },
  {
    "benchmark_name": "RobustBench",
    "benchmark_paper": "RobustBench: a standardized adversarial robustness benchmark",
    "title": "Jailbreaking leading safety-aligned llms with simple adaptive attacks",
    "authors": "M Andriushchenko,F Croce,N Flammarion- arXiv preprint arXivÂ â€¦, 2024",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2404.02151",
    "pdf_url": NaN,
    "abstract": "We show that even the most recent safety-aligned LLMs are not robust to simple adaptivejailbreaking attacks. First, we demonstrate how to successfully leverage access to logprobsÂ â€¦",
    "cited_by_count": 284.0
  },
  {
    "benchmark_name": "AutoAttack",
    "benchmark_paper": "Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks",
    "title": "A survey of attacks on large visionâ€“language models: Resources, advances, and future trends",
    "authors": "D Liu, M Yang,X Qu,P Zhouâ€¦Â - IEEE Transactions onÂ â€¦, 2025",
    "publication": "ieeexplore.ieee.org",
    "year": NaN,
    "url": "https://ieeexplore.ieee.org/abstract/document/11127221/",
    "pdf_url": NaN,
    "abstract": "With the significant development of large models in recent years, large visionâ€“languagemodels (LVLMs) have demonstrated remarkable capabilities across a wide range ofÂ â€¦",
    "cited_by_count": 86.0
  },
  {
    "benchmark_name": "AutoAttack",
    "benchmark_paper": "Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks",
    "title": "How deep learning sees the world: A survey on adversarial attacks & defenses",
    "authors": "JC Costa,T Roxo,H ProenÃ§a,PRM Inacio- IEEE Access, 2024",
    "publication": "ieeexplore.ieee.org",
    "year": NaN,
    "url": "https://ieeexplore.ieee.org/abstract/document/10510296/",
    "pdf_url": NaN,
    "abstract": "Deep Learning is currently used to perform multiple tasks, such as object recognition, facerecognition, and natural language processing. However, Deep Neural Networks (DNNs) areÂ â€¦",
    "cited_by_count": 143.0
  },
  {
    "benchmark_name": "AutoAttack",
    "benchmark_paper": "Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks",
    "title": "Cross-entropy loss functions: Theoretical analysis and applications",
    "authors": "A Mao,M Mohri,Y Zhong- International conference onÂ â€¦, 2023",
    "publication": "proceedings.mlr.press",
    "year": NaN,
    "url": "http://proceedings.mlr.press/v202/mao23b.html",
    "pdf_url": NaN,
    "abstract": "Cross-entropy is a widely used loss function in applications. It coincides with the logistic lossapplied to the outputs of a neural network, when the softmax is used. But, what guaranteesÂ â€¦",
    "cited_by_count": 948.0
  },
  {
    "benchmark_name": "AutoAttack",
    "benchmark_paper": "Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks",
    "title": "Harmbench: A standardized evaluation framework for automated red teaming and robust refusal",
    "authors": "M Mazeika,L Phan,X Yin,A Zou,Z Wang,N Muâ€¦Â - arXiv preprint arXivÂ â€¦, 2024",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2402.04249",
    "pdf_url": NaN,
    "abstract": "Automated red teaming holds substantial promise for uncovering and mitigating the risksassociated with the malicious use of large language models (LLMs), yet the field lacks aÂ â€¦",
    "cited_by_count": 549.0
  },
  {
    "benchmark_name": "AutoAttack",
    "benchmark_paper": "Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks",
    "title": "Better diffusion models further improve adversarial training",
    "authors": "Z Wang,T Pang,C Du,M Linâ€¦Â - â€¦Â on machine learning, 2023",
    "publication": "proceedings.mlr.press",
    "year": NaN,
    "url": "http://proceedings.mlr.press/v202/wang23ad.html",
    "pdf_url": NaN,
    "abstract": "It has been recognized that the data generated by the denoising diffusion probabilisticmodel (DDPM) improves adversarial training. After two years of rapid development inÂ â€¦",
    "cited_by_count": 347.0
  },
  {
    "benchmark_name": "AutoAttack",
    "benchmark_paper": "Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks",
    "title": "Diffusion models for adversarial purification",
    "authors": "W Nie,B Guo,Y Huang,C Xiao,A Vahdatâ€¦Â - arXiv preprint arXivÂ â€¦, 2022",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2205.07460",
    "pdf_url": NaN,
    "abstract": "Adversarial purification refers to a class of defense methods that remove adversarialperturbations using a generative model. These methods do not make assumptions on theÂ â€¦",
    "cited_by_count": 743.0
  },
  {
    "benchmark_name": "AutoAttack",
    "benchmark_paper": "Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks",
    "title": "Square attack: a query-efficient black-box adversarial attack via random search",
    "authors": "M Andriushchenko,F Croce,N Flammarionâ€¦Â - European conference onÂ â€¦, 2020",
    "publication": "Springer",
    "year": NaN,
    "url": "https://link.springer.com/chapter/10.1007/978-3-030-58592-1_29",
    "pdf_url": NaN,
    "abstract": "Abstract We propose the Square Attack, a score-based black-box l 2-and lâˆž-adversarialattack that does not rely on local gradient information and thus is not affected by gradientÂ â€¦",
    "cited_by_count": 1428.0
  },
  {
    "benchmark_name": "AutoAttack",
    "benchmark_paper": "Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks",
    "title": "Robustbench: a standardized adversarial robustness benchmark",
    "authors": "F Croce,M Andriushchenko,V Sehwagâ€¦Â - arXiv preprint arXivÂ â€¦, 2020",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2010.09670",
    "pdf_url": NaN,
    "abstract": "As a research community, we are still lacking a systematic understanding of the progress onadversarial robustness which often makes it hard to identify the most promising ideas inÂ â€¦",
    "cited_by_count": 989.0
  },
  {
    "benchmark_name": "AutoAttack",
    "benchmark_paper": "Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks",
    "title": "Data augmentation can improve robustness",
    "authors": "SA Rebuffi,S Gowal,DA Calianâ€¦Â - Advances in neuralÂ â€¦, 2021",
    "publication": "proceedings.neurips.cc",
    "year": NaN,
    "url": "https://proceedings.neurips.cc/paper/2021/hash/fb4c48608ce8825b558ccf07169a3421-Abstract.html",
    "pdf_url": NaN,
    "abstract": "Adversarial training suffers from robust overfitting, a phenomenon where the robust testaccuracy starts to decrease during training. In this paper, we focus on reducing robustÂ â€¦",
    "cited_by_count": 524.0
  },
  {
    "benchmark_name": "AutoAttack",
    "benchmark_paper": "Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks",
    "title": "Defensive unlearning with adversarial training for robust concept erasure in diffusion models",
    "authors": "Y Zhang,X Chen,J Jia,Y Zhangâ€¦Â - Advances in neuralÂ â€¦, 2024",
    "publication": "proceedings.neurips.cc",
    "year": NaN,
    "url": "https://proceedings.neurips.cc/paper_files/paper/2024/hash/40954ac18a457dd5f11145bae6454cdf-Abstract-Conference.html",
    "pdf_url": NaN,
    "abstract": "Diffusion models (DMs) have achieved remarkable success in text-to-image generation, butthey also pose safety risks, such as the potential generation of harmful content and copyrightÂ â€¦",
    "cited_by_count": 103.0
  },
  {
    "benchmark_name": "ImageNet-C / ImageNet-P",
    "benchmark_paper": "Benchmarking Neural Network Robustness to Common Corruptions and Perturbations",
    "title": "The rise and potential of large language model based agents: A survey",
    "authors": "Z Xi,W Chen,X Guo,W He,Y Ding, B Hongâ€¦Â - Science ChinaÂ â€¦, 2025",
    "publication": "Springer",
    "year": NaN,
    "url": "https://link.springer.com/article/10.1007/s11432-024-4222-0",
    "pdf_url": NaN,
    "abstract": "For a long time, researchers have sought artificial intelligence (AI) that matches or exceedshuman intelligence. AI agents, which are artificial entities capable of sensing theÂ â€¦",
    "cited_by_count": 1558.0
  },
  {
    "benchmark_name": "ImageNet-C / ImageNet-P",
    "benchmark_paper": "Benchmarking Neural Network Robustness to Common Corruptions and Perturbations",
    "title": "Ai alignment: A comprehensive survey",
    "authors": "J Ji,T Qiu,B Chen, B Zhang,H Lou, K Wangâ€¦Â - arXiv preprint arXivÂ â€¦, 2023",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2310.19852",
    "pdf_url": NaN,
    "abstract": "AI alignment aims to make AI systems behave in line with human intentions and values. AsAI systems grow more capable, so do risks from misalignment. To provide a comprehensiveÂ â€¦",
    "cited_by_count": 459.0
  },
  {
    "benchmark_name": "ImageNet-C / ImageNet-P",
    "benchmark_paper": "Benchmarking Neural Network Robustness to Common Corruptions and Perturbations",
    "title": "Dinov2: Learning robust visual features without supervision",
    "authors": "M Oquab,T Darcet,T Moutakanni,H Voâ€¦Â - arXiv preprint arXivÂ â€¦, 2023",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2304.07193",
    "pdf_url": NaN,
    "abstract": "The recent breakthroughs in natural language processing for model pretraining on largequantities of data have opened the way for similar foundation models in computer visionÂ â€¦",
    "cited_by_count": 5336.0
  },
  {
    "benchmark_name": "ImageNet-C / ImageNet-P",
    "benchmark_paper": "Benchmarking Neural Network Robustness to Common Corruptions and Perturbations",
    "title": "Reproducible scaling laws for contrastive language-image learning",
    "authors": "M Cherti, R Beaumont,R Wightmanâ€¦Â - Proceedings of theÂ â€¦, 2023",
    "publication": "openaccess.thecvf.com",
    "year": NaN,
    "url": "https://openaccess.thecvf.com/content/CVPR2023/html/Cherti_Reproducible_Scaling_Laws_for_Contrastive_Language-Image_Learning_CVPR_2023_paper",
    "pdf_url": NaN,
    "abstract": "Scaling up neural networks has led to remarkable performance across a wide range oftasks. Moreover, performance often follows reliable scaling laws as a function of training setÂ â€¦",
    "cited_by_count": 1238.0
  },
  {
    "benchmark_name": "ImageNet-C / ImageNet-P",
    "benchmark_paper": "Benchmarking Neural Network Robustness to Common Corruptions and Perturbations",
    "title": "Scaling vision transformers to 22 billion parameters",
    "authors": "M Dehghani,J Djolonga,B Mustafaâ€¦Â - InternationalÂ â€¦, 2023",
    "publication": "proceedings.mlr.press",
    "year": NaN,
    "url": "http://proceedings.mlr.press/v202/dehghani23a.html",
    "pdf_url": NaN,
    "abstract": "The scaling of Transformers has driven breakthrough capabilities for language models. Atpresent, the largest large language models (LLMs) contain upwards of 100B parametersÂ â€¦",
    "cited_by_count": 824.0
  },
  {
    "benchmark_name": "ImageNet-C / ImageNet-P",
    "benchmark_paper": "Benchmarking Neural Network Robustness to Common Corruptions and Perturbations",
    "title": "Cellpose 2.0: how to train your own model",
    "authors": "M Pachitariu,C Stringer- Nature methods, 2022",
    "publication": "nature.com",
    "year": NaN,
    "url": "https://www.nature.com/articles/s41592-022-01663-4",
    "pdf_url": NaN,
    "abstract": "Pretrained neural network models for biological segmentation can provide good out-of-the-box results for many image types. However, such models do not allow users to adapt theÂ â€¦",
    "cited_by_count": 1063.0
  },
  {
    "benchmark_name": "ImageNet-C / ImageNet-P",
    "benchmark_paper": "Benchmarking Neural Network Robustness to Common Corruptions and Perturbations",
    "title": "A convnet for the 2020s",
    "authors": "Z Liu,H Mao,CY Wu,C Feichtenhoferâ€¦Â - Proceedings of theÂ â€¦, 2022",
    "publication": "openaccess.thecvf.com",
    "year": NaN,
    "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Liu_A_ConvNet_for_the_2020s_CVPR_2022_paper.html",
    "pdf_url": NaN,
    "abstract": "The\" Roaring 20s\" of visual recognition began with the introduction of Vision Transformers(ViTs), which quickly superseded ConvNets as the state-of-the-art image classificationÂ â€¦",
    "cited_by_count": 10224.0
  },
  {
    "benchmark_name": "ImageNet-C / ImageNet-P",
    "benchmark_paper": "Benchmarking Neural Network Robustness to Common Corruptions and Perturbations",
    "title": "Repvit: Revisiting mobile cnn from vit perspective",
    "authors": "A Wang,H Chen,Z Lin,J Hanâ€¦Â - Proceedings of the IEEEÂ â€¦, 2024",
    "publication": "openaccess.thecvf.com",
    "year": NaN,
    "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Wang_RepViT_Revisiting_Mobile_CNN_From_ViT_Perspective_CVPR_2024_paper.html",
    "pdf_url": NaN,
    "abstract": "Abstract Recently lightweight Vision Transformers (ViTs) demonstrate superior performanceand lower latency compared with lightweight Convolutional Neural Networks (CNNs) onÂ â€¦",
    "cited_by_count": 580.0
  },
  {
    "benchmark_name": "ImageNet-C / ImageNet-P",
    "benchmark_paper": "Benchmarking Neural Network Robustness to Common Corruptions and Perturbations",
    "title": "Masked autoencoders are scalable vision learners",
    "authors": "K He,X Chen,S Xie,Y Li,P DollÃ¡râ€¦Â - Proceedings of theÂ â€¦, 2022",
    "publication": "openaccess.thecvf.com",
    "year": NaN,
    "url": "https://openaccess.thecvf.com/content/CVPR2022/html/He_Masked_Autoencoders_Are_Scalable_Vision_Learners_CVPR_2022_paper",
    "pdf_url": NaN,
    "abstract": "This paper shows that masked autoencoders (MAE) are scalable self-supervised learnersfor computer vision. Our MAE approach is simple: we mask random patches of the inputÂ â€¦",
    "cited_by_count": 12162.0
  },
  {
    "benchmark_name": "ImageNet-C / ImageNet-P",
    "benchmark_paper": "Benchmarking Neural Network Robustness to Common Corruptions and Perturbations",
    "title": "Learning transferable visual models from natural language supervision",
    "authors": "A Radford,JW Kim, C Hallacyâ€¦Â - InternationalÂ â€¦, 2021",
    "publication": "proceedings.mlr.press",
    "year": NaN,
    "url": "http://proceedings.mlr.press/v139/radford21a",
    "pdf_url": NaN,
    "abstract": "State-of-the-art computer vision systems are trained to predict a fixed set of predeterminedobject categories. This restricted form of supervision limits their generality and usability sinceÂ â€¦",
    "cited_by_count": 46323.0
  },
  {
    "benchmark_name": "WILDS",
    "benchmark_paper": "WILDS: A Benchmark of in-the-Wild Distribution Shifts",
    "title": "Scientific discovery in the age of artificial intelligence",
    "authors": "H Wang, T Fu,Y Du,W Gao,K Huang,Z Liuâ€¦Â - Nature, 2023",
    "publication": "nature.com",
    "year": NaN,
    "url": "https://www.nature.com/articles/s41586-023-06221-2",
    "pdf_url": NaN,
    "abstract": "Artificial intelligence (AI) is being increasingly integrated into scientific discovery to augmentand accelerate research, helping scientists to generate hypotheses, design experimentsÂ â€¦",
    "cited_by_count": 1660.0
  },
  {
    "benchmark_name": "WILDS",
    "benchmark_paper": "WILDS: A Benchmark of in-the-Wild Distribution Shifts",
    "title": "The current and future state of AI interpretation of medical images",
    "authors": "P Rajpurkar,MP Lungren- New England Journal of Medicine, 2023",
    "publication": "Mass Medical Soc",
    "year": NaN,
    "url": "https://www.nejm.org/doi/full/10.1056/NEJMra2301725",
    "pdf_url": NaN,
    "abstract": "The Current and Future State of AI Interpretation of Medical Images | New England Journal ofMedicine Skip to main content The New England Journal of Medicine homepage AdvancedÂ â€¦",
    "cited_by_count": 396.0
  },
  {
    "benchmark_name": "WILDS",
    "benchmark_paper": "WILDS: A Benchmark of in-the-Wild Distribution Shifts",
    "title": "Towards a general-purpose foundation model for computational pathology",
    "authors": "RJ Chen,T Ding,MY Lu,DFK Williamson,G Jaumeâ€¦Â - Nature medicine, 2024",
    "publication": "nature.com",
    "year": NaN,
    "url": "https://www.nature.com/articles/s41591-024-02857-3",
    "pdf_url": NaN,
    "abstract": "Quantitative evaluation of tissue images is crucial for computational pathology (CPath) tasks,requiring the objective characterization of histopathological entities from whole-slide imagesÂ â€¦",
    "cited_by_count": 1031.0
  },
  {
    "benchmark_name": "WILDS",
    "benchmark_paper": "WILDS: A Benchmark of in-the-Wild Distribution Shifts",
    "title": "Holistic evaluation of language models",
    "authors": "P Liang,R Bommasani,T Lee, D Tsiprasâ€¦Â - arXiv preprint arXivÂ â€¦, 2022",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2211.09110",
    "pdf_url": NaN,
    "abstract": "Language models (LMs) are becoming the foundation for almost all major languagetechnologies, but their capabilities, limitations, and risks are not well understood. We presentÂ â€¦",
    "cited_by_count": 1905.0
  },
  {
    "benchmark_name": "WILDS",
    "benchmark_paper": "WILDS: A Benchmark of in-the-Wild Distribution Shifts",
    "title": "Datacomp: In search of the next generation of multimodal datasets",
    "authors": "SY Gadre,G Ilharco,A Fangâ€¦Â - Advances inÂ â€¦, 2023",
    "publication": "proceedings.neurips.cc",
    "year": NaN,
    "url": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/56332d41d55ad7ad8024aac625881be7-Abstract-Datasets_and_Benchmarks.html",
    "pdf_url": NaN,
    "abstract": "Multimodal datasets are a critical component in recent breakthroughs such as CLIP, StableDiffusion and GPT-4, yet their design does not receive the same research attention as modelÂ â€¦",
    "cited_by_count": 626.0
  },
  {
    "benchmark_name": "WILDS",
    "benchmark_paper": "WILDS: A Benchmark of in-the-Wild Distribution Shifts",
    "title": "DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models.",
    "authors": "B Wang,W Chen,H Pei,C Xie,M Kang,C Zhang,C Xuâ€¦Â - NeurIPS, 2023",
    "publication": "blogs.qub.ac.uk",
    "year": NaN,
    "url": "https://blogs.qub.ac.uk/wp-content/uploads/sites/7/2024/01/A-comprehensive-Assessment-of-Trustworthiness-in-GPT-Models.pdf",
    "pdf_url": NaN,
    "abstract": "Abstract Generative Pre-trained Transformer (GPT) models have exhibited exciting progressin their capabilities, capturing the interest of practitioners and the public alike. Yet, while theÂ â€¦",
    "cited_by_count": 625.0
  },
  {
    "benchmark_name": "WILDS",
    "benchmark_paper": "WILDS: A Benchmark of in-the-Wild Distribution Shifts",
    "title": "Generalized out-of-distribution detection: A survey",
    "authors": "J Yang,K Zhou,Y Li,Z Liu- International Journal of Computer Vision, 2024",
    "publication": "Springer",
    "year": NaN,
    "url": "https://link.springer.com/article/10.1007/s11263-024-02117-4",
    "pdf_url": NaN,
    "abstract": "Abstract Out-of-distribution (OOD) detection is critical to ensuring the reliability and safety ofmachine learning systems. For instance, in autonomous driving, we would like the drivingÂ â€¦",
    "cited_by_count": 1451.0
  },
  {
    "benchmark_name": "WILDS",
    "benchmark_paper": "WILDS: A Benchmark of in-the-Wild Distribution Shifts",
    "title": "Trustworthy llms: a survey and guideline for evaluating large language models' alignment",
    "authors": "Y Liu,Y Yao,JF Ton,X Zhang,R Guo,H Chengâ€¦Â - arXiv preprint arXivÂ â€¦, 2023",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2308.05374",
    "pdf_url": NaN,
    "abstract": "Ensuring alignment, which refers to making models behave in accordance with humanintentions [1, 2], has become a critical task before deploying large language models (LLMs)Â â€¦",
    "cited_by_count": 516.0
  },
  {
    "benchmark_name": "WILDS",
    "benchmark_paper": "WILDS: A Benchmark of in-the-Wild Distribution Shifts",
    "title": "Data-centric artificial intelligence: A survey",
    "authors": "D Zha, ZP Bhat,KH Lai,F Yang,Z Jiangâ€¦Â - ACM ComputingÂ â€¦, 2025",
    "publication": "dl.acm.org",
    "year": NaN,
    "url": "https://dl.acm.org/doi/abs/10.1145/3711118",
    "pdf_url": NaN,
    "abstract": "Artificial Intelligence (AI) is making a profound impact in almost every domain. A vital enablerof its great success is the availability of abundant and high-quality data for building machineÂ â€¦",
    "cited_by_count": 441.0
  },
  {
    "benchmark_name": "WILDS",
    "benchmark_paper": "WILDS: A Benchmark of in-the-Wild Distribution Shifts",
    "title": "Transfer learning in environmental remote sensing",
    "authors": "Y Ma,S Chen,S Ermon,DB Lobell- Remote Sensing of Environment, 2024",
    "publication": "Elsevier",
    "year": NaN,
    "url": "https://www.sciencedirect.com/science/article/pii/S0034425723004765",
    "pdf_url": NaN,
    "abstract": "Abstract Machine learning (ML) has proven to be a powerful tool for utilizing the rapidlyincreasing amounts of remote sensing data for environmental monitoring. Yet ML modelsÂ â€¦",
    "cited_by_count": 291.0
  },
  {
    "benchmark_name": "OoD-Bench",
    "benchmark_paper": "OoD-Bench: Quantifying and Understanding Two Dimensions of Out-of-Distribution Generalization",
    "title": "AUC maximization in the era of big data and AI: A survey",
    "authors": "T Yang,Y Ying- ACM computing surveys, 2022",
    "publication": "dl.acm.org",
    "year": NaN,
    "url": "https://dl.acm.org/doi/abs/10.1145/3554729",
    "pdf_url": NaN,
    "abstract": "Area under the ROC curve, aka AUC, is a measure of choice for assessing the performanceof a classifier for imbalanced data. AUC maximization refers to a learning paradigm thatÂ â€¦",
    "cited_by_count": 146.0
  },
  {
    "benchmark_name": "OoD-Bench",
    "benchmark_paper": "OoD-Bench: Quantifying and Understanding Two Dimensions of Out-of-Distribution Generalization",
    "title": "Physics-informed machine learning: A survey on problems, methods and applications",
    "authors": "Z Hao,S Liu,Y Zhang,C Ying,Y Feng,H Suâ€¦Â - arXiv preprint arXivÂ â€¦, 2022",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2211.08064",
    "pdf_url": NaN,
    "abstract": "Recent advances of data-driven machine learning have revolutionized fields like computervision, reinforcement learning, and many scientific and engineering domains. In many realÂ â€¦",
    "cited_by_count": 263.0
  },
  {
    "benchmark_name": "OoD-Bench",
    "benchmark_paper": "OoD-Bench: Quantifying and Understanding Two Dimensions of Out-of-Distribution Generalization",
    "title": "A survey on evaluation of out-of-distribution generalization",
    "authors": "H Yu,J Liu,X Zhang,J Wu,P Cui- arXiv preprint arXiv:2403.01874, 2024",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2403.01874",
    "pdf_url": NaN,
    "abstract": "Machine learning models, while progressively advanced, rely heavily on the IID assumption,which is often unfulfilled in practice due to inevitable distribution shifts. This renders themÂ â€¦",
    "cited_by_count": 35.0
  },
  {
    "benchmark_name": "OoD-Bench",
    "benchmark_paper": "OoD-Bench: Quantifying and Understanding Two Dimensions of Out-of-Distribution Generalization",
    "title": "A sentence speaks a thousand images: Domain generalization through distilling clip with language guidance",
    "authors": "Z Huang,A Zhou,Z Ling,M Caiâ€¦Â - Proceedings of theÂ â€¦, 2023",
    "publication": "openaccess.thecvf.com",
    "year": NaN,
    "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Huang_A_Sentence_Speaks_a_Thousand_Images_Domain_Generalization_through_Distilling_ICCV_2023_paper.html",
    "pdf_url": NaN,
    "abstract": "Abstract Domain generalization studies the problem of training a model with samples fromseveral domains (or distributions) and then testing the model with samples from a newÂ â€¦",
    "cited_by_count": 58.0
  },
  {
    "benchmark_name": "OoD-Bench",
    "benchmark_paper": "OoD-Bench: Quantifying and Understanding Two Dimensions of Out-of-Distribution Generalization",
    "title": "Improved test-time adaptation for domain generalization",
    "authors": "L Chen, Y Zhang,Y Song,Y Shanâ€¦Â - Proceedings of theÂ â€¦, 2023",
    "publication": "openaccess.thecvf.com",
    "year": NaN,
    "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Chen_Improved_Test-Time_Adaptation_for_Domain_Generalization_CVPR_2023_paper.html",
    "pdf_url": NaN,
    "abstract": "The main challenge in domain generalization (DG) is to handle the distribution shift problemthat lies between the training and test data. Recent studies suggest that test-time trainingÂ â€¦",
    "cited_by_count": 84.0
  },
  {
    "benchmark_name": "OoD-Bench",
    "benchmark_paper": "OoD-Bench: Quantifying and Understanding Two Dimensions of Out-of-Distribution Generalization",
    "title": "Coda: A real-world road corner case dataset for object detection in autonomous driving",
    "authors": "K Li,K Chen,H Wang,L Hong, C Ye,J Hanâ€¦Â - European conference onÂ â€¦, 2022",
    "publication": "Springer",
    "year": NaN,
    "url": "https://link.springer.com/chapter/10.1007/978-3-031-19839-7_24",
    "pdf_url": NaN,
    "abstract": "Contemporary deep-learning object detection methods for autonomous driving usuallypresume fixed categories of common traffic participants, such as pedestrians and cars. MostÂ â€¦",
    "cited_by_count": 156.0
  },
  {
    "benchmark_name": "OoD-Bench",
    "benchmark_paper": "OoD-Bench: Quantifying and Understanding Two Dimensions of Out-of-Distribution Generalization",
    "title": "Wild-time: A benchmark of in-the-wild distribution shift over time",
    "authors": "H Yao,C Choi,B Cao,Y Leeâ€¦Â - Advances in NeuralÂ â€¦, 2022",
    "publication": "proceedings.neurips.cc",
    "year": NaN,
    "url": "https://proceedings.neurips.cc/paper_files/paper/2022/hash/43119db5d59f07cc08fca7ba6820179a-Abstract-Datasets_and_Benchmarks.html",
    "pdf_url": NaN,
    "abstract": "Distribution shifts occur when the test distribution differs from the training distribution, andcan considerably degrade performance of machine learning models deployed in the realÂ â€¦",
    "cited_by_count": 137.0
  },
  {
    "benchmark_name": "OoD-Bench",
    "benchmark_paper": "OoD-Bench: Quantifying and Understanding Two Dimensions of Out-of-Distribution Generalization",
    "title": "Feed two birds with one scone: Exploiting wild data for both out-of-distribution generalization and detection",
    "authors": "H Bai,G Canal,X Du,J Kwonâ€¦Â - â€¦Â on Machine Learning, 2023",
    "publication": "proceedings.mlr.press",
    "year": NaN,
    "url": "https://proceedings.mlr.press/v202/bai23a.html",
    "pdf_url": NaN,
    "abstract": "Modern machine learning models deployed in the wild can encounter both covariate andsemantic shifts, giving rise to the problems of out-of-distribution (OOD) generalization andÂ â€¦",
    "cited_by_count": 65.0
  },
  {
    "benchmark_name": "OoD-Bench",
    "benchmark_paper": "OoD-Bench: Quantifying and Understanding Two Dimensions of Out-of-Distribution Generalization",
    "title": "Sparse invariant risk minimization",
    "authors": "X Zhou,Y Lin,W Zhangâ€¦Â - â€¦Â Conference on MachineÂ â€¦, 2022",
    "publication": "proceedings.mlr.press",
    "year": NaN,
    "url": "https://proceedings.mlr.press/v162/zhou22e.html",
    "pdf_url": NaN,
    "abstract": "Abstract Invariant Risk Minimization (IRM) is an emerging invariant feature extractingtechnique to help generalization with distributional shift. However, we find that there exists aÂ â€¦",
    "cited_by_count": 92.0
  },
  {
    "benchmark_name": "OoD-Bench",
    "benchmark_paper": "OoD-Bench: Quantifying and Understanding Two Dimensions of Out-of-Distribution Generalization",
    "title": "Domain generalization via rationale invariance",
    "authors": "L Chen, Y Zhang,Y Songâ€¦Â - Proceedings of theÂ â€¦, 2023",
    "publication": "openaccess.thecvf.com",
    "year": NaN,
    "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Chen_Domain_Generalization_via_Rationale_Invariance_ICCV_2023_paper.html",
    "pdf_url": NaN,
    "abstract": "This paper offers a new perspective to ease the challenge of domain generalization, whichinvolves maintaining robust results even in unseen environments. Our design focuses on theÂ â€¦",
    "cited_by_count": 44.0
  },
  {
    "benchmark_name": "SoK: Certified Robustness",
    "benchmark_paper": "SoK: Certified Robustness for Deep Neural Networks",
    "title": "Security and privacy challenges of large language models: A survey",
    "authors": "BC Das,MH Amini,Y Wu- ACM Computing Surveys, 2025",
    "publication": "dl.acm.org",
    "year": NaN,
    "url": "https://dl.acm.org/doi/abs/10.1145/3712001",
    "pdf_url": NaN,
    "abstract": "Large language models (LLMs) have demonstrated extraordinary capabilities andcontributed to multiple fields, such as generating and summarizing text, languageÂ â€¦",
    "cited_by_count": 388.0
  },
  {
    "benchmark_name": "SoK: Certified Robustness",
    "benchmark_paper": "SoK: Certified Robustness for Deep Neural Networks",
    "title": "Universal and transferable adversarial attacks on aligned language models",
    "authors": "A Zou,Z Wang,N Carlini,M Nasr,JZ Kolterâ€¦Â - arXiv preprint arXivÂ â€¦, 2023",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2307.15043",
    "pdf_url": NaN,
    "abstract": "Because\" out-of-the-box\" large language models are capable of generating a great deal ofobjectionable content, recent work has focused on aligning these models in an attempt toÂ â€¦",
    "cited_by_count": 2181.0
  },
  {
    "benchmark_name": "SoK: Certified Robustness",
    "benchmark_paper": "SoK: Certified Robustness for Deep Neural Networks",
    "title": "Visual adversarial examples jailbreak aligned large language models",
    "authors": "X Qi,K Huang,A Panda,P Hendersonâ€¦Â - Proceedings of theÂ â€¦, 2024",
    "publication": "ojs.aaai.org",
    "year": NaN,
    "url": "https://ojs.aaai.org/index.php/AAAI/article/view/30150",
    "pdf_url": NaN,
    "abstract": "Warning: this paper contains data, prompts, and model outputs that are offensive in nature.Recently, there has been a surge of interest in integrating vision into Large LanguageÂ â€¦",
    "cited_by_count": 331.0
  },
  {
    "benchmark_name": "SoK: Certified Robustness",
    "benchmark_paper": "SoK: Certified Robustness for Deep Neural Networks",
    "title": "Adversarial machine learning: a review of methods, tools, and critical industry sectors.",
    "authors": "S Pelekis,T Koutroubas,A Blikaâ€¦Â - Artificial IntelligenceÂ â€¦, 2025",
    "publication": "drive.google.com",
    "year": NaN,
    "url": "https://drive.google.com/file/d/1N1s5ndgZkIXhlJeo4kisJ1ey2Fw6_zHY/view",
    "pdf_url": NaN,
    "abstract": "The rapid advancement of Artificial Intelligence (AI), particularly Machine Learning (ML) andDeep Learning (DL), has produced high-performance models widely used in variousÂ â€¦",
    "cited_by_count": 14.0
  },
  {
    "benchmark_name": "SoK: Certified Robustness",
    "benchmark_paper": "SoK: Certified Robustness for Deep Neural Networks",
    "title": "Invisible for both camera and lidar: Security of multi-sensor fusion based perception in autonomous driving under physical-world attacks",
    "authors": "Y Cao,N Wang,C Xiao, D Yang,J Fangâ€¦Â - â€¦Â IEEE symposium onÂ â€¦, 2021",
    "publication": "ieeexplore.ieee.org",
    "year": NaN,
    "url": "https://ieeexplore.ieee.org/abstract/document/9519442/",
    "pdf_url": NaN,
    "abstract": "In Autonomous Driving (AD) systems, perception is both security and safety critical. Despitevarious prior studies on its security issues, all of them only consider attacks on camera-orÂ â€¦",
    "cited_by_count": 356.0
  },
  {
    "benchmark_name": "SoK: Certified Robustness",
    "benchmark_paper": "SoK: Certified Robustness for Deep Neural Networks",
    "title": "Attention-enhancing backdoor attacks against bert-based models",
    "authors": "W Lyu,S Zheng,L Pang,H Ling,C Chen- arXiv preprint arXiv:2310.14480, 2023",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2310.14480",
    "pdf_url": NaN,
    "abstract": "Recent studies have revealed that\\textit {Backdoor Attacks} can threaten the safety of naturallanguage processing (NLP) models. Investigating the strategies of backdoor attacks will helpÂ â€¦",
    "cited_by_count": 82.0
  },
  {
    "benchmark_name": "SoK: Certified Robustness",
    "benchmark_paper": "SoK: Certified Robustness for Deep Neural Networks",
    "title": "Adversarial robustness of deep neural networks: A survey from a formal verification perspective",
    "authors": "MH Meng,G Bai,SG Teo, Z Hou,Y Xiaoâ€¦Â - â€¦Â on Dependable andÂ â€¦, 2022",
    "publication": "ieeexplore.ieee.org",
    "year": NaN,
    "url": "https://ieeexplore.ieee.org/abstract/document/9785704/",
    "pdf_url": NaN,
    "abstract": "Neural networks have been widely applied in security applications such as spam andphishing detection, intrusion prevention, and malware detection. This black-box methodÂ â€¦",
    "cited_by_count": 109.0
  },
  {
    "benchmark_name": "SoK: Certified Robustness",
    "benchmark_paper": "SoK: Certified Robustness for Deep Neural Networks",
    "title": "Rab: Provable robustness against backdoor attacks",
    "authors": "M Weber,X Xu,B KarlaÅ¡,C Zhangâ€¦Â - 2023 IEEE SymposiumÂ â€¦, 2023",
    "publication": "ieeexplore.ieee.org",
    "year": NaN,
    "url": "https://ieeexplore.ieee.org/abstract/document/10179451/",
    "pdf_url": NaN,
    "abstract": "Recent studies have shown that deep neural net-works (DNNs) are vulnerable toadversarial attacks, including evasion and backdoor (poisoning) attacks. On the defenseÂ â€¦",
    "cited_by_count": 223.0
  },
  {
    "benchmark_name": "SoK: Certified Robustness",
    "benchmark_paper": "SoK: Certified Robustness for Deep Neural Networks",
    "title": "A comprehensive survey of robust deep learning in computer vision",
    "authors": "J Liu,Y Jin- Journal of Automation and Intelligence, 2023",
    "publication": "Elsevier",
    "year": NaN,
    "url": "https://www.sciencedirect.com/science/article/pii/S294985542300045X",
    "pdf_url": NaN,
    "abstract": "Deep learning has presented remarkable progress in various tasks. Despite the excellentperformance, deep learning models remain not robust, especially to well-designedÂ â€¦",
    "cited_by_count": 42.0
  },
  {
    "benchmark_name": "SoK: Certified Robustness",
    "benchmark_paper": "SoK: Certified Robustness for Deep Neural Networks",
    "title": "Text-crs: A generalized certified robustness framework against textual adversarial attacks",
    "authors": "X Zhang,H Hong,Y Hong,P Huangâ€¦Â - â€¦Â IEEE Symposium onÂ â€¦, 2024",
    "publication": "ieeexplore.ieee.org",
    "year": NaN,
    "url": "https://ieeexplore.ieee.org/abstract/document/10646716/",
    "pdf_url": NaN,
    "abstract": "The language models, especially the basic text classification models, have been shown tobe susceptible to textual adversarial attacks such as synonym substitution and wordÂ â€¦",
    "cited_by_count": 37.0
  },
  {
    "benchmark_name": "Adversarial Robustness Benchmark",
    "benchmark_paper": "Benchmarking Adversarial Robustness on Image Classification",
    "title": "Data-centric artificial intelligence: A survey",
    "authors": "D Zha, ZP Bhat,KH Lai,F Yang,Z Jiangâ€¦Â - ACM ComputingÂ â€¦, 2025",
    "publication": "dl.acm.org",
    "year": NaN,
    "url": "https://dl.acm.org/doi/abs/10.1145/3711118",
    "pdf_url": NaN,
    "abstract": "Artificial Intelligence (AI) is making a profound impact in almost every domain. A vital enablerof its great success is the availability of abundant and high-quality data for building machineÂ â€¦",
    "cited_by_count": 441.0
  },
  {
    "benchmark_name": "Adversarial Robustness Benchmark",
    "benchmark_paper": "Benchmarking Adversarial Robustness on Image Classification",
    "title": "Toward the third generation artificial intelligence",
    "authors": "B Zhang,J Zhu,H Su- Science China Information Sciences, 2023",
    "publication": "Springer",
    "year": NaN,
    "url": "https://link.springer.com/article/10.1007/s11432-021-3449-x",
    "pdf_url": NaN,
    "abstract": "There have been two competing paradigms in artificial intelligence (AI) development eversince its birth in 1956, ie, symbolism and connectionism (or sub-symbolism). WhileÂ â€¦",
    "cited_by_count": 346.0
  },
  {
    "benchmark_name": "Adversarial Robustness Benchmark",
    "benchmark_paper": "Benchmarking Adversarial Robustness on Image Classification",
    "title": "Better diffusion models further improve adversarial training",
    "authors": "Z Wang,T Pang,C Du,M Linâ€¦Â - â€¦Â on machine learning, 2023",
    "publication": "proceedings.mlr.press",
    "year": NaN,
    "url": "http://proceedings.mlr.press/v202/wang23ad.html",
    "pdf_url": NaN,
    "abstract": "It has been recognized that the data generated by the denoising diffusion probabilisticmodel (DDPM) improves adversarial training. After two years of rapid development inÂ â€¦",
    "cited_by_count": 347.0
  },
  {
    "benchmark_name": "Adversarial Robustness Benchmark",
    "benchmark_paper": "Benchmarking Adversarial Robustness on Image Classification",
    "title": "Robustbench: a standardized adversarial robustness benchmark",
    "authors": "F Croce,M Andriushchenko,V Sehwagâ€¦Â - arXiv preprint arXivÂ â€¦, 2020",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2010.09670",
    "pdf_url": NaN,
    "abstract": "As a research community, we are still lacking a systematic understanding of the progress onadversarial robustness which often makes it hard to identify the most promising ideas inÂ â€¦",
    "cited_by_count": 989.0
  },
  {
    "benchmark_name": "Adversarial Robustness Benchmark",
    "benchmark_paper": "Benchmarking Adversarial Robustness on Image Classification",
    "title": "Interpreting adversarial examples in deep learning: A review",
    "authors": "S Han,C Lin,C Shen,Q Wang, X GuanÂ - ACM Computing Surveys, 2023",
    "publication": "dl.acm.org",
    "year": NaN,
    "url": "https://dl.acm.org/doi/abs/10.1145/3594869",
    "pdf_url": NaN,
    "abstract": "Deep learning technology is increasingly being applied in safety-critical scenarios but hasrecently been found to be susceptible to imperceptible adversarial perturbations. This raisesÂ â€¦",
    "cited_by_count": 102.0
  },
  {
    "benchmark_name": "Adversarial Robustness Benchmark",
    "benchmark_paper": "Benchmarking Adversarial Robustness on Image Classification",
    "title": "Robustness in deep learning models for medical diagnostics: security and adversarial challenges towards robust AI applications",
    "authors": "H Javed,S El-Sappagh,T Abuhmed- Artificial Intelligence Review, 2024",
    "publication": "Springer",
    "year": NaN,
    "url": "https://link.springer.com/article/10.1007/s10462-024-11005-9",
    "pdf_url": NaN,
    "abstract": "The current study investigates the robustness of deep learning models for accurate medicaldiagnosis systems with a specific focus on their ability to maintain performance in theÂ â€¦",
    "cited_by_count": 85.0
  },
  {
    "benchmark_name": "Adversarial Robustness Benchmark",
    "benchmark_paper": "Benchmarking Adversarial Robustness on Image Classification",
    "title": "Backdoorbench: A comprehensive benchmark of backdoor learning",
    "authors": "B Wu,H Chen,M Zhang,Z Zhu,S Weiâ€¦Â - Advances inÂ â€¦, 2022",
    "publication": "proceedings.neurips.cc",
    "year": NaN,
    "url": "https://proceedings.neurips.cc/paper_files/paper/2022/hash/4491ea1c91aa2b22c373e5f1dfce234f-Abstract-Datasets_and_Benchmarks.html",
    "pdf_url": NaN,
    "abstract": "Backdoor learning is an emerging and vital topic for studying deep neural networks'vulnerability (DNNs). Many pioneering backdoor attack and defense methods are beingÂ â€¦",
    "cited_by_count": 205.0
  },
  {
    "benchmark_name": "Adversarial Robustness Benchmark",
    "benchmark_paper": "Benchmarking Adversarial Robustness on Image Classification",
    "title": "A comprehensive study on robustness of image classification models: Benchmarking and rethinking",
    "authors": "C Liu,Y Dong,W Xiang,X Yang,H Su,J Zhuâ€¦Â - International Journal ofÂ â€¦, 2025",
    "publication": "Springer",
    "year": NaN,
    "url": "https://link.springer.com/article/10.1007/s11263-024-02196-3",
    "pdf_url": NaN,
    "abstract": "The robustness of deep neural networks is frequently compromised when faced withadversarial examples, common corruptions, and distribution shifts, posing a significantÂ â€¦",
    "cited_by_count": 134.0
  },
  {
    "benchmark_name": "Adversarial Robustness Benchmark",
    "benchmark_paper": "Benchmarking Adversarial Robustness on Image Classification",
    "title": "On adversarial robustness of trajectory prediction for autonomous vehicles",
    "authors": "Q Zhang,S Hu,J Sun,QA Chenâ€¦Â - Proceedings of theÂ â€¦, 2022",
    "publication": "openaccess.thecvf.com",
    "year": NaN,
    "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Zhang_On_Adversarial_Robustness_of_Trajectory_Prediction_for_Autonomous_Vehicles_CVPR_2022_paper.html",
    "pdf_url": NaN,
    "abstract": "Trajectory prediction is a critical component for autonomous vehicles (AVs) to perform safeplanning and navigation. However, few studies have analyzed the adversarial robustness ofÂ â€¦",
    "cited_by_count": 209.0
  },
  {
    "benchmark_name": "Adversarial Robustness Benchmark",
    "benchmark_paper": "Benchmarking Adversarial Robustness on Image Classification",
    "title": "Understanding the robustness of 3D object detection with bird's-eye-view representations in autonomous driving",
    "authors": "Z Zhu,Y Zhang,H Chen,Y Dongâ€¦Â - Proceedings of theÂ â€¦, 2023",
    "publication": "openaccess.thecvf.com",
    "year": NaN,
    "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Zhu_Understanding_the_Robustness_of_3D_Object_Detection_With_Birds-Eye-View_Representations_CVPR_2023_paper.html",
    "pdf_url": NaN,
    "abstract": "Abstract 3D object detection is an essential perception task in autonomous driving tounderstand the environments. The Bird's-Eye-View (BEV) representations have significantlyÂ â€¦",
    "cited_by_count": 86.0
  },
  {
    "benchmark_name": "GRB",
    "benchmark_paper": "Graph Robustness Benchmark: Benchmarking the Adversarial Robustness of Graph Machine Learning",
    "title": "Evaluating explainability for graph neural networks",
    "authors": "C Agarwal,O Queen,H Lakkaraju,M Zitnik- Scientific Data, 2023",
    "publication": "nature.com",
    "year": NaN,
    "url": "https://www.nature.com/articles/s41597-023-01974-x",
    "pdf_url": NaN,
    "abstract": "As explanations are increasingly used to understand the behavior of graph neural networks(GNNs), evaluating the quality and reliability of GNN explanations is crucial. HoweverÂ â€¦",
    "cited_by_count": 214.0
  },
  {
    "benchmark_name": "GRB",
    "benchmark_paper": "Graph Robustness Benchmark: Benchmarking the Adversarial Robustness of Graph Machine Learning",
    "title": "AI robustness: a human-centered perspective on technological challenges and opportunities",
    "authors": "A Tocchetti,L Corti,A Balayn,M Yurritaâ€¦Â - ACM ComputingÂ â€¦, 2025",
    "publication": "dl.acm.org",
    "year": NaN,
    "url": "https://dl.acm.org/doi/abs/10.1145/3665926",
    "pdf_url": NaN,
    "abstract": "Despite the impressive performance of Artificial Intelligence (AI) systems, their robustnessremains elusive and constitutes a key issue that impedes large-scale adoption. BesidesÂ â€¦",
    "cited_by_count": 50.0
  },
  {
    "benchmark_name": "GRB",
    "benchmark_paper": "Graph Robustness Benchmark: Benchmarking the Adversarial Robustness of Graph Machine Learning",
    "title": "Trustworthy graph neural networks: Aspects, methods, and trends",
    "authors": "H Zhang,B Wu,X Yuan,S Pan,H Tongâ€¦Â - Proceedings of theÂ â€¦, 2024",
    "publication": "ieeexplore.ieee.org",
    "year": NaN,
    "url": "https://ieeexplore.ieee.org/abstract/document/10477407/",
    "pdf_url": NaN,
    "abstract": "Graph neural networks (GNNs) have emerged as a series of competent graph learningmethods for diverse real-world scenarios, ranging from daily applications such asÂ â€¦",
    "cited_by_count": 189.0
  },
  {
    "benchmark_name": "GRB",
    "benchmark_paper": "Graph Robustness Benchmark: Benchmarking the Adversarial Robustness of Graph Machine Learning",
    "title": "Adversarial attack and defense on graph data: A survey",
    "authors": "L Sun,Y Dou,C Yang,K Zhang,J Wangâ€¦Â - â€¦Â on Knowledge andÂ â€¦, 2022",
    "publication": "ieeexplore.ieee.org",
    "year": NaN,
    "url": "https://ieeexplore.ieee.org/abstract/document/9878092/",
    "pdf_url": NaN,
    "abstract": "Deep neural networks (DNNs) have been widely applied to various applications, includingimage classification, text generation, audio recognition, and graph data analysis. HoweverÂ â€¦",
    "cited_by_count": 467.0
  },
  {
    "benchmark_name": "GRB",
    "benchmark_paper": "Graph Robustness Benchmark: Benchmarking the Adversarial Robustness of Graph Machine Learning",
    "title": "Bond: Benchmarking unsupervised outlier node detection on static attributed graphs",
    "authors": "K Liu,Y Dou,Y Zhao,X Ding,X Huâ€¦Â - Advances inÂ â€¦, 2022",
    "publication": "proceedings.neurips.cc",
    "year": NaN,
    "url": "https://proceedings.neurips.cc/paper_files/paper/2022/hash/acc1ec4a9c780006c9aafd595104816b-Abstract-Datasets_and_Benchmarks.html",
    "pdf_url": NaN,
    "abstract": "Detecting which nodes in graphs are outliers is a relatively new machine learning task withnumerous applications. Despite the proliferation of algorithms developed in recent years forÂ â€¦",
    "cited_by_count": 146.0
  },
  {
    "benchmark_name": "GRB",
    "benchmark_paper": "Graph Robustness Benchmark: Benchmarking the Adversarial Robustness of Graph Machine Learning",
    "title": "Are defenses for graph neural networks robust?",
    "authors": "F Mujkanovic,S Geislerâ€¦Â - Advances in NeuralÂ â€¦, 2022",
    "publication": "proceedings.neurips.cc",
    "year": NaN,
    "url": "https://proceedings.neurips.cc/paper_files/paper/2022/hash/3ac904a31f9141444009777abef2ed8e-Abstract-Conference.html",
    "pdf_url": NaN,
    "abstract": "A cursory reading of the literature suggests that we have made a lot of progress in designingeffective adversarial defenses for Graph Neural Networks (GNNs). Yet, the standardÂ â€¦",
    "cited_by_count": 99.0
  },
  {
    "benchmark_name": "GRB",
    "benchmark_paper": "Graph Robustness Benchmark: Benchmarking the Adversarial Robustness of Graph Machine Learning",
    "title": "Adversarial robustness in graph neural networks: A hamiltonian approach",
    "authors": "K Zhao,Q Kang,Y Song,R Sheâ€¦Â - Advances in NeuralÂ â€¦, 2023",
    "publication": "proceedings.neurips.cc",
    "year": NaN,
    "url": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/0a443a000e1cb2281480b3bac395b3b8-Abstract-Conference.html",
    "pdf_url": NaN,
    "abstract": "Graph neural networks (GNNs) are vulnerable to adversarial perturbations, including thosethat affect both node features and graph topology. This paper investigates GNNs derivedÂ â€¦",
    "cited_by_count": 44.0
  },
  {
    "benchmark_name": "GRB",
    "benchmark_paper": "Graph Robustness Benchmark: Benchmarking the Adversarial Robustness of Graph Machine Learning",
    "title": "Adversarial training for graph neural networks: Pitfalls, solutions, and new directions",
    "authors": "L Gosch,S Geisler, D Sturmâ€¦Â - Advances in neuralÂ â€¦, 2023",
    "publication": "proceedings.neurips.cc",
    "year": NaN,
    "url": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/b5a801e6bc4f4ffa3e6786518a324488-Abstract-Conference.html",
    "pdf_url": NaN,
    "abstract": "Despite its success in the image domain, adversarial training did not (yet) stand out as aneffective defense for Graph Neural Networks (GNNs) against graph structure perturbationsÂ â€¦",
    "cited_by_count": 55.0
  },
  {
    "benchmark_name": "GRB",
    "benchmark_paper": "Graph Robustness Benchmark: Benchmarking the Adversarial Robustness of Graph Machine Learning",
    "title": "A comprehensive study on text-attributed graphs: Benchmarking and rethinking",
    "authors": "H Yan,C Li, R Long, C Yan,J Zhaoâ€¦Â - Advances inÂ â€¦, 2023",
    "publication": "proceedings.neurips.cc",
    "year": NaN,
    "url": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/37d00f567a18b478065f1a91b95622a0-Abstract-Datasets_and_Benchmarks.html",
    "pdf_url": NaN,
    "abstract": "Text-attributed graphs (TAGs) are prevalent in various real-world scenarios, where eachnode is associated with a text description. The cornerstone of representation learning onÂ â€¦",
    "cited_by_count": 41.0
  },
  {
    "benchmark_name": "GRB",
    "benchmark_paper": "Graph Robustness Benchmark: Benchmarking the Adversarial Robustness of Graph Machine Learning",
    "title": "Can large language models improve the adversarial robustness of graph neural networks?",
    "authors": "Z Zhang,X Wang,H Zhou,Y Yu, M Zhangâ€¦Â - Proceedings of the 31stÂ â€¦, 2025",
    "publication": "dl.acm.org",
    "year": NaN,
    "url": "https://dl.acm.org/doi/abs/10.1145/3690624.3709256",
    "pdf_url": NaN,
    "abstract": "Graph neural networks (GNNs) are vulnerable to adversarial attacks, especially for topologyperturbations, and many methods that improve the robustness of GNNs have receivedÂ â€¦",
    "cited_by_count": 24.0
  },
  {
    "benchmark_name": "RealToxicityPrompts",
    "benchmark_paper": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models",
    "title": "A survey on evaluation of large language models",
    "authors": "Y Chang,X Wang,J Wang,Y Wu,L Yangâ€¦Â - ACM transactions onÂ â€¦, 2024",
    "publication": "dl.acm.org",
    "year": NaN,
    "url": "https://dl.acm.org/doi/abs/10.1145/3641289",
    "pdf_url": NaN,
    "abstract": "Large language models (LLMs) are gaining increasing popularity in both academia andindustry, owing to their unprecedented performance in various applications. As LLMsÂ â€¦",
    "cited_by_count": 4405.0
  },
  {
    "benchmark_name": "RealToxicityPrompts",
    "benchmark_paper": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models",
    "title": "A survey on large language model (llm) security and privacy: The good, the bad, and the ugly",
    "authors": "Y Yao,J Duan,K Xu, Y Cai,Z Sun,Y Zhang- High-Confidence Computing, 2024",
    "publication": "Elsevier",
    "year": NaN,
    "url": "https://www.sciencedirect.com/science/article/pii/S266729522400014X",
    "pdf_url": NaN,
    "abstract": "Abstract Large Language Models (LLMs), such as ChatGPT and Bard, have revolutionizednatural language understanding and generation. They possess deep languageÂ â€¦",
    "cited_by_count": 1335.0
  },
  {
    "benchmark_name": "RealToxicityPrompts",
    "benchmark_paper": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models",
    "title": "Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context",
    "authors": "G Team,P Georgiev, VI Lei, R Burnell, L Baiâ€¦Â - arXiv preprint arXivÂ â€¦, 2024",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2403.05530",
    "pdf_url": NaN,
    "abstract": "In this report, we introduce the Gemini 1.5 family of models, representing the next generationof highly compute-efficient multimodal models capable of recalling and reasoning over fineÂ â€¦",
    "cited_by_count": 2793.0
  },
  {
    "benchmark_name": "RealToxicityPrompts",
    "benchmark_paper": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models",
    "title": "A comprehensive overview of large language models",
    "authors": "H Naveed,AU Khan,S Qiu,M Saqib,S Anwarâ€¦Â - ACM Transactions onÂ â€¦, 2025",
    "publication": "dl.acm.org",
    "year": NaN,
    "url": "https://dl.acm.org/doi/abs/10.1145/3744746",
    "pdf_url": NaN,
    "abstract": "Large Language Models (LLMs) have recently demonstrated remarkable capabilities innatural language processing tasks and beyond. This success of LLMs has led to a largeÂ â€¦",
    "cited_by_count": 1815.0
  },
  {
    "benchmark_name": "RealToxicityPrompts",
    "benchmark_paper": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models",
    "title": "Jailbroken: How does llm safety training fail?",
    "authors": "A Wei,N Haghtalabâ€¦Â - Advances in NeuralÂ â€¦, 2023",
    "publication": "proceedings.neurips.cc",
    "year": NaN,
    "url": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/fd6613131889a4b656206c50a8bd7790-Abstract-Conference.html",
    "pdf_url": NaN,
    "abstract": "Large language models trained for safety and harmlessness remain susceptible toadversarial misuse, as evidenced by the prevalence of â€œjailbreakâ€ attacks on early releasesÂ â€¦",
    "cited_by_count": 1426.0
  },
  {
    "benchmark_name": "RealToxicityPrompts",
    "benchmark_paper": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models",
    "title": "Palm 2 technical report",
    "authors": "R Anil,AM Dai,O Firat,M Johnson,D Lepikhinâ€¦Â - arXiv preprint arXivÂ â€¦, 2023",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2305.10403",
    "pdf_url": NaN,
    "abstract": "We introduce PaLM 2, a new state-of-the-art language model that has better multilingual andreasoning capabilities and is more compute-efficient than its predecessor PaLM. PaLM 2 isÂ â€¦",
    "cited_by_count": 2120.0
  },
  {
    "benchmark_name": "RealToxicityPrompts",
    "benchmark_paper": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models",
    "title": "A survey of large language models",
    "authors": "WX Zhao,K Zhou,J Li,T Tang,X Wangâ€¦Â - arXiv preprint arXivÂ â€¦, 2023",
    "publication": "researchgate.net",
    "year": NaN,
    "url": "https://www.researchgate.net/profile/Tang-Tianyi-3/publication/369740832_A_Survey_of_Large_Language_Models/links/665fd2e3637e4448a37dd281/A-Survey-of-Large-Language-Models.pdf",
    "pdf_url": NaN,
    "abstract": "Ever since the Turing Test was proposed in the 1950s, humans have explored the masteringof language intelligence by machine. Language is essentially a complex, intricate system ofÂ â€¦",
    "cited_by_count": 6302.0
  },
  {
    "benchmark_name": "RealToxicityPrompts",
    "benchmark_paper": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models",
    "title": "Gpt-4 technical report",
    "authors": "J Achiam,S Adler,S Agarwal,L Ahmadâ€¦Â - arXiv preprint arXivÂ â€¦, 2023",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2303.08774",
    "pdf_url": NaN,
    "abstract": "We report the development of GPT-4, a large-scale, multimodal model which can acceptimage and text inputs and produce text outputs. While less capable than humans in manyÂ â€¦",
    "cited_by_count": 19617.0
  },
  {
    "benchmark_name": "RealToxicityPrompts",
    "benchmark_paper": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models",
    "title": "Starcoder: may the source be with you!",
    "authors": "R Li,LB Allal,Y Zi,N Muennighoff,D Kocetkovâ€¦Â - arXiv preprint arXivÂ â€¦, 2023",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2305.06161",
    "pdf_url": NaN,
    "abstract": "The BigCode community, an open-scientific collaboration working on the responsibledevelopment of Large Language Models for Code (Code LLMs), introduces StarCoder andÂ â€¦",
    "cited_by_count": 1296.0
  },
  {
    "benchmark_name": "RealToxicityPrompts",
    "benchmark_paper": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models",
    "title": "Holistic evaluation of language models",
    "authors": "P Liang,R Bommasani,T Lee, D Tsiprasâ€¦Â - arXiv preprint arXivÂ â€¦, 2022",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2211.09110",
    "pdf_url": NaN,
    "abstract": "Language models (LMs) are becoming the foundation for almost all major languagetechnologies, but their capabilities, limitations, and risks are not well understood. We presentÂ â€¦",
    "cited_by_count": 1905.0
  },
  {
    "benchmark_name": "SafetyBench",
    "benchmark_paper": "SafetyBench: Evaluating the Safety of Large Language Models with Multiple Choice Questions",
    "title": "Igniting language intelligence: The hitchhiker's guide from chain-of-thought reasoning to language agents",
    "authors": "Z Zhang,Y Yao,A Zhang,X Tang,X Ma,Z Heâ€¦Â - ACM ComputingÂ â€¦, 2025",
    "publication": "dl.acm.org",
    "year": NaN,
    "url": "https://dl.acm.org/doi/abs/10.1145/3719341",
    "pdf_url": NaN,
    "abstract": "Large language models (LLMs) have dramatically enhanced the field of languageintelligence, as demonstrably evidenced by their formidable empirical performance across aÂ â€¦",
    "cited_by_count": 100.0
  },
  {
    "benchmark_name": "SafetyBench",
    "benchmark_paper": "SafetyBench: Evaluating the Safety of Large Language Models with Multiple Choice Questions",
    "title": "A comprehensive survey in llm (-agent) full stack safety: Data, training and deployment",
    "authors": "K Wang,G Zhang,Z Zhou,J Wu,M Yu,S Zhaoâ€¦Â - arXiv preprint arXivÂ â€¦, 2025",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2504.15585",
    "pdf_url": NaN,
    "abstract": "The remarkable success of Large Language Models (LLMs) has illuminated a promisingpathway toward achieving Artificial General Intelligence for both academic and industrialÂ â€¦",
    "cited_by_count": 61.0
  },
  {
    "benchmark_name": "SafetyBench",
    "benchmark_paper": "SafetyBench: Evaluating the Safety of Large Language Models with Multiple Choice Questions",
    "title": "The art of saying no: Contextual noncompliance in language models",
    "authors": "F Brahman,S Kumar,V Balachandranâ€¦Â - Advances inÂ â€¦, 2024",
    "publication": "proceedings.neurips.cc",
    "year": NaN,
    "url": "https://proceedings.neurips.cc/paper_files/paper/2024/hash/58e79894267cf72c66202228ad9c6057-Abstract-Datasets_and_Benchmarks_Track.html",
    "pdf_url": NaN,
    "abstract": "Chat-based language models are designed to be helpful, yet they should not comply withevery user request. While most existing work primarily focuses on refusal of``unsafe''queriesÂ â€¦",
    "cited_by_count": 58.0
  },
  {
    "benchmark_name": "SafetyBench",
    "benchmark_paper": "SafetyBench: Evaluating the Safety of Large Language Models with Multiple Choice Questions",
    "title": "Safetyprompts: a systematic review of open datasets for evaluating and improving large language model safety",
    "authors": "P RÃ¶ttger,F Pernisi,B Vidgen,D Hovy- Proceedings of the AAAIÂ â€¦, 2025",
    "publication": "ojs.aaai.org",
    "year": NaN,
    "url": "https://ojs.aaai.org/index.php/AAAI/article/view/34975",
    "pdf_url": NaN,
    "abstract": "The last two years have seen a rapid growth in concerns around the safety of largelanguage models (LLMs). Researchers and practitioners have met these concerns byÂ â€¦",
    "cited_by_count": 52.0
  },
  {
    "benchmark_name": "SafetyBench",
    "benchmark_paper": "SafetyBench: Evaluating the Safety of Large Language Models with Multiple Choice Questions",
    "title": "Agent-safetybench: Evaluating the safety of llm agents",
    "authors": "Z Zhang,S Cui,Y Lu, J Zhou,J Yang,H Wangâ€¦Â - arXiv preprint arXivÂ â€¦, 2024",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2412.14470",
    "pdf_url": NaN,
    "abstract": "As large language models (LLMs) are increasingly deployed as agents, their integration intointeractive environments and tool use introduce new safety challenges beyond thoseÂ â€¦",
    "cited_by_count": 52.0
  },
  {
    "benchmark_name": "SafetyBench",
    "benchmark_paper": "SafetyBench: Evaluating the Safety of Large Language Models with Multiple Choice Questions",
    "title": "On the trustworthiness of generative foundation models: Guideline, assessment, and perspective",
    "authors": "Y Huang,C Gao, S Wu,H Wang,X Wangâ€¦Â - arXiv preprint arXivÂ â€¦, 2025",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2502.14296",
    "pdf_url": NaN,
    "abstract": "Generative Foundation Models (GenFMs) have emerged as transformative tools. However,their widespread adoption raises critical concerns regarding trustworthiness acrossÂ â€¦",
    "cited_by_count": 32.0
  },
  {
    "benchmark_name": "SafetyBench",
    "benchmark_paper": "SafetyBench: Evaluating the Safety of Large Language Models with Multiple Choice Questions",
    "title": "Safety at scale: A comprehensive survey of large model safety",
    "authors": "X Ma, Y Gao,Y Wang,R Wang,X Wang,Y Sunâ€¦Â - arXiv preprint arXivÂ â€¦, 2025",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2502.05206",
    "pdf_url": NaN,
    "abstract": "The rapid advancement of large models, driven by their exceptional abilities in learning andgeneralization through large-scale pre-training, has reshaped the landscape of ArtificialÂ â€¦",
    "cited_by_count": 37.0
  },
  {
    "benchmark_name": "SafetyBench",
    "benchmark_paper": "SafetyBench: Evaluating the Safety of Large Language Models with Multiple Choice Questions",
    "title": "Big5-chat: Shaping llm personalities through training on human-grounded data",
    "authors": "W Li,J Liu,A Liu,X Zhou,M Diab,M Sap- arXiv preprint arXiv:2410.16491, 2024",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2410.16491",
    "pdf_url": NaN,
    "abstract": "In this work, we tackle the challenge of embedding realistic human personality traits intoLLMs. Previous approaches have primarily focused on prompt-based methods that describeÂ â€¦",
    "cited_by_count": 24.0
  },
  {
    "benchmark_name": "SafetyBench",
    "benchmark_paper": "SafetyBench: Evaluating the Safety of Large Language Models with Multiple Choice Questions",
    "title": "Machine Against the {RAG}: Jamming {Retrieval-Augmented} Generation with Blocker Documents",
    "authors": "A Shafran,R Schuster,V Shmatikov- 34th USENIX Security SymposiumÂ â€¦, 2025",
    "publication": "usenix.org",
    "year": NaN,
    "url": "https://www.usenix.org/conference/usenixsecurity25/presentation/shafran",
    "pdf_url": NaN,
    "abstract": "Retrieval-augmented generation (RAG) systems respond to queries by retrieving relevantdocuments from a knowledge database and applying an LLM to the retrieved documentsÂ â€¦",
    "cited_by_count": 15.0
  },
  {
    "benchmark_name": "SafetyBench",
    "benchmark_paper": "SafetyBench: Evaluating the Safety of Large Language Models with Multiple Choice Questions",
    "title": "Toward generalizable evaluation in the llm era: A survey beyond benchmarks",
    "authors": "Y Cao, S Hong,X Li,J Ying,Y Ma, H Liangâ€¦Â - arXiv preprint arXivÂ â€¦, 2025",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2504.18838",
    "pdf_url": NaN,
    "abstract": "Large Language Models (LLMs) are advancing at an amazing speed and have becomeindispensable across academia, industry, and daily applications. To keep pace with theÂ â€¦",
    "cited_by_count": 19.0
  },
  {
    "benchmark_name": "ToxiGen",
    "benchmark_paper": "ToxiGen: A Large-Scale Machine-Generated Dataset for Adversarial and Implicit Hate Speech Detection",
    "title": "A survey of GPT-3 family large language models including ChatGPT and GPT-4",
    "authors": "KS Kalyan- Natural Language Processing Journal, 2024",
    "publication": "Elsevier",
    "year": NaN,
    "url": "https://www.sciencedirect.com/science/article/pii/S2949719123000456",
    "pdf_url": NaN,
    "abstract": "Large language models (LLMs) are a special class of pretrained language models (PLMs)obtained by scaling model size, pretraining corpus and computation. LLMs, because of theirÂ â€¦",
    "cited_by_count": 512.0
  },
  {
    "benchmark_name": "ToxiGen",
    "benchmark_paper": "ToxiGen: A Large-Scale Machine-Generated Dataset for Adversarial and Implicit Hate Speech Detection",
    "title": "Ai alignment: A comprehensive survey",
    "authors": "J Ji,T Qiu,B Chen, B Zhang,H Lou, K Wangâ€¦Â - arXiv preprint arXivÂ â€¦, 2023",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2310.19852",
    "pdf_url": NaN,
    "abstract": "AI alignment aims to make AI systems behave in line with human intentions and values. AsAI systems grow more capable, so do risks from misalignment. To provide a comprehensiveÂ â€¦",
    "cited_by_count": 459.0
  },
  {
    "benchmark_name": "ToxiGen",
    "benchmark_paper": "ToxiGen: A Large-Scale Machine-Generated Dataset for Adversarial and Implicit Hate Speech Detection",
    "title": "Llama 2: Open foundation and fine-tuned chat models",
    "authors": "H Touvron,L Martin,K Stone, P Albertâ€¦Â - arXiv preprint arXivÂ â€¦, 2023",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2307.09288",
    "pdf_url": NaN,
    "abstract": "In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned largelanguage models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fineÂ â€¦",
    "cited_by_count": 19070.0
  },
  {
    "benchmark_name": "ToxiGen",
    "benchmark_paper": "ToxiGen: A Large-Scale Machine-Generated Dataset for Adversarial and Implicit Hate Speech Detection",
    "title": "The llama 3 herd of models",
    "authors": "A Grattafiori,A Dubey, A Jauhri, A Pandeyâ€¦Â - arXiv preprint arXivÂ â€¦, 2024",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2407.21783",
    "pdf_url": NaN,
    "abstract": "Modern artificial intelligence (AI) systems are powered by foundation models. This paperpresents a new set of foundation models, called Llama 3. It is a herd of language modelsÂ â€¦",
    "cited_by_count": 3144.0
  },
  {
    "benchmark_name": "ToxiGen",
    "benchmark_paper": "ToxiGen: A Large-Scale Machine-Generated Dataset for Adversarial and Implicit Hate Speech Detection",
    "title": "Trustllm: Trustworthiness in large language models",
    "authors": "Y Huang,L Sun,H Wang, S Wu,Q Zhang, Y Liâ€¦Â - arXiv preprint arXivÂ â€¦, 2024",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2401.05561",
    "pdf_url": NaN,
    "abstract": "Large language models (LLMs), exemplified by ChatGPT, have gained considerableattention for their excellent natural language processing capabilities. Nonetheless, theseÂ â€¦",
    "cited_by_count": 487.0
  },
  {
    "benchmark_name": "ToxiGen",
    "benchmark_paper": "ToxiGen: A Large-Scale Machine-Generated Dataset for Adversarial and Implicit Hate Speech Detection",
    "title": "Agieval: A human-centric benchmark for evaluating foundation models",
    "authors": "W Zhong,R Cui,Y Guo,Y Liang,S Lu,Y Wangâ€¦Â - arXiv preprint arXivÂ â€¦, 2023",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2304.06364",
    "pdf_url": NaN,
    "abstract": "Evaluating the general abilities of foundation models to tackle human-level tasks is a vitalaspect of their development and application in the pursuit of Artificial General IntelligenceÂ â€¦",
    "cited_by_count": 599.0
  },
  {
    "benchmark_name": "ToxiGen",
    "benchmark_paper": "ToxiGen: A Large-Scale Machine-Generated Dataset for Adversarial and Implicit Hate Speech Detection",
    "title": "Textbooks are all you need ii: phi-1.5 technical report",
    "authors": "Y Li,S Bubeck,R Eldan,A Del Giornoâ€¦Â - arXiv preprint arXivÂ â€¦, 2023",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2309.05463",
    "pdf_url": NaN,
    "abstract": "We continue the investigation into the power of smaller Transformer-based languagemodels as initiated by\\textbf {TinyStories}--a 10 million parameter model that can produceÂ â€¦",
    "cited_by_count": 597.0
  },
  {
    "benchmark_name": "ToxiGen",
    "benchmark_paper": "ToxiGen: A Large-Scale Machine-Generated Dataset for Adversarial and Implicit Hate Speech Detection",
    "title": "Orca: Progressive learning from complex explanation traces of gpt-4",
    "authors": "S Mukherjee,A Mitra,G Jawahar,S Agarwalâ€¦Â - arXiv preprint arXivÂ â€¦, 2023",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2306.02707",
    "pdf_url": NaN,
    "abstract": "Recent research has focused on enhancing the capability of smaller models throughimitation learning, drawing on the outputs generated by large foundation models (LFMs). AÂ â€¦",
    "cited_by_count": 482.0
  },
  {
    "benchmark_name": "ToxiGen",
    "benchmark_paper": "ToxiGen: A Large-Scale Machine-Generated Dataset for Adversarial and Implicit Hate Speech Detection",
    "title": "Almanacâ€”retrieval-augmented language models for clinical medicine",
    "authors": "C Zakka,R Shad,A Chaurasia,AR Dalal, JL Kimâ€¦Â - Nejm ai, 2024",
    "publication": "ai.nejm.org",
    "year": NaN,
    "url": "https://ai.nejm.org/doi/abs/10.1056/AIoa2300068",
    "pdf_url": NaN,
    "abstract": "Abstract Background Large language models (LLMs) have recently shown impressive zero-shot capabilities, whereby they can use auxiliary data, without the availability of task-specificÂ â€¦",
    "cited_by_count": 386.0
  },
  {
    "benchmark_name": "ToxiGen",
    "benchmark_paper": "ToxiGen: A Large-Scale Machine-Generated Dataset for Adversarial and Implicit Hate Speech Detection",
    "title": "Meditron-70b: Scaling medical pretraining for large language models",
    "authors": "Z Chen,AH Cano,A Romanou, A Bonnetâ€¦Â - arXiv preprint arXivÂ â€¦, 2023",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2311.16079",
    "pdf_url": NaN,
    "abstract": "Large language models (LLMs) can potentially democratize access to medical knowledge.While many efforts have been made to harness and improve LLMs' medical knowledge andÂ â€¦",
    "cited_by_count": 461.0
  },
  {
    "benchmark_name": "BBQ",
    "benchmark_paper": "BBQ: A Hand-Built Bias Benchmark for Question Answering",
    "title": "A survey on evaluation of large language models",
    "authors": "Y Chang,X Wang,J Wang,Y Wu,L Yangâ€¦Â - ACM transactions onÂ â€¦, 2024",
    "publication": "dl.acm.org",
    "year": NaN,
    "url": "https://dl.acm.org/doi/abs/10.1145/3641289",
    "pdf_url": NaN,
    "abstract": "Large language models (LLMs) are gaining increasing popularity in both academia andindustry, owing to their unprecedented performance in various applications. As LLMsÂ â€¦",
    "cited_by_count": 4405.0
  },
  {
    "benchmark_name": "BBQ",
    "benchmark_paper": "BBQ: A Hand-Built Bias Benchmark for Question Answering",
    "title": "A comprehensive overview of large language models",
    "authors": "H Naveed,AU Khan,S Qiu,M Saqib,S Anwarâ€¦Â - ACM Transactions onÂ â€¦, 2025",
    "publication": "dl.acm.org",
    "year": NaN,
    "url": "https://dl.acm.org/doi/abs/10.1145/3744746",
    "pdf_url": NaN,
    "abstract": "Large Language Models (LLMs) have recently demonstrated remarkable capabilities innatural language processing tasks and beyond. This success of LLMs has led to a largeÂ â€¦",
    "cited_by_count": 1815.0
  },
  {
    "benchmark_name": "BBQ",
    "benchmark_paper": "BBQ: A Hand-Built Bias Benchmark for Question Answering",
    "title": "Gemini: a family of highly capable multimodal models",
    "authors": "G Team, R Anil, S Borgeaud, JB Alayrac, J Yuâ€¦Â - arXiv preprint arXivÂ â€¦, 2023",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2312.11805",
    "pdf_url": NaN,
    "abstract": "This report introduces a new family of multimodal models, Gemini, that exhibit remarkablecapabilities across image, audio, video, and text understanding. The Gemini family consistsÂ â€¦",
    "cited_by_count": 6181.0
  },
  {
    "benchmark_name": "BBQ",
    "benchmark_paper": "BBQ: A Hand-Built Bias Benchmark for Question Answering",
    "title": "Mixtral of experts",
    "authors": "AQ Jiang,A Sablayrolles, A Roux,A Menschâ€¦Â - arXiv preprint arXivÂ â€¦, 2024",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2401.04088",
    "pdf_url": NaN,
    "abstract": "We introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral hasthe same architecture as Mistral 7B, with the difference that each layer is composed of 8Â â€¦",
    "cited_by_count": 2440.0
  },
  {
    "benchmark_name": "BBQ",
    "benchmark_paper": "BBQ: A Hand-Built Bias Benchmark for Question Answering",
    "title": "Palm 2 technical report",
    "authors": "R Anil,AM Dai,O Firat,M Johnson,D Lepikhinâ€¦Â - arXiv preprint arXivÂ â€¦, 2023",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2305.10403",
    "pdf_url": NaN,
    "abstract": "We introduce PaLM 2, a new state-of-the-art language model that has better multilingual andreasoning capabilities and is more compute-efficient than its predecessor PaLM. PaLM 2 isÂ â€¦",
    "cited_by_count": 2120.0
  },
  {
    "benchmark_name": "BBQ",
    "benchmark_paper": "BBQ: A Hand-Built Bias Benchmark for Question Answering",
    "title": "Holistic evaluation of language models",
    "authors": "P Liang,R Bommasani,T Lee, D Tsiprasâ€¦Â - arXiv preprint arXivÂ â€¦, 2022",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2211.09110",
    "pdf_url": NaN,
    "abstract": "Language models (LMs) are becoming the foundation for almost all major languagetechnologies, but their capabilities, limitations, and risks are not well understood. We presentÂ â€¦",
    "cited_by_count": 1905.0
  },
  {
    "benchmark_name": "BBQ",
    "benchmark_paper": "BBQ: A Hand-Built Bias Benchmark for Question Answering",
    "title": "Beavertails: Towards improved safety alignment of llm via a human-preference dataset",
    "authors": "J Ji,M Liu,J Dai,X Pan, C Zhangâ€¦Â - Advances inÂ â€¦, 2023",
    "publication": "proceedings.neurips.cc",
    "year": NaN,
    "url": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/4dbb61cb68671edc4ca3712d70083b9f-Abstract-Datasets_and_Benchmarks.html",
    "pdf_url": NaN,
    "abstract": "In this paper, we introduce the BeaverTails dataset, aimed at fostering research on safetyalignment in large language models (LLMs). This dataset uniquely separates annotations ofÂ â€¦",
    "cited_by_count": 665.0
  },
  {
    "benchmark_name": "BBQ",
    "benchmark_paper": "BBQ: A Hand-Built Bias Benchmark for Question Answering",
    "title": "Training a helpful and harmless assistant with reinforcement learning from human feedback",
    "authors": "Y Bai,A Jones,K Ndousse,A Askell, A Chenâ€¦Â - arXiv preprint arXivÂ â€¦, 2022",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2204.05862",
    "pdf_url": NaN,
    "abstract": "We apply preference modeling and reinforcement learning from human feedback (RLHF) tofinetune language models to act as helpful and harmless assistants. We find this alignmentÂ â€¦",
    "cited_by_count": 2994.0
  },
  {
    "benchmark_name": "BBQ",
    "benchmark_paper": "BBQ: A Hand-Built Bias Benchmark for Question Answering",
    "title": "Language models don't always say what they think: Unfaithful explanations in chain-of-thought prompting",
    "authors": "M Turpin,J Michael,E Perezâ€¦Â - Advances in NeuralÂ â€¦, 2023",
    "publication": "proceedings.neurips.cc",
    "year": NaN,
    "url": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/ed3fea9033a80fea1376299fa7863f4a-Abstract-Conference.html",
    "pdf_url": NaN,
    "abstract": "Abstract Large Language Models (LLMs) can achieve strong performance on many tasks byproducing step-by-step reasoning before giving a final output, often referred to as chain-ofÂ â€¦",
    "cited_by_count": 668.0
  },
  {
    "benchmark_name": "BBQ",
    "benchmark_paper": "BBQ: A Hand-Built Bias Benchmark for Question Answering",
    "title": "Beyond the imitation game: Quantifying and extrapolating the capabilities of language models",
    "authors": "A Srivastava, A Rastogi, A Rao,AAM Shoebâ€¦Â - â€¦Â on machine learningÂ â€¦, 2023",
    "publication": "openreview.net",
    "year": NaN,
    "url": "https://openreview.net/forum?id=uyTL5Bvosj&nesting=2&sort=date-desc",
    "pdf_url": NaN,
    "abstract": "Language models demonstrate both quantitative improvement and new qualitativecapabilities with increasing scale. Despite their potentially transformative impact, these newÂ â€¦",
    "cited_by_count": 2020.0
  },
  {
    "benchmark_name": "BOLD",
    "benchmark_paper": "BOLD: Dataset and Metrics for Measuring Biases in Open-Ended Language Generation",
    "title": "A survey on evaluation of large language models",
    "authors": "Y Chang,X Wang,J Wang,Y Wu,L Yangâ€¦Â - ACM transactions onÂ â€¦, 2024",
    "publication": "dl.acm.org",
    "year": NaN,
    "url": "https://dl.acm.org/doi/abs/10.1145/3641289",
    "pdf_url": NaN,
    "abstract": "Large language models (LLMs) are gaining increasing popularity in both academia andindustry, owing to their unprecedented performance in various applications. As LLMsÂ â€¦",
    "cited_by_count": 4405.0
  },
  {
    "benchmark_name": "BOLD",
    "benchmark_paper": "BOLD: Dataset and Metrics for Measuring Biases in Open-Ended Language Generation",
    "title": "A comprehensive survey of ai-generated content (aigc): A history of generative ai from gan to chatgpt",
    "authors": "Y Cao,S Li,Y Liu,Z Yan,Y Dai,PS Yuâ€¦Â - arXiv preprint arXivÂ â€¦, 2023",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2303.04226",
    "pdf_url": NaN,
    "abstract": "Recently, ChatGPT, along with DALL-E-2 and Codex, has been gaining significant attentionfrom society. As a result, many individuals have become interested in related resources andÂ â€¦",
    "cited_by_count": 1247.0
  },
  {
    "benchmark_name": "BOLD",
    "benchmark_paper": "BOLD: Dataset and Metrics for Measuring Biases in Open-Ended Language Generation",
    "title": "Mixtral of experts",
    "authors": "AQ Jiang,A Sablayrolles, A Roux,A Menschâ€¦Â - arXiv preprint arXivÂ â€¦, 2024",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2401.04088",
    "pdf_url": NaN,
    "abstract": "We introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral hasthe same architecture as Mistral 7B, with the difference that each layer is composed of 8Â â€¦",
    "cited_by_count": 2440.0
  },
  {
    "benchmark_name": "BOLD",
    "benchmark_paper": "BOLD: Dataset and Metrics for Measuring Biases in Open-Ended Language Generation",
    "title": "Llama 2: Open foundation and fine-tuned chat models",
    "authors": "H Touvron,L Martin,K Stone, P Albertâ€¦Â - arXiv preprint arXivÂ â€¦, 2023",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2307.09288",
    "pdf_url": NaN,
    "abstract": "In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned largelanguage models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fineÂ â€¦",
    "cited_by_count": 19070.0
  },
  {
    "benchmark_name": "BOLD",
    "benchmark_paper": "BOLD: Dataset and Metrics for Measuring Biases in Open-Ended Language Generation",
    "title": "Holistic evaluation of language models",
    "authors": "P Liang,R Bommasani,T Lee, D Tsiprasâ€¦Â - arXiv preprint arXivÂ â€¦, 2022",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2211.09110",
    "pdf_url": NaN,
    "abstract": "Language models (LMs) are becoming the foundation for almost all major languagetechnologies, but their capabilities, limitations, and risks are not well understood. We presentÂ â€¦",
    "cited_by_count": 1905.0
  },
  {
    "benchmark_name": "BOLD",
    "benchmark_paper": "BOLD: Dataset and Metrics for Measuring Biases in Open-Ended Language Generation",
    "title": "Training language models to follow instructions with human feedback",
    "authors": "L Ouyang,J Wu, X Jiang, D Almeidaâ€¦Â - Advances in neuralÂ â€¦, 2022",
    "publication": "proceedings.neurips.cc",
    "year": NaN,
    "url": "https://proceedings.neurips.cc/paper_files/paper/2022/hash/b1efde53be364a73914f58805a001731-Abstract-Conference.html",
    "pdf_url": NaN,
    "abstract": "Making language models bigger does not inherently make them better at following a user'sintent. For example, large language models can generate outputs that are untruthful, toxic, orÂ â€¦",
    "cited_by_count": 19069.0
  },
  {
    "benchmark_name": "BOLD",
    "benchmark_paper": "BOLD: Dataset and Metrics for Measuring Biases in Open-Ended Language Generation",
    "title": "Whose opinions do language models reflect?",
    "authors": "S Santurkar,E Durmus,F Ladhakâ€¦Â - InternationalÂ â€¦, 2023",
    "publication": "proceedings.mlr.press",
    "year": NaN,
    "url": "https://proceedings.mlr.press/v202/santurkar23a?utm_source=chatgpt.com",
    "pdf_url": NaN,
    "abstract": "Abstract Language models (LMs) are increasingly being used in open-ended contexts,where the opinions they reflect in response to subjective queries can have a profoundÂ â€¦",
    "cited_by_count": 693.0
  },
  {
    "benchmark_name": "BOLD",
    "benchmark_paper": "BOLD: Dataset and Metrics for Measuring Biases in Open-Ended Language Generation",
    "title": "DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models.",
    "authors": "B Wang,W Chen,H Pei,C Xie,M Kang,C Zhang,C Xuâ€¦Â - NeurIPS, 2023",
    "publication": "blogs.qub.ac.uk",
    "year": NaN,
    "url": "https://blogs.qub.ac.uk/wp-content/uploads/sites/7/2024/01/A-comprehensive-Assessment-of-Trustworthiness-in-GPT-Models.pdf",
    "pdf_url": NaN,
    "abstract": "Abstract Generative Pre-trained Transformer (GPT) models have exhibited exciting progressin their capabilities, capturing the interest of practitioners and the public alike. Yet, while theÂ â€¦",
    "cited_by_count": 625.0
  },
  {
    "benchmark_name": "BOLD",
    "benchmark_paper": "BOLD: Dataset and Metrics for Measuring Biases in Open-Ended Language Generation",
    "title": "Starcoder 2 and the stack v2: The next generation",
    "authors": "A Lozhkov,R Li,LB Allal,F Cassanoâ€¦Â - arXiv preprint arXivÂ â€¦, 2024",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2402.19173",
    "pdf_url": NaN,
    "abstract": "The BigCode project, an open-scientific collaboration focused on the responsibledevelopment of Large Language Models for Code (Code LLMs), introduces StarCoder2. InÂ â€¦",
    "cited_by_count": 456.0
  },
  {
    "benchmark_name": "BOLD",
    "benchmark_paper": "BOLD: Dataset and Metrics for Measuring Biases in Open-Ended Language Generation",
    "title": "Agieval: A human-centric benchmark for evaluating foundation models",
    "authors": "W Zhong,R Cui,Y Guo,Y Liang,S Lu,Y Wangâ€¦Â - arXiv preprint arXivÂ â€¦, 2023",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2304.06364",
    "pdf_url": NaN,
    "abstract": "Evaluating the general abilities of foundation models to tackle human-level tasks is a vitalaspect of their development and application in the pursuit of Artificial General IntelligenceÂ â€¦",
    "cited_by_count": 599.0
  },
  {
    "benchmark_name": "JailbreakBench",
    "benchmark_paper": "JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models",
    "title": "A comprehensive survey of small language models in the era of large language models: Techniques, enhancements, applications, collaboration with llms, andÂ â€¦",
    "authors": "F Wang,Z Zhang,X Zhang,Z Wu, T Mo,Q Luâ€¦Â - ACM Transactions onÂ â€¦, 2024",
    "publication": "dl.acm.org",
    "year": NaN,
    "url": "https://dl.acm.org/doi/abs/10.1145/3768165",
    "pdf_url": NaN,
    "abstract": "Large language models (LLMs) have demonstrated emergent abilities in text generation,question answering, and reasoning, facilitating various tasks and domains. Despite theirÂ â€¦",
    "cited_by_count": 111.0
  },
  {
    "benchmark_name": "JailbreakBench",
    "benchmark_paper": "JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models",
    "title": "A comprehensive survey in llm (-agent) full stack safety: Data, training and deployment",
    "authors": "K Wang,G Zhang,Z Zhou,J Wu,M Yu,S Zhaoâ€¦Â - arXiv preprint arXivÂ â€¦, 2025",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2504.15585",
    "pdf_url": NaN,
    "abstract": "The remarkable success of Large Language Models (LLMs) has illuminated a promisingpathway toward achieving Artificial General Intelligence for both academic and industrialÂ â€¦",
    "cited_by_count": 61.0
  },
  {
    "benchmark_name": "JailbreakBench",
    "benchmark_paper": "JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models",
    "title": "Jailbreaking black box large language models in twenty queries",
    "authors": "P Chao,A Robey,E Dobribanâ€¦Â - â€¦Â IEEE Conference onÂ â€¦, 2025",
    "publication": "ieeexplore.ieee.org",
    "year": NaN,
    "url": "https://ieeexplore.ieee.org/abstract/document/10992337/",
    "pdf_url": NaN,
    "abstract": "There is growing interest in ensuring that large language models (LLMs) align with humanvalues. However, the alignment of such models is vulnerable to adversarial jailbreaks, whichÂ â€¦",
    "cited_by_count": 953.0
  },
  {
    "benchmark_name": "JailbreakBench",
    "benchmark_paper": "JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models",
    "title": "Refusal in language models is mediated by a single direction",
    "authors": "A Arditi,O Obeso,A Syed,D Palekaâ€¦Â - Advances inÂ â€¦, 2024",
    "publication": "proceedings.neurips.cc",
    "year": NaN,
    "url": "https://proceedings.neurips.cc/paper_files/paper/2024/hash/f545448535dfde4f9786555403ab7c49-Abstract-Conference.html",
    "pdf_url": NaN,
    "abstract": "Conversational large language models are fine-tuned for both instruction-following andsafety, resulting in models that obey benign requests but refuse harmful ones. While thisÂ â€¦",
    "cited_by_count": 265.0
  },
  {
    "benchmark_name": "JailbreakBench",
    "benchmark_paper": "JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models",
    "title": "Jailbreaking leading safety-aligned llms with simple adaptive attacks",
    "authors": "M Andriushchenko,F Croce,N Flammarion- arXiv preprint arXivÂ â€¦, 2024",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2404.02151",
    "pdf_url": NaN,
    "abstract": "We show that even the most recent safety-aligned LLMs are not robust to simple adaptivejailbreaking attacks. First, we demonstrate how to successfully leverage access to logprobsÂ â€¦",
    "cited_by_count": 284.0
  },
  {
    "benchmark_name": "JailbreakBench",
    "benchmark_paper": "JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models",
    "title": "Deepinception: Hypnotize large language model to be jailbreaker",
    "authors": "X Li,Z Zhou,J Zhu,J Yao,T Liu,B Han- arXiv preprint arXiv:2311.03191, 2023",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2311.03191",
    "pdf_url": NaN,
    "abstract": "Despite remarkable success in various applications, large language models (LLMs) arevulnerable to adversarial jailbreaks that make the safety guardrails void. However, previousÂ â€¦",
    "cited_by_count": 282.0
  },
  {
    "benchmark_name": "JailbreakBench",
    "benchmark_paper": "JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models",
    "title": "Rainbow teaming: Open-ended generation of diverse adversarial prompts",
    "authors": "M Samvelyan, SC Raparthy,A Lupuâ€¦Â - Advances inÂ â€¦, 2024",
    "publication": "proceedings.neurips.cc",
    "year": NaN,
    "url": "https://proceedings.neurips.cc/paper_files/paper/2024/hash/8147a43d030b43a01020774ae1d3e3bb-Abstract-Conference.html",
    "pdf_url": NaN,
    "abstract": "As large language models (LLMs) become increasingly prevalent across many real-worldapplications, understanding and enhancing their robustness to adversarial attacks is ofÂ â€¦",
    "cited_by_count": 118.0
  },
  {
    "benchmark_name": "JailbreakBench",
    "benchmark_paper": "JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models",
    "title": "Deliberative alignment: Reasoning enables safer language models",
    "authors": "MY Guan,M Joglekar,E Wallace,S Jainâ€¦Â - arXiv preprint arXivÂ â€¦, 2024",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2412.16339",
    "pdf_url": NaN,
    "abstract": "As large-scale language models increasingly impact safety-critical domains, ensuring theirreliable adherence to well-defined principles remains a fundamental challenge. WeÂ â€¦",
    "cited_by_count": 153.0
  },
  {
    "benchmark_name": "JailbreakBench",
    "benchmark_paper": "JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models",
    "title": "Sorry-bench: Systematically evaluating large language model safety refusal",
    "authors": "T Xie,X Qi,Y Zeng,Y Huang,UM Sehwagâ€¦Â - arXiv preprint arXivÂ â€¦, 2024",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2406.14598",
    "pdf_url": NaN,
    "abstract": "Evaluating aligned large language models'(LLMs) ability to recognize and reject unsafe userrequests is crucial for safe, policy-compliant deployments. Existing evaluation effortsÂ â€¦",
    "cited_by_count": 141.0
  },
  {
    "benchmark_name": "JailbreakBench",
    "benchmark_paper": "JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models",
    "title": "Jailbreak attacks and defenses against large language models: A survey",
    "authors": "S Yi,Y Liu,Z Sun,T Cong,X He, J Song,K Xuâ€¦Â - arXiv preprint arXivÂ â€¦, 2024",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2407.04295",
    "pdf_url": NaN,
    "abstract": "Large Language Models (LLMs) have performed exceptionally in various text-generativetasks, including question answering, translation, code completion, etc. However, the overÂ â€¦",
    "cited_by_count": 200.0
  },
  {
    "benchmark_name": "HarmBench",
    "benchmark_paper": "HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal",
    "title": "A comprehensive survey in llm (-agent) full stack safety: Data, training and deployment",
    "authors": "K Wang,G Zhang,Z Zhou,J Wu,M Yu,S Zhaoâ€¦Â - arXiv preprint arXivÂ â€¦, 2025",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2504.15585",
    "pdf_url": NaN,
    "abstract": "The remarkable success of Large Language Models (LLMs) has illuminated a promisingpathway toward achieving Artificial General Intelligence for both academic and industrialÂ â€¦",
    "cited_by_count": 61.0
  },
  {
    "benchmark_name": "HarmBench",
    "benchmark_paper": "HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal",
    "title": "Adversarial attacks of vision tasks in the past 10 years: A survey",
    "authors": "C Zhang, L Zhou,X Xu, J Wu,Z Liu- ACM Computing Surveys, 2025",
    "publication": "dl.acm.org",
    "year": NaN,
    "url": "https://dl.acm.org/doi/abs/10.1145/3743126",
    "pdf_url": NaN,
    "abstract": "With the advent of Large Vision-Language Models (LVLMs), new attack vectors, such ascognitive bias, prompt injection, and jailbreaking, have emerged. Understanding theseÂ â€¦",
    "cited_by_count": 17.0
  },
  {
    "benchmark_name": "HarmBench",
    "benchmark_paper": "HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal",
    "title": "Jailbreakbench: An open robustness benchmark for jailbreaking large language models",
    "authors": "P Chao,E Debenedetti,A Robeyâ€¦Â - Advances inÂ â€¦, 2024",
    "publication": "proceedings.neurips.cc",
    "year": NaN,
    "url": "https://proceedings.neurips.cc/paper_files/paper/2024/hash/63092d79154adebd7305dfd498cbff70-Abstract-Datasets_and_Benchmarks_Track.html",
    "pdf_url": NaN,
    "abstract": "Jailbreak attacks cause large language models (LLMs) to generate harmful, unethical, orotherwise objectionable content. Evaluating these attacks presents a number of challengesÂ â€¦",
    "cited_by_count": 350.0
  },
  {
    "benchmark_name": "HarmBench",
    "benchmark_paper": "HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal",
    "title": "Tree of attacks: Jailbreaking black-box llms automatically",
    "authors": "A Mehrotra,M Zampetakisâ€¦Â - Advances inÂ â€¦, 2024",
    "publication": "proceedings.neurips.cc",
    "year": NaN,
    "url": "https://proceedings.neurips.cc/paper_files/paper/2024/hash/70702e8cbb4890b4a467b984ae59828a-Abstract-Conference.html",
    "pdf_url": NaN,
    "abstract": "Abstract While Large Language Models (LLMs) display versatile functionality, they continueto generate harmful, biased, and toxic content, as demonstrated by the prevalence of humanÂ â€¦",
    "cited_by_count": 424.0
  },
  {
    "benchmark_name": "HarmBench",
    "benchmark_paper": "HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal",
    "title": "Refusal in language models is mediated by a single direction",
    "authors": "A Arditi,O Obeso,A Syed,D Palekaâ€¦Â - Advances inÂ â€¦, 2024",
    "publication": "proceedings.neurips.cc",
    "year": NaN,
    "url": "https://proceedings.neurips.cc/paper_files/paper/2024/hash/f545448535dfde4f9786555403ab7c49-Abstract-Conference.html",
    "pdf_url": NaN,
    "abstract": "Conversational large language models are fine-tuned for both instruction-following andsafety, resulting in models that obey benign requests but refuse harmful ones. While thisÂ â€¦",
    "cited_by_count": 265.0
  },
  {
    "benchmark_name": "HarmBench",
    "benchmark_paper": "HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal",
    "title": "The wmdp benchmark: Measuring and reducing malicious use with unlearning",
    "authors": "N Li,A Pan,A Gopal, S Yue, D Berrios,A Gattiâ€¦Â - arXiv preprint arXivÂ â€¦, 2024",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2403.03218",
    "pdf_url": NaN,
    "abstract": "The White House Executive Order on Artificial Intelligence highlights the risks of largelanguage models (LLMs) empowering malicious actors in developing biological, cyber, andÂ â€¦",
    "cited_by_count": 284.0
  },
  {
    "benchmark_name": "HarmBench",
    "benchmark_paper": "HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal",
    "title": "Jailbreaking leading safety-aligned llms with simple adaptive attacks",
    "authors": "M Andriushchenko,F Croce,N Flammarion- arXiv preprint arXivÂ â€¦, 2024",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2404.02151",
    "pdf_url": NaN,
    "abstract": "We show that even the most recent safety-aligned LLMs are not robust to simple adaptivejailbreaking attacks. First, we demonstrate how to successfully leverage access to logprobsÂ â€¦",
    "cited_by_count": 284.0
  },
  {
    "benchmark_name": "HarmBench",
    "benchmark_paper": "HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal",
    "title": "Figstep: Jailbreaking large vision-language models via typographic visual prompts",
    "authors": "Y Gong, D Ran,J Liu, C Wang,T Congâ€¦Â - Proceedings of theÂ â€¦, 2025",
    "publication": "ojs.aaai.org",
    "year": NaN,
    "url": "https://ojs.aaai.org/index.php/AAAI/article/view/34568",
    "pdf_url": NaN,
    "abstract": "Abstract Large Vision-Language Models (LVLMs) signify a groundbreaking paradigm shiftwithin the Artificial Intelligence (AI) community, extending beyond the capabilities of LargeÂ â€¦",
    "cited_by_count": 264.0
  },
  {
    "benchmark_name": "HarmBench",
    "benchmark_paper": "HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal",
    "title": "Wildguard: Open one-stop moderation tools for safety risks, jailbreaks, and refusals of llms",
    "authors": "S Han,K Rao,A Ettinger,L Jiangâ€¦Â - Advances inÂ â€¦, 2024",
    "publication": "proceedings.neurips.cc",
    "year": NaN,
    "url": "https://proceedings.neurips.cc/paper_files/paper/2024/hash/0f69b4b96a46f284b726fbd70f74fb3b-Abstract-Datasets_and_Benchmarks_Track.html",
    "pdf_url": NaN,
    "abstract": "We introduce WildGuard---an open, light-weight moderation tool for LLM safety thatachieves three goals:(1) identifying malicious intent in user prompts,(2) detecting safety risksÂ â€¦",
    "cited_by_count": 169.0
  },
  {
    "benchmark_name": "HarmBench",
    "benchmark_paper": "HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal",
    "title": "Tulu 3: Pushing frontiers in open language model post-training",
    "authors": "N Lambert,J Morrison,V Pyatkin,S Huangâ€¦Â - arXiv preprint arXivÂ â€¦, 2024",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2411.15124",
    "pdf_url": NaN,
    "abstract": "Language model post-training is applied to refine behaviors and unlock new skills across awide range of recent language models, but open recipes for applying these techniques lagÂ â€¦",
    "cited_by_count": 252.0
  },
  {
    "benchmark_name": "DecodingTrust",
    "benchmark_paper": "DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models",
    "title": "A survey on evaluation of large language models",
    "authors": "Y Chang,X Wang,J Wang,Y Wu,L Yangâ€¦Â - ACM transactions onÂ â€¦, 2024",
    "publication": "dl.acm.org",
    "year": NaN,
    "url": "https://dl.acm.org/doi/abs/10.1145/3641289",
    "pdf_url": NaN,
    "abstract": "Large language models (LLMs) are gaining increasing popularity in both academia andindustry, owing to their unprecedented performance in various applications. As LLMsÂ â€¦",
    "cited_by_count": 4405.0
  },
  {
    "benchmark_name": "DecodingTrust",
    "benchmark_paper": "DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models",
    "title": "Mm-llms: Recent advances in multimodal large language models",
    "authors": "D Zhang,Y Yu,J Dong,C Li,D Su,C Chuâ€¦Â - arXiv preprint arXivÂ â€¦, 2024",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2401.13601",
    "pdf_url": NaN,
    "abstract": "In the past year, MultiModal Large Language Models (MM-LLMs) have undergonesubstantial advancements, augmenting off-the-shelf LLMs to support MM inputs or outputsÂ â€¦",
    "cited_by_count": 493.0
  },
  {
    "benchmark_name": "DecodingTrust",
    "benchmark_paper": "DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models",
    "title": "Simpo: Simple preference optimization with a reference-free reward",
    "authors": "Y Meng,M Xia,D Chen- Advances in Neural InformationÂ â€¦, 2024",
    "publication": "proceedings.neurips.cc",
    "year": NaN,
    "url": "https://proceedings.neurips.cc/paper_files/paper/2024/hash/e099c1c9699814af0be873a175361713-Abstract-Conference.html",
    "pdf_url": NaN,
    "abstract": "Abstract Direct Preference Optimization (DPO) is a widely used offline preferenceoptimization algorithm that reparameterizes reward functions in reinforcement learning fromÂ â€¦",
    "cited_by_count": 657.0
  },
  {
    "benchmark_name": "DecodingTrust",
    "benchmark_paper": "DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models",
    "title": "\" do anything now\": Characterizing and evaluating in-the-wild jailbreak prompts on large language models",
    "authors": "X Shen,Z Chen,M Backes,Y Shenâ€¦Â - Proceedings of the 2024 onÂ â€¦, 2024",
    "publication": "dl.acm.org",
    "year": NaN,
    "url": "https://dl.acm.org/doi/abs/10.1145/3658644.3670388",
    "pdf_url": NaN,
    "abstract": "The misuse of large language models (LLMs) has drawn significant attention from thegeneral public and LLM vendors. One particular type of adversarial prompt, known asÂ â€¦",
    "cited_by_count": 849.0
  },
  {
    "benchmark_name": "DecodingTrust",
    "benchmark_paper": "DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models",
    "title": "Jailbreakbench: An open robustness benchmark for jailbreaking large language models",
    "authors": "P Chao,E Debenedetti,A Robeyâ€¦Â - Advances inÂ â€¦, 2024",
    "publication": "proceedings.neurips.cc",
    "year": NaN,
    "url": "https://proceedings.neurips.cc/paper_files/paper/2024/hash/63092d79154adebd7305dfd498cbff70-Abstract-Datasets_and_Benchmarks_Track.html",
    "pdf_url": NaN,
    "abstract": "Jailbreak attacks cause large language models (LLMs) to generate harmful, unethical, orotherwise objectionable content. Evaluating these attacks presents a number of challengesÂ â€¦",
    "cited_by_count": 350.0
  },
  {
    "benchmark_name": "DecodingTrust",
    "benchmark_paper": "DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models",
    "title": "Trustllm: Trustworthiness in large language models",
    "authors": "Y Huang,L Sun,H Wang, S Wu,Q Zhang, Y Liâ€¦Â - arXiv preprint arXivÂ â€¦, 2024",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2401.05561",
    "pdf_url": NaN,
    "abstract": "Large language models (LLMs), exemplified by ChatGPT, have gained considerableattention for their excellent natural language processing capabilities. Nonetheless, theseÂ â€¦",
    "cited_by_count": 487.0
  },
  {
    "benchmark_name": "DecodingTrust",
    "benchmark_paper": "DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models",
    "title": "Trustworthy llms: a survey and guideline for evaluating large language models' alignment",
    "authors": "Y Liu,Y Yao,JF Ton,X Zhang,R Guo,H Chengâ€¦Â - arXiv preprint arXivÂ â€¦, 2023",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2308.05374",
    "pdf_url": NaN,
    "abstract": "Ensuring alignment, which refers to making models behave in accordance with humanintentions [1, 2], has become a critical task before deploying large language models (LLMs)Â â€¦",
    "cited_by_count": 516.0
  },
  {
    "benchmark_name": "DecodingTrust",
    "benchmark_paper": "DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models",
    "title": "Gptfuzzer: Red teaming large language models with auto-generated jailbreak prompts",
    "authors": "J Yu,X Lin,Z Yu,X Xing- arXiv preprint arXiv:2309.10253, 2023",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2309.10253",
    "pdf_url": NaN,
    "abstract": "Large language models (LLMs) have recently experienced tremendous popularity and arewidely used from casual conversations to AI-driven programming. However, despite theirÂ â€¦",
    "cited_by_count": 446.0
  },
  {
    "benchmark_name": "DecodingTrust",
    "benchmark_paper": "DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models",
    "title": "Combating misinformation in the age of llms: Opportunities and challenges",
    "authors": "C Chen,K Shu- AI Magazine, 2024",
    "publication": "Wiley Online Library",
    "year": NaN,
    "url": "https://onlinelibrary.wiley.com/doi/abs/10.1002/aaai.12188",
    "pdf_url": NaN,
    "abstract": "Misinformation such as fake news and rumors is a serious threat for information ecosystemsand public trust. The emergence of large language models (LLMs) has great potential toÂ â€¦",
    "cited_by_count": 279.0
  },
  {
    "benchmark_name": "DecodingTrust",
    "benchmark_paper": "DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models",
    "title": "Medical large language models are vulnerable to data-poisoning attacks",
    "authors": "DA Alber,Z Yang,A Alyakin,E Yang,S Raiâ€¦Â - Nature Medicine, 2025",
    "publication": "nature.com",
    "year": NaN,
    "url": "https://www.nature.com/articles/s41591-024-03445-1",
    "pdf_url": NaN,
    "abstract": "The adoption of large language models (LLMs) in healthcare demands a careful analysis oftheir potential to spread false medical knowledge. Because LLMs ingest massive volumes ofÂ â€¦",
    "cited_by_count": 117.0
  },
  {
    "benchmark_name": "AdvBench",
    "benchmark_paper": "Universal and Transferable Adversarial Attacks on Aligned Language Models",
    "title": "A survey on large language model (llm) security and privacy: The good, the bad, and the ugly",
    "authors": "Y Yao,J Duan,K Xu, Y Cai,Z Sun,Y Zhang- High-Confidence Computing, 2024",
    "publication": "Elsevier",
    "year": NaN,
    "url": "https://www.sciencedirect.com/science/article/pii/S266729522400014X",
    "pdf_url": NaN,
    "abstract": "Abstract Large Language Models (LLMs), such as ChatGPT and Bard, have revolutionizednatural language understanding and generation. They possess deep languageÂ â€¦",
    "cited_by_count": 1335.0
  },
  {
    "benchmark_name": "AdvBench",
    "benchmark_paper": "Universal and Transferable Adversarial Attacks on Aligned Language Models",
    "title": "The rise and potential of large language model based agents: A survey",
    "authors": "Z Xi,W Chen,X Guo,W He,Y Ding, B Hongâ€¦Â - Science ChinaÂ â€¦, 2025",
    "publication": "Springer",
    "year": NaN,
    "url": "https://link.springer.com/article/10.1007/s11432-024-4222-0",
    "pdf_url": NaN,
    "abstract": "For a long time, researchers have sought artificial intelligence (AI) that matches or exceedshuman intelligence. AI agents, which are artificial entities capable of sensing theÂ â€¦",
    "cited_by_count": 1558.0
  },
  {
    "benchmark_name": "AdvBench",
    "benchmark_paper": "Universal and Transferable Adversarial Attacks on Aligned Language Models",
    "title": "Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context",
    "authors": "G Team,P Georgiev, VI Lei, R Burnell, L Baiâ€¦Â - arXiv preprint arXivÂ â€¦, 2024",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2403.05530",
    "pdf_url": NaN,
    "abstract": "In this report, we introduce the Gemini 1.5 family of models, representing the next generationof highly compute-efficient multimodal models capable of recalling and reasoning over fineÂ â€¦",
    "cited_by_count": 2793.0
  },
  {
    "benchmark_name": "AdvBench",
    "benchmark_paper": "Universal and Transferable Adversarial Attacks on Aligned Language Models",
    "title": "ðŸ§œ Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models",
    "authors": "Y Zhang,Y Li,L Cui,D Cai,L Liu,T Fuâ€¦Â - ComputationalÂ â€¦, 2025",
    "publication": "direct.mit.edu",
    "year": NaN,
    "url": "https://direct.mit.edu/coli/article/doi/10.1162/coli.a.16/131631",
    "pdf_url": NaN,
    "abstract": "While large language models (LLMs) have demonstrated remarkable capabilities across arange of downstream tasks, a significant concern revolves around their propensity to exhibitÂ â€¦",
    "cited_by_count": 1549.0
  },
  {
    "benchmark_name": "AdvBench",
    "benchmark_paper": "Universal and Transferable Adversarial Attacks on Aligned Language Models",
    "title": "Jailbreaking black box large language models in twenty queries",
    "authors": "P Chao,A Robey,E Dobribanâ€¦Â - â€¦Â IEEE Conference onÂ â€¦, 2025",
    "publication": "ieeexplore.ieee.org",
    "year": NaN,
    "url": "https://ieeexplore.ieee.org/abstract/document/10992337/",
    "pdf_url": NaN,
    "abstract": "There is growing interest in ensuring that large language models (LLMs) align with humanvalues. However, the alignment of such models is vulnerable to adversarial jailbreaks, whichÂ â€¦",
    "cited_by_count": 953.0
  },
  {
    "benchmark_name": "AdvBench",
    "benchmark_paper": "Universal and Transferable Adversarial Attacks on Aligned Language Models",
    "title": "\" do anything now\": Characterizing and evaluating in-the-wild jailbreak prompts on large language models",
    "authors": "X Shen,Z Chen,M Backes,Y Shenâ€¦Â - Proceedings of the 2024 onÂ â€¦, 2024",
    "publication": "dl.acm.org",
    "year": NaN,
    "url": "https://dl.acm.org/doi/abs/10.1145/3658644.3670388",
    "pdf_url": NaN,
    "abstract": "The misuse of large language models (LLMs) has drawn significant attention from thegeneral public and LLM vendors. One particular type of adversarial prompt, known asÂ â€¦",
    "cited_by_count": 849.0
  },
  {
    "benchmark_name": "AdvBench",
    "benchmark_paper": "Universal and Transferable Adversarial Attacks on Aligned Language Models",
    "title": "Fine-tuning aligned language models compromises safety, even when users do not intend to!",
    "authors": "X Qi,Y Zeng,T Xie,PY Chen,R Jia,P Mittalâ€¦Â - arXiv preprint arXivÂ â€¦, 2023",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2310.03693",
    "pdf_url": NaN,
    "abstract": "Optimizing large language models (LLMs) for downstream use cases often involves thecustomization of pre-trained LLMs through further fine-tuning. Meta's open release of LlamaÂ â€¦",
    "cited_by_count": 841.0
  },
  {
    "benchmark_name": "AdvBench",
    "benchmark_paper": "Universal and Transferable Adversarial Attacks on Aligned Language Models",
    "title": "Jailbreakbench: An open robustness benchmark for jailbreaking large language models",
    "authors": "P Chao,E Debenedetti,A Robeyâ€¦Â - Advances inÂ â€¦, 2024",
    "publication": "proceedings.neurips.cc",
    "year": NaN,
    "url": "https://proceedings.neurips.cc/paper_files/paper/2024/hash/63092d79154adebd7305dfd498cbff70-Abstract-Datasets_and_Benchmarks_Track.html",
    "pdf_url": NaN,
    "abstract": "Jailbreak attacks cause large language models (LLMs) to generate harmful, unethical, orotherwise objectionable content. Evaluating these attacks presents a number of challengesÂ â€¦",
    "cited_by_count": 350.0
  },
  {
    "benchmark_name": "AdvBench",
    "benchmark_paper": "Universal and Transferable Adversarial Attacks on Aligned Language Models",
    "title": "Large language models cannot self-correct reasoning yet",
    "authors": "J Huang,X Chen,S Mishra,HS Zheng,AW Yuâ€¦Â - arXiv preprint arXivÂ â€¦, 2023",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2310.01798",
    "pdf_url": NaN,
    "abstract": "Large Language Models (LLMs) have emerged as a groundbreaking technology with theirunparalleled text generation capabilities across various applications. NeverthelessÂ â€¦",
    "cited_by_count": 643.0
  },
  {
    "benchmark_name": "AdvBench",
    "benchmark_paper": "Universal and Transferable Adversarial Attacks on Aligned Language Models",
    "title": "Tree of attacks: Jailbreaking black-box llms automatically",
    "authors": "A Mehrotra,M Zampetakisâ€¦Â - Advances inÂ â€¦, 2024",
    "publication": "proceedings.neurips.cc",
    "year": NaN,
    "url": "https://proceedings.neurips.cc/paper_files/paper/2024/hash/70702e8cbb4890b4a467b984ae59828a-Abstract-Conference.html",
    "pdf_url": NaN,
    "abstract": "Abstract While Large Language Models (LLMs) display versatile functionality, they continueto generate harmful, biased, and toxic content, as demonstrated by the prevalence of humanÂ â€¦",
    "cited_by_count": 424.0
  },
  {
    "benchmark_name": "HarmfulQA",
    "benchmark_paper": "Red-Teaming Large Language Models using Chain of Utterances for Safety-Alignment",
    "title": "A survey of GPT-3 family large language models including ChatGPT and GPT-4",
    "authors": "KS Kalyan- Natural Language Processing Journal, 2024",
    "publication": "Elsevier",
    "year": NaN,
    "url": "https://www.sciencedirect.com/science/article/pii/S2949719123000456",
    "pdf_url": NaN,
    "abstract": "Large language models (LLMs) are a special class of pretrained language models (PLMs)obtained by scaling model size, pretraining corpus and computation. LLMs, because of theirÂ â€¦",
    "cited_by_count": 512.0
  },
  {
    "benchmark_name": "HarmfulQA",
    "benchmark_paper": "Red-Teaming Large Language Models using Chain of Utterances for Safety-Alignment",
    "title": "Survey of vulnerabilities in large language models revealed by adversarial attacks",
    "authors": "E Shayegani,MAA Mamun,Y Fu,P Zareeâ€¦Â - arXiv preprint arXivÂ â€¦, 2023",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2310.10844",
    "pdf_url": NaN,
    "abstract": "Large Language Models (LLMs) are swiftly advancing in architecture and capability, and asthey integrate more deeply into complex systems, the urgency to scrutinize their securityÂ â€¦",
    "cited_by_count": 234.0
  },
  {
    "benchmark_name": "HarmfulQA",
    "benchmark_paper": "Red-Teaming Large Language Models using Chain of Utterances for Safety-Alignment",
    "title": "Trustllm: Trustworthiness in large language models",
    "authors": "Y Huang,L Sun,H Wang, S Wu,Q Zhang, Y Liâ€¦Â - arXiv preprint arXivÂ â€¦, 2024",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2401.05561",
    "pdf_url": NaN,
    "abstract": "Large language models (LLMs), exemplified by ChatGPT, have gained considerableattention for their excellent natural language processing capabilities. Nonetheless, theseÂ â€¦",
    "cited_by_count": 487.0
  },
  {
    "benchmark_name": "HarmfulQA",
    "benchmark_paper": "Red-Teaming Large Language Models using Chain of Utterances for Safety-Alignment",
    "title": "Smoothllm: Defending large language models against jailbreaking attacks",
    "authors": "A Robey,E Wong,H Hassani,GJ Pappas- arXiv preprint arXivÂ â€¦, 2023",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2310.03684",
    "pdf_url": NaN,
    "abstract": "Despite efforts to align large language models (LLMs) with human intentions, widely-usedLLMs such as GPT, Llama, and Claude are susceptible to jailbreaking attacks, wherein anÂ â€¦",
    "cited_by_count": 417.0
  },
  {
    "benchmark_name": "HarmfulQA",
    "benchmark_paper": "Red-Teaming Large Language Models using Chain of Utterances for Safety-Alignment",
    "title": "Multilingual jailbreak challenges in large language models",
    "authors": "Y Deng,W Zhang,SJ Pan,L Bing- arXiv preprint arXiv:2310.06474, 2023",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2310.06474",
    "pdf_url": NaN,
    "abstract": "While large language models (LLMs) exhibit remarkable capabilities across a wide range oftasks, they pose potential safety concerns, such as the``jailbreak''problem, whereinÂ â€¦",
    "cited_by_count": 359.0
  },
  {
    "benchmark_name": "HarmfulQA",
    "benchmark_paper": "Red-Teaming Large Language Models using Chain of Utterances for Safety-Alignment",
    "title": "Salad-bench: A hierarchical and comprehensive safety benchmark for large language models",
    "authors": "L Li,B Dong,R Wang,X Hu,W Zuo,D Linâ€¦Â - arXiv preprint arXivÂ â€¦, 2024",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2402.05044",
    "pdf_url": NaN,
    "abstract": "In the rapidly evolving landscape of Large Language Models (LLMs), ensuring robust safetymeasures is paramount. To meet this crucial need, we propose\\emph {SALAD-Bench}, aÂ â€¦",
    "cited_by_count": 197.0
  },
  {
    "benchmark_name": "HarmfulQA",
    "benchmark_paper": "Red-Teaming Large Language Models using Chain of Utterances for Safety-Alignment",
    "title": "Jailbreak attacks and defenses against large language models: A survey",
    "authors": "S Yi,Y Liu,Z Sun,T Cong,X He, J Song,K Xuâ€¦Â - arXiv preprint arXivÂ â€¦, 2024",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2407.04295",
    "pdf_url": NaN,
    "abstract": "Large Language Models (LLMs) have performed exceptionally in various text-generativetasks, including question answering, translation, code completion, etc. However, the overÂ â€¦",
    "cited_by_count": 200.0
  },
  {
    "benchmark_name": "HarmfulQA",
    "benchmark_paper": "Red-Teaming Large Language Models using Chain of Utterances for Safety-Alignment",
    "title": "Red-teaming for generative AI: Silver bullet or security theater?",
    "authors": "M Feffer,A Sinha,WH Deng,ZC Liptonâ€¦Â - Proceedings of the AAAIÂ â€¦, 2024",
    "publication": "ojs.aaai.org",
    "year": NaN,
    "url": "https://ojs.aaai.org/index.php/AIES/article/view/31647",
    "pdf_url": NaN,
    "abstract": "In response to rising concerns surrounding the safety, security, and trustworthiness ofGenerative AI (GenAI) models, practitioners and regulators alike have pointed to AI redÂ â€¦",
    "cited_by_count": 115.0
  },
  {
    "benchmark_name": "HarmfulQA",
    "benchmark_paper": "Red-Teaming Large Language Models using Chain of Utterances for Safety-Alignment",
    "title": "R-judge: Benchmarking safety risk awareness for llm agents",
    "authors": "T Yuan,Z He,L Dong,Y Wang, R Zhao, T Xiaâ€¦Â - arXiv preprint arXivÂ â€¦, 2024",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2401.10019",
    "pdf_url": NaN,
    "abstract": "Large language models (LLMs) have exhibited great potential in autonomously completingtasks across real-world applications. Despite this, these LLM agents introduce unexpectedÂ â€¦",
    "cited_by_count": 135.0
  },
  {
    "benchmark_name": "HarmfulQA",
    "benchmark_paper": "Red-Teaming Large Language Models using Chain of Utterances for Safety-Alignment",
    "title": "T2vsafetybench: Evaluating the safety of text-to-video generative models",
    "authors": "Y Miao, Y Zhu, L Yu,J Zhu,XS Gaoâ€¦Â - Advances in NeuralÂ â€¦, 2024",
    "publication": "proceedings.neurips.cc",
    "year": NaN,
    "url": "https://proceedings.neurips.cc/paper_files/paper/2024/hash/74eed5f568354c2e77dd9b018f38a9d4-Abstract-Datasets_and_Benchmarks_Track.html",
    "pdf_url": NaN,
    "abstract": "The recent development of Sora leads to a new era in text-to-video (T2V) generation. Alongwith this comes the rising concern about its safety risks. The generated videos may containÂ â€¦",
    "cited_by_count": 27.0
  },
  {
    "benchmark_name": "DoNotAnswer",
    "benchmark_paper": "Do-Not-Answer: A Dataset for Evaluating Safeguards in LLMs",
    "title": "Combating misinformation in the age of llms: Opportunities and challenges",
    "authors": "C Chen,K Shu- AI Magazine, 2024",
    "publication": "Wiley Online Library",
    "year": NaN,
    "url": "https://onlinelibrary.wiley.com/doi/abs/10.1002/aaai.12188",
    "pdf_url": NaN,
    "abstract": "Misinformation such as fake news and rumors is a serious threat for information ecosystemsand public trust. The emergence of large language models (LLMs) has great potential toÂ â€¦",
    "cited_by_count": 279.0
  },
  {
    "benchmark_name": "DoNotAnswer",
    "benchmark_paper": "Do-Not-Answer: A Dataset for Evaluating Safeguards in LLMs",
    "title": "A survey of confidence estimation and calibration in large language models",
    "authors": "J Geng,F Cai,Y Wang,H Koeppl,P Nakovâ€¦Â - arXiv preprint arXivÂ â€¦, 2023",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2311.08298",
    "pdf_url": NaN,
    "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities across a widerange of tasks in various domains. Despite their impressive performance, they can beÂ â€¦",
    "cited_by_count": 149.0
  },
  {
    "benchmark_name": "DoNotAnswer",
    "benchmark_paper": "Do-Not-Answer: A Dataset for Evaluating Safeguards in LLMs",
    "title": "Trustllm: Trustworthiness in large language models",
    "authors": "Y Huang,L Sun,H Wang, S Wu,Q Zhang, Y Liâ€¦Â - arXiv preprint arXivÂ â€¦, 2024",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2401.05561",
    "pdf_url": NaN,
    "abstract": "Large language models (LLMs), exemplified by ChatGPT, have gained considerableattention for their excellent natural language processing capabilities. Nonetheless, theseÂ â€¦",
    "cited_by_count": 487.0
  },
  {
    "benchmark_name": "DoNotAnswer",
    "benchmark_paper": "Do-Not-Answer: A Dataset for Evaluating Safeguards in LLMs",
    "title": "Rewardbench: Evaluating reward models for language modeling",
    "authors": "N Lambert,V Pyatkin,J Morrison,LJ Mirandaâ€¦Â - arXiv preprint arXivÂ â€¦, 2024",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2403.13787",
    "pdf_url": NaN,
    "abstract": "Reward models (RMs) are at the crux of successfully using RLHF to align pretrained modelsto human preferences, yet there has been relatively little study that focuses on evaluation ofÂ â€¦",
    "cited_by_count": 393.0
  },
  {
    "benchmark_name": "DoNotAnswer",
    "benchmark_paper": "Do-Not-Answer: A Dataset for Evaluating Safeguards in LLMs",
    "title": "Low-resource languages jailbreak gpt-4",
    "authors": "ZX Yong,C Menghini,SH Bach- arXiv preprint arXiv:2310.02446, 2023",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2310.02446",
    "pdf_url": NaN,
    "abstract": "AI safety training and red-teaming of large language models (LLMs) are measures tomitigate the generation of unsafe content. Our work exposes the inherent cross-lingualÂ â€¦",
    "cited_by_count": 313.0
  },
  {
    "benchmark_name": "DoNotAnswer",
    "benchmark_paper": "Do-Not-Answer: A Dataset for Evaluating Safeguards in LLMs",
    "title": "Salad-bench: A hierarchical and comprehensive safety benchmark for large language models",
    "authors": "L Li,B Dong,R Wang,X Hu,W Zuo,D Linâ€¦Â - arXiv preprint arXivÂ â€¦, 2024",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2402.05044",
    "pdf_url": NaN,
    "abstract": "In the rapidly evolving landscape of Large Language Models (LLMs), ensuring robust safetymeasures is paramount. To meet this crucial need, we propose\\emph {SALAD-Bench}, aÂ â€¦",
    "cited_by_count": 197.0
  },
  {
    "benchmark_name": "DoNotAnswer",
    "benchmark_paper": "Do-Not-Answer: A Dataset for Evaluating Safeguards in LLMs",
    "title": "Position: Trustllm: Trustworthiness in large language models",
    "authors": "Y Huang,L Sun,H Wang, S Wuâ€¦Â - InternationalÂ â€¦, 2024",
    "publication": "proceedings.mlr.press",
    "year": NaN,
    "url": "http://proceedings.mlr.press/v235/huang24x.html",
    "pdf_url": NaN,
    "abstract": "Large language models (LLMs) have gained considerable attention for their excellentnatural language processing capabilities. Nonetheless, these LLMs present manyÂ â€¦",
    "cited_by_count": 88.0
  },
  {
    "benchmark_name": "DoNotAnswer",
    "benchmark_paper": "Do-Not-Answer: A Dataset for Evaluating Safeguards in LLMs",
    "title": "Sorry-bench: Systematically evaluating large language model safety refusal",
    "authors": "T Xie,X Qi,Y Zeng,Y Huang,UM Sehwagâ€¦Â - arXiv preprint arXivÂ â€¦, 2024",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2406.14598",
    "pdf_url": NaN,
    "abstract": "Evaluating aligned large language models'(LLMs) ability to recognize and reject unsafe userrequests is crucial for safe, policy-compliant deployments. Existing evaluation effortsÂ â€¦",
    "cited_by_count": 141.0
  },
  {
    "benchmark_name": "DoNotAnswer",
    "benchmark_paper": "Do-Not-Answer: A Dataset for Evaluating Safeguards in LLMs",
    "title": "Jailbreak attacks and defenses against large language models: A survey",
    "authors": "S Yi,Y Liu,Z Sun,T Cong,X He, J Song,K Xuâ€¦Â - arXiv preprint arXivÂ â€¦, 2024",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2407.04295",
    "pdf_url": NaN,
    "abstract": "Large Language Models (LLMs) have performed exceptionally in various text-generativetasks, including question answering, translation, code completion, etc. However, the overÂ â€¦",
    "cited_by_count": 200.0
  },
  {
    "benchmark_name": "DoNotAnswer",
    "benchmark_paper": "Do-Not-Answer: A Dataset for Evaluating Safeguards in LLMs",
    "title": "Judgebench: A benchmark for evaluating llm-based judges",
    "authors": "S Tan,S Zhuang,K Montgomery, WY Tangâ€¦Â - arXiv preprint arXivÂ â€¦, 2024",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2410.12784",
    "pdf_url": NaN,
    "abstract": "LLM-based judges have emerged as a scalable alternative to human evaluation and areincreasingly used to assess, compare, and improve models. However, the reliability of LLMÂ â€¦",
    "cited_by_count": 108.0
  },
  {
    "benchmark_name": "SALAD-Bench",
    "benchmark_paper": "SALAD-Bench: A Hierarchical and Comprehensive Safety Benchmark for Large Language Models",
    "title": "A comprehensive survey in llm (-agent) full stack safety: Data, training and deployment",
    "authors": "K Wang,G Zhang,Z Zhou,J Wu,M Yu,S Zhaoâ€¦Â - arXiv preprint arXivÂ â€¦, 2025",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2504.15585",
    "pdf_url": NaN,
    "abstract": "The remarkable success of Large Language Models (LLMs) has illuminated a promisingpathway toward achieving Artificial General Intelligence for both academic and industrialÂ â€¦",
    "cited_by_count": 61.0
  },
  {
    "benchmark_name": "SALAD-Bench",
    "benchmark_paper": "SALAD-Bench: A Hierarchical and Comprehensive Safety Benchmark for Large Language Models",
    "title": "Adversarial attacks of vision tasks in the past 10 years: A survey",
    "authors": "C Zhang, L Zhou,X Xu, J Wu,Z Liu- ACM Computing Surveys, 2025",
    "publication": "dl.acm.org",
    "year": NaN,
    "url": "https://dl.acm.org/doi/abs/10.1145/3743126",
    "pdf_url": NaN,
    "abstract": "With the advent of Large Vision-Language Models (LVLMs), new attack vectors, such ascognitive bias, prompt injection, and jailbreaking, have emerged. Understanding theseÂ â€¦",
    "cited_by_count": 17.0
  },
  {
    "benchmark_name": "SALAD-Bench",
    "benchmark_paper": "SALAD-Bench: A Hierarchical and Comprehensive Safety Benchmark for Large Language Models",
    "title": "A survey on llm-as-a-judge",
    "authors": "J Gu,X Jiang,Z Shi,H Tan,X Zhai,C Xu,W Liâ€¦Â - arXiv preprint arXivÂ â€¦, 2024",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2411.15594",
    "pdf_url": NaN,
    "abstract": "Accurate and consistent evaluation is crucial for decision-making across numerous fields,yet it remains a challenging task due to inherent subjectivity, variability, and scale. LargeÂ â€¦",
    "cited_by_count": 714.0
  },
  {
    "benchmark_name": "SALAD-Bench",
    "benchmark_paper": "SALAD-Bench: A Hierarchical and Comprehensive Safety Benchmark for Large Language Models",
    "title": "Wildguard: Open one-stop moderation tools for safety risks, jailbreaks, and refusals of llms",
    "authors": "S Han,K Rao,A Ettinger,L Jiangâ€¦Â - Advances inÂ â€¦, 2024",
    "publication": "proceedings.neurips.cc",
    "year": NaN,
    "url": "https://proceedings.neurips.cc/paper_files/paper/2024/hash/0f69b4b96a46f284b726fbd70f74fb3b-Abstract-Datasets_and_Benchmarks_Track.html",
    "pdf_url": NaN,
    "abstract": "We introduce WildGuard---an open, light-weight moderation tool for LLM safety thatachieves three goals:(1) identifying malicious intent in user prompts,(2) detecting safety risksÂ â€¦",
    "cited_by_count": 169.0
  },
  {
    "benchmark_name": "SALAD-Bench",
    "benchmark_paper": "SALAD-Bench: A Hierarchical and Comprehensive Safety Benchmark for Large Language Models",
    "title": "Sorry-bench: Systematically evaluating large language model safety refusal",
    "authors": "T Xie,X Qi,Y Zeng,Y Huang,UM Sehwagâ€¦Â - arXiv preprint arXivÂ â€¦, 2024",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2406.14598",
    "pdf_url": NaN,
    "abstract": "Evaluating aligned large language models'(LLMs) ability to recognize and reject unsafe userrequests is crucial for safe, policy-compliant deployments. Existing evaluation effortsÂ â€¦",
    "cited_by_count": 141.0
  },
  {
    "benchmark_name": "SALAD-Bench",
    "benchmark_paper": "SALAD-Bench: A Hierarchical and Comprehensive Safety Benchmark for Large Language Models",
    "title": "Or-bench: An over-refusal benchmark for large language models",
    "authors": "J Cui,WL Chiang,I Stoica,CJ Hsieh- arXiv preprint arXiv:2405.20947, 2024",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2405.20947",
    "pdf_url": NaN,
    "abstract": "Large Language Models (LLMs) require careful safety alignment to prevent maliciousoutputs. While significant research focuses on mitigating harmful content generation, theÂ â€¦",
    "cited_by_count": 104.0
  },
  {
    "benchmark_name": "SALAD-Bench",
    "benchmark_paper": "SALAD-Bench: A Hierarchical and Comprehensive Safety Benchmark for Large Language Models",
    "title": "Optimization-based prompt injection attack to llm-as-a-judge",
    "authors": "J Shi,Z Yuan,Y Liu,Y Huang,P Zhou,L Sunâ€¦Â - Proceedings of theÂ â€¦, 2024",
    "publication": "dl.acm.org",
    "year": NaN,
    "url": "https://dl.acm.org/doi/abs/10.1145/3658644.3690291",
    "pdf_url": NaN,
    "abstract": "LLM-as-a-Judge uses a large language model (LLM) to select the best response from a setof candidates for a given question. LLM-as-a-Judge has many applications such as LLMÂ â€¦",
    "cited_by_count": 108.0
  },
  {
    "benchmark_name": "SALAD-Bench",
    "benchmark_paper": "SALAD-Bench: A Hierarchical and Comprehensive Safety Benchmark for Large Language Models",
    "title": "Sg-bench: Evaluating llm safety generalization across diverse tasks and prompt types",
    "authors": "Y Mou,S Zhang,W Ye- Advances in Neural InformationÂ â€¦, 2024",
    "publication": "proceedings.neurips.cc",
    "year": NaN,
    "url": "https://proceedings.neurips.cc/paper_files/paper/2024/hash/de7b99107c53e60257c727dc73daf1d1-Abstract-Datasets_and_Benchmarks_Track.html",
    "pdf_url": NaN,
    "abstract": "Ensuring the safety of large language model (LLM) applications is essential for developingtrustworthy artificial intelligence. Current LLM safety benchmarks have two limitations. FirstÂ â€¦",
    "cited_by_count": 34.0
  },
  {
    "benchmark_name": "SALAD-Bench",
    "benchmark_paper": "SALAD-Bench: A Hierarchical and Comprehensive Safety Benchmark for Large Language Models",
    "title": "Audiobench: A universal benchmark for audio large language models",
    "authors": "B Wang, X Zou,G Lin,S Sun,Z Liu,W Zhangâ€¦Â - arXiv preprint arXivÂ â€¦, 2024",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2406.16020",
    "pdf_url": NaN,
    "abstract": "We introduce AudioBench, a universal benchmark designed to evaluate Audio LargeLanguage Models (AudioLLMs). It encompasses 8 distinct tasks and 26 datasets, amongÂ â€¦",
    "cited_by_count": 107.0
  },
  {
    "benchmark_name": "SALAD-Bench",
    "benchmark_paper": "SALAD-Bench: A Hierarchical and Comprehensive Safety Benchmark for Large Language Models",
    "title": "Shieldgemma: Generative ai content moderation based on gemma",
    "authors": "W Zeng, Y Liu,R Mullins,L Peran, J Fernandezâ€¦Â - arXiv preprint arXivÂ â€¦, 2024",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2407.21772",
    "pdf_url": NaN,
    "abstract": "We present ShieldGemma, a comprehensive suite of LLM-based safety content moderationmodels built upon Gemma2. These models provide robust, state-of-the-art predictions ofÂ â€¦",
    "cited_by_count": 85.0
  },
  {
    "benchmark_name": "MACHIAVELLI",
    "benchmark_paper": "Do the Rewards Justify the Means? Measuring Trade-Offs Between Rewards and Ethical Behavior in the MACHIAVELLI Benchmark",
    "title": "Ai alignment: A comprehensive survey",
    "authors": "J Ji,T Qiu,B Chen, B Zhang,H Lou, K Wangâ€¦Â - arXiv preprint arXivÂ â€¦, 2023",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2310.19852",
    "pdf_url": NaN,
    "abstract": "AI alignment aims to make AI systems behave in line with human intentions and values. AsAI systems grow more capable, so do risks from misalignment. To provide a comprehensiveÂ â€¦",
    "cited_by_count": 459.0
  },
  {
    "benchmark_name": "MACHIAVELLI",
    "benchmark_paper": "Do the Rewards Justify the Means? Measuring Trade-Offs Between Rewards and Ethical Behavior in the MACHIAVELLI Benchmark",
    "title": "AI deception: A survey of examples, risks, and potential solutions",
    "authors": "PS Park,S Goldstein,A O'Gara,M Chen,D Hendrycks- Patterns, 2024",
    "publication": "cell.com",
    "year": NaN,
    "url": "https://www.cell.com/patterns/fulltext/S2666-3899(24)00103-X?ref=aiexec.whitegloveai.com",
    "pdf_url": NaN,
    "abstract": "This paper argues that a range of current AI systems have learned how to deceive humans.We define deception as the systematic inducement of false beliefs in the pursuit of someÂ â€¦",
    "cited_by_count": 341.0
  },
  {
    "benchmark_name": "MACHIAVELLI",
    "benchmark_paper": "Do the Rewards Justify the Means? Measuring Trade-Offs Between Rewards and Ethical Behavior in the MACHIAVELLI Benchmark",
    "title": "DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models.",
    "authors": "B Wang,W Chen,H Pei,C Xie,M Kang,C Zhang,C Xuâ€¦Â - NeurIPS, 2023",
    "publication": "blogs.qub.ac.uk",
    "year": NaN,
    "url": "https://blogs.qub.ac.uk/wp-content/uploads/sites/7/2024/01/A-comprehensive-Assessment-of-Trustworthiness-in-GPT-Models.pdf",
    "pdf_url": NaN,
    "abstract": "Abstract Generative Pre-trained Transformer (GPT) models have exhibited exciting progressin their capabilities, capturing the interest of practitioners and the public alike. Yet, while theÂ â€¦",
    "cited_by_count": 625.0
  },
  {
    "benchmark_name": "MACHIAVELLI",
    "benchmark_paper": "Do the Rewards Justify the Means? Measuring Trade-Offs Between Rewards and Ethical Behavior in the MACHIAVELLI Benchmark",
    "title": "Representation engineering: A top-down approach to ai transparency",
    "authors": "A Zou,L Phan,S Chen,J Campbell,P Guoâ€¦Â - arXiv preprint arXivÂ â€¦, 2023",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2310.01405",
    "pdf_url": NaN,
    "abstract": "In this paper, we identify and characterize the emerging area of representation engineering(RepE), an approach to enhancing the transparency of AI systems that draws on insightsÂ â€¦",
    "cited_by_count": 567.0
  },
  {
    "benchmark_name": "MACHIAVELLI",
    "benchmark_paper": "Do the Rewards Justify the Means? Measuring Trade-Offs Between Rewards and Ethical Behavior in the MACHIAVELLI Benchmark",
    "title": "Managing extreme AI risks amid rapid progress",
    "authors": "Y Bengio,G Hinton,A Yao,D Song,P Abbeel,T Darrellâ€¦Â - Science, 2024",
    "publication": "science.org",
    "year": NaN,
    "url": "https://www.science.org/doi/abs/10.1126/science.adn0117",
    "pdf_url": NaN,
    "abstract": "Artificial intelligence (AI) is progressing rapidly, and companies are shifting their focus todeveloping generalist AI systems that can autonomously act and pursue goals. Increases inÂ â€¦",
    "cited_by_count": 435.0
  },
  {
    "benchmark_name": "MACHIAVELLI",
    "benchmark_paper": "Do the Rewards Justify the Means? Measuring Trade-Offs Between Rewards and Ethical Behavior in the MACHIAVELLI Benchmark",
    "title": "The wmdp benchmark: Measuring and reducing malicious use with unlearning",
    "authors": "N Li,A Pan,A Gopal, S Yue, D Berrios,A Gattiâ€¦Â - arXiv preprint arXivÂ â€¦, 2024",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2403.03218",
    "pdf_url": NaN,
    "abstract": "The White House Executive Order on Artificial Intelligence highlights the risks of largelanguage models (LLMs) empowering malicious actors in developing biological, cyber, andÂ â€¦",
    "cited_by_count": 284.0
  },
  {
    "benchmark_name": "MACHIAVELLI",
    "benchmark_paper": "Do the Rewards Justify the Means? Measuring Trade-Offs Between Rewards and Ethical Behavior in the MACHIAVELLI Benchmark",
    "title": "Foundational challenges in assuring alignment and safety of large language models",
    "authors": "U Anwar,A Saparov,J Rando,D Palekaâ€¦Â - arXiv preprint arXivÂ â€¦, 2024",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2404.09932",
    "pdf_url": NaN,
    "abstract": "This work identifies 18 foundational challenges in assuring the alignment and safety of largelanguage models (LLMs). These challenges are organized into three different categoriesÂ â€¦",
    "cited_by_count": 254.0
  },
  {
    "benchmark_name": "MACHIAVELLI",
    "benchmark_paper": "Do the Rewards Justify the Means? Measuring Trade-Offs Between Rewards and Ethical Behavior in the MACHIAVELLI Benchmark",
    "title": "An overview of catastrophic AI risks",
    "authors": "D Hendrycks,M Mazeika,T Woodside- arXiv preprint arXiv:2306.12001, 2023",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2306.12001",
    "pdf_url": NaN,
    "abstract": "Rapid advancements in artificial intelligence (AI) have sparked growing concerns amongexperts, policymakers, and world leaders regarding the potential for increasingly advancedÂ â€¦",
    "cited_by_count": 316.0
  },
  {
    "benchmark_name": "MACHIAVELLI",
    "benchmark_paper": "Do the Rewards Justify the Means? Measuring Trade-Offs Between Rewards and Ethical Behavior in the MACHIAVELLI Benchmark",
    "title": "Review of large vision models and visual prompt engineering",
    "authors": "J Wang,Z Liu,L Zhao,Z Wu,C Ma, S Yu,H Daiâ€¦Â - Meta-Radiology, 2023",
    "publication": "Elsevier",
    "year": NaN,
    "url": "https://www.sciencedirect.com/science/article/pii/S2950162823000474",
    "pdf_url": NaN,
    "abstract": "Visual prompt engineering is a fundamental methodology in the field of visual and imageartificial general intelligence. As the development of large vision models progresses, theÂ â€¦",
    "cited_by_count": 244.0
  },
  {
    "benchmark_name": "MACHIAVELLI",
    "benchmark_paper": "Do the Rewards Justify the Means? Measuring Trade-Offs Between Rewards and Ethical Behavior in the MACHIAVELLI Benchmark",
    "title": "Rewarded soups: towards pareto-optimal alignment by interpolating weights fine-tuned on diverse rewards",
    "authors": "A Rame,G Couairon,C Dancetteâ€¦Â - Advances inÂ â€¦, 2023",
    "publication": "proceedings.neurips.cc",
    "year": NaN,
    "url": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/e12a3b98b67e8395f639fde4c2b03168-Abstract-Conference.html",
    "pdf_url": NaN,
    "abstract": "Foundation models are first pre-trained on vast unsupervised datasets and then fine-tunedon labeled data. Reinforcement learning, notably from human feedback (RLHF), can furtherÂ â€¦",
    "cited_by_count": 208.0
  },
  {
    "benchmark_name": "ELEPHANT",
    "benchmark_paper": "ELEPHANT: Measuring and Understanding Social Sycophancy in LLMs",
    "title": "Training language models to be warm and empathetic makes them less reliable and more sycophantic",
    "authors": "L Ibrahim,FS Hafner,L Rocher- arXiv preprint arXiv:2507.21919, 2025",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2507.21919",
    "pdf_url": NaN,
    "abstract": "Artificial intelligence (AI) developers are increasingly building language models with warmand empathetic personas that millions of people now use for advice, therapy, andÂ â€¦",
    "cited_by_count": 7.0
  },
  {
    "benchmark_name": "ELEPHANT",
    "benchmark_paper": "ELEPHANT: Measuring and Understanding Social Sycophancy in LLMs",
    "title": "Selective agreement, not sycophancy: investigating opinion dynamics in LLM interactions",
    "authors": "E Cau,V Pansanella,D Pedreschi,G Rossetti- EPJ Data Science, 2025",
    "publication": "Springer",
    "year": NaN,
    "url": "https://link.springer.com/content/pdf/10.1140/epjds/s13688-025-00579-1.pdf",
    "pdf_url": NaN,
    "abstract": "Understanding how opinions evolve is essential for addressing phenomena such aspolarization, radicalization, and consensus formation. In this work, we investigate howÂ â€¦",
    "cited_by_count": 4.0
  },
  {
    "benchmark_name": "ELEPHANT",
    "benchmark_paper": "ELEPHANT: Measuring and Understanding Social Sycophancy in LLMs",
    "title": "Sycophancy under pressure: Evaluating and mitigating sycophantic bias via adversarial dialogues in scientific qa",
    "authors": "K Zhang, Q Jia, Z Chen,W Sun,X Zhu,C Liâ€¦Â - arXiv preprint arXivÂ â€¦, 2025",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2508.13743",
    "pdf_url": NaN,
    "abstract": "Large language models (LLMs), while increasingly used in domains requiring factual rigor,often display a troubling behavior: sycophancy, the tendency to align with user beliefsÂ â€¦",
    "cited_by_count": 3.0
  },
  {
    "benchmark_name": "ELEPHANT",
    "benchmark_paper": "ELEPHANT: Measuring and Understanding Social Sycophancy in LLMs",
    "title": "The personality illusion: Revealing dissociation between self-reports & behavior in llms",
    "authors": "P Han,R Kocielnik,P Song,R Debnathâ€¦Â - arXiv preprint arXivÂ â€¦, 2025",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2509.03730",
    "pdf_url": NaN,
    "abstract": "Personality traits have long been studied as predictors of human behavior. Recent advancesin Large Language Models (LLMs) suggest similar patterns may emerge in artificial systemsÂ â€¦",
    "cited_by_count": 2.0
  },
  {
    "benchmark_name": "ELEPHANT",
    "benchmark_paper": "ELEPHANT: Measuring and Understanding Social Sycophancy in LLMs",
    "title": "The pimmur principles: Ensuring validity in collective behavior of llm societies",
    "authors": "J Zhou,J Huang,X Zhou,MH Lam,X Wangâ€¦Â - arXiv preprint arXivÂ â€¦, 2025",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2509.18052",
    "pdf_url": NaN,
    "abstract": "Large Language Models (LLMs) are increasingly used for social simulation, wherepopulations of agents are expected to reproduce human-like collective behavior. HoweverÂ â€¦",
    "cited_by_count": 1.0
  },
  {
    "benchmark_name": "ELEPHANT",
    "benchmark_paper": "ELEPHANT: Measuring and Understanding Social Sycophancy in LLMs",
    "title": "Simulating and Understanding Deceptive Behaviors in Long-Horizon Interactions",
    "authors": "Y Xu,X Zhang,MH Yeh,J Dhamala,O Diaâ€¦Â - arXiv preprint arXivÂ â€¦, 2025",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2510.03999",
    "pdf_url": NaN,
    "abstract": "Deception is a pervasive feature of human communication and an emerging concern inlarge language models (LLMs). While recent studies document instances of LLM deceptionÂ â€¦",
    "cited_by_count": 1.0
  },
  {
    "benchmark_name": "ELEPHANT",
    "benchmark_paper": "ELEPHANT: Measuring and Understanding Social Sycophancy in LLMs",
    "title": "Virtual agent economies",
    "authors": "N Tomasev,M Franklin,JZ Leibo,J Jacobsâ€¦Â - arXiv preprint arXivÂ â€¦, 2025",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2509.10147",
    "pdf_url": NaN,
    "abstract": "The rapid adoption of autonomous AI agents is giving rise to a new economic layer whereagents transact and coordinate at scales and speeds beyond direct human oversight. WeÂ â€¦",
    "cited_by_count": 1.0
  },
  {
    "benchmark_name": "ELEPHANT",
    "benchmark_paper": "ELEPHANT: Measuring and Understanding Social Sycophancy in LLMs",
    "title": "Can Large Language Models Simulate Spoken Human Conversations?",
    "authors": "E Mayor,LM Bietti,A Bangerter- Cognitive Science, 2025",
    "publication": "Wiley Online Library",
    "year": NaN,
    "url": "https://onlinelibrary.wiley.com/doi/abs/10.1111/cogs.70106",
    "pdf_url": NaN,
    "abstract": "Large language models (LLMs) can emulate many aspects of human cognition and havebeen heralded as a potential paradigm shift. They are proficient in chatâ€based conversationÂ â€¦",
    "cited_by_count": 1.0
  },
  {
    "benchmark_name": "ELEPHANT",
    "benchmark_paper": "ELEPHANT: Measuring and Understanding Social Sycophancy in LLMs",
    "title": "Evolving Health Informationâ€“Seeking Behavior in the Context of Google AI Overviews, ChatGPT, and Alexa: Interview Study Using the Think-Aloud Protocol",
    "authors": "C Wardle, S Urbani, E WangÂ - Journal of Medical Internet Research, 2025",
    "publication": "jmir.org",
    "year": NaN,
    "url": "https://www.jmir.org/2025/1/e79961/",
    "pdf_url": NaN,
    "abstract": "Background Online health information seeking is undergoing a major shift with the advent ofartificial intelligence (AI)â€“powered technologies such as voice assistants and largeÂ â€¦",
    "cited_by_count": NaN
  },
  {
    "benchmark_name": "ELEPHANT",
    "benchmark_paper": "ELEPHANT: Measuring and Understanding Social Sycophancy in LLMs",
    "title": "When truth is overridden: Uncovering the internal origins of sycophancy in large language models",
    "authors": "K Wang, J Li, S Yang,Z Zhang,D Wang- arXiv preprint arXiv:2508.02087, 2025",
    "publication": "arxiv.org",
    "year": NaN,
    "url": "https://arxiv.org/abs/2508.02087",
    "pdf_url": NaN,
    "abstract": "Large Language Models (LLMs) often exhibit sycophantic behavior, agreeing with user-stated opinions even when those contradict factual knowledge. While prior work hasÂ â€¦",
    "cited_by_count": 1.0
  },
  {
    "pdf_url": "",
    "title": "Lessons from a Chimp: AI\" Scheming\" and the Quest for Ape Language",
    "url": "https://arxiv.org/abs/2507.03409",
    "authors": "C Summerfield,L Luettgau,M Dubois,HR Kirkâ€¦Â - arXiv preprint arXivÂ â€¦, 2025",
    "publication": "arxiv.org",
    "abstract": "We examine recent research that asks whether current AI systems may be developing acapacity for\" scheming\"(covertly and strategically pursuing misaligned goals). We compareÂ â€¦",
    "cited_by_count": 2,
    "benchmark_name": "OpenDeception",
    "benchmark_paper": "OpenDeception: Benchmarking and Investigating AI Deceptive Behaviors via Open-ended Interaction Simulation"
  },
  {
    "pdf_url": "",
    "title": "Simulating and Understanding Deceptive Behaviors in Long-Horizon Interactions",
    "url": "https://arxiv.org/abs/2510.03999",
    "authors": "Y Xu,X Zhang,MH Yeh,J Dhamala,O Diaâ€¦Â - arXiv preprint arXivÂ â€¦, 2025",
    "publication": "arxiv.org",
    "abstract": "Deception is a pervasive feature of human communication and an emerging concern inlarge language models (LLMs). While recent studies document instances of LLM deceptionÂ â€¦",
    "cited_by_count": 1,
    "benchmark_name": "OpenDeception",
    "benchmark_paper": "OpenDeception: Benchmarking and Investigating AI Deceptive Behaviors via Open-ended Interaction Simulation"
  },
  {
    "pdf_url": "",
    "title": "Mitigating deceptive alignment via self-monitoring",
    "url": "https://arxiv.org/abs/2505.18807",
    "authors": "J Ji, W Chen, K Wang,D Hong,S Fang,B Chenâ€¦Â - arXiv preprint arXivÂ â€¦, 2025",
    "publication": "arxiv.org",
    "abstract": "Modern large language models rely on chain-of-thought (CoT) reasoning to achieveimpressive performance, yet the same mechanism can amplify deceptive alignmentÂ â€¦",
    "cited_by_count": 7,
    "benchmark_name": "OpenDeception",
    "benchmark_paper": "OpenDeception: Benchmarking and Investigating AI Deceptive Behaviors via Open-ended Interaction Simulation"
  },
  {
    "pdf_url": "",
    "title": "The hidden complexities of android TPL detection: An empirical analysis of techniques, challenges, and effectiveness",
    "url": "https://www.sciencedirect.com/science/article/pii/S016740482500361X",
    "authors": "L Zhan,J Ming, J Fu, G Peng, L Sha, L LanÂ - Computers & Security, 2025",
    "publication": "Elsevier",
    "abstract": "Third-party libraries (TPLs) play a crucial role in Android application (app) development andhave become an indispensable part of the Android ecosystem. However, TPLs alsoÂ â€¦",
    "benchmark_name": "OpenDeception",
    "benchmark_paper": "OpenDeception: Benchmarking and Investigating AI Deceptive Behaviors via Open-ended Interaction Simulation"
  },
  {
    "pdf_url": "",
    "title": "Quantized but Deceptive? A Multi-Dimensional Truthfulness Evaluation of Quantized LLMs",
    "url": "https://arxiv.org/abs/2508.19432",
    "authors": "Y Fu,X Long, R Li, H Yu, M Sheng,X Hanâ€¦Â - arXiv preprint arXivÂ â€¦, 2025",
    "publication": "arxiv.org",
    "abstract": "Quantization enables efficient deployment of large language models (LLMs) in resource-constrained environments by significantly reducing memory and computation costs. WhileÂ â€¦",
    "cited_by_count": 1,
    "benchmark_name": "OpenDeception",
    "benchmark_paper": "OpenDeception: Benchmarking and Investigating AI Deceptive Behaviors via Open-ended Interaction Simulation"
  },
  {
    "pdf_url": "",
    "title": "DeceptionBench: A Comprehensive Benchmark for AI Deception Behaviors in Real-world Scenarios",
    "url": "https://arxiv.org/abs/2510.15501",
    "authors": "Y Huang,Y Sun,Y Zhang, R Zhang,Y Dongâ€¦Â - arXiv preprint arXivÂ â€¦, 2025",
    "publication": "arxiv.org",
    "abstract": "Despite the remarkable advances of Large Language Models (LLMs) across diversecognitive tasks, the rapid enhancement of these capabilities also introduces emergentÂ â€¦",
    "benchmark_name": "OpenDeception",
    "benchmark_paper": "OpenDeception: Benchmarking and Investigating AI Deceptive Behaviors via Open-ended Interaction Simulation"
  },
  {
    "pdf_url": "",
    "title": "WGSR-Bench: Wargame-based Game-theoretic Strategic Reasoning Benchmark for Large Language Models",
    "url": "https://arxiv.org/abs/2506.10264",
    "authors": "Q Yin, P Xu,Q Li, S Liu, S Shen, T Wang, Y Hanâ€¦Â - arXiv preprint arXivÂ â€¦, 2025",
    "publication": "arxiv.org",
    "abstract": "Recent breakthroughs in Large Language Models (LLMs) have led to a qualitative leap inartificial intelligence's performance on reasoning tasks, particularly demonstratingÂ â€¦",
    "benchmark_name": "OpenDeception",
    "benchmark_paper": "OpenDeception: Benchmarking and Investigating AI Deceptive Behaviors via Open-ended Interaction Simulation"
  },
  {
    "pdf_url": "",
    "title": "Ai alignment: A comprehensive survey",
    "url": "https://arxiv.org/abs/2310.19852",
    "authors": "J Ji,T Qiu,B Chen, B Zhang,H Lou, K Wangâ€¦Â - arXiv preprint arXivÂ â€¦, 2023",
    "publication": "arxiv.org",
    "abstract": "AI alignment aims to make AI systems behave in line with human intentions and values. AsAI systems grow more capable, so do risks from misalignment. To provide a comprehensiveÂ â€¦",
    "cited_by_count": 459,
    "benchmark_name": "Goal Misgeneralization (Procgen)",
    "benchmark_paper": "Goal Misgeneralization in Deep Reinforcement Learning"
  },
  {
    "pdf_url": "",
    "title": "AI deception: A survey of examples, risks, and potential solutions",
    "url": "https://www.cell.com/patterns/fulltext/S2666-3899(24)00103-X?ref=aiexec.whitegloveai.com",
    "authors": "PS Park,S Goldstein,A O'Gara,M Chen,D Hendrycks- Patterns, 2024",
    "publication": "cell.com",
    "abstract": "This paper argues that a range of current AI systems have learned how to deceive humans.We define deception as the systematic inducement of false beliefs in the pursuit of someÂ â€¦",
    "cited_by_count": 341,
    "benchmark_name": "Goal Misgeneralization (Procgen)",
    "benchmark_paper": "Goal Misgeneralization in Deep Reinforcement Learning"
  },
  {
    "pdf_url": "",
    "title": "Open problems and fundamental limitations of reinforcement learning from human feedback",
    "url": "https://arxiv.org/abs/2307.15217",
    "authors": "S Casper,X Davies,C Shi,TK Gilbertâ€¦Â - arXiv preprint arXivÂ â€¦, 2023",
    "publication": "arxiv.org",
    "abstract": "Reinforcement learning from human feedback (RLHF) is a technique for training AI systemsto align with human goals. RLHF has emerged as the central method used to finetune stateÂ â€¦",
    "cited_by_count": 777,
    "benchmark_name": "Goal Misgeneralization (Procgen)",
    "benchmark_paper": "Goal Misgeneralization in Deep Reinforcement Learning"
  },
  {
    "pdf_url": "",
    "title": "Managing extreme AI risks amid rapid progress",
    "url": "https://www.science.org/doi/abs/10.1126/science.adn0117",
    "authors": "Y Bengio,G Hinton,A Yao,D Song,P Abbeel,T Darrellâ€¦Â - Science, 2024",
    "publication": "science.org",
    "abstract": "Artificial intelligence (AI) is progressing rapidly, and companies are shifting their focus todeveloping generalist AI systems that can autonomously act and pursue goals. Increases inÂ â€¦",
    "cited_by_count": 435,
    "benchmark_name": "Goal Misgeneralization (Procgen)",
    "benchmark_paper": "Goal Misgeneralization in Deep Reinforcement Learning"
  },
  {
    "pdf_url": "",
    "title": "Foundational challenges in assuring alignment and safety of large language models",
    "url": "https://arxiv.org/abs/2404.09932",
    "authors": "U Anwar,A Saparov,J Rando,D Palekaâ€¦Â - arXiv preprint arXivÂ â€¦, 2024",
    "publication": "arxiv.org",
    "abstract": "This work identifies 18 foundational challenges in assuring the alignment and safety of largelanguage models (LLMs). These challenges are organized into three different categoriesÂ â€¦",
    "cited_by_count": 254,
    "benchmark_name": "Goal Misgeneralization (Procgen)",
    "benchmark_paper": "Goal Misgeneralization in Deep Reinforcement Learning"
  },
  {
    "pdf_url": "",
    "title": "Rewarded soups: towards pareto-optimal alignment by interpolating weights fine-tuned on diverse rewards",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/e12a3b98b67e8395f639fde4c2b03168-Abstract-Conference.html",
    "authors": "A Rame,G Couairon,C Dancetteâ€¦Â - Advances inÂ â€¦, 2023",
    "publication": "proceedings.neurips.cc",
    "abstract": "Foundation models are first pre-trained on vast unsupervised datasets and then fine-tunedon labeled data. Reinforcement learning, notably from human feedback (RLHF), can furtherÂ â€¦",
    "cited_by_count": 208,
    "benchmark_name": "Goal Misgeneralization (Procgen)",
    "benchmark_paper": "Goal Misgeneralization in Deep Reinforcement Learning"
  },
  {
    "pdf_url": "",
    "title": "Towards reasoning era: A survey of long chain-of-thought for reasoning large language models",
    "url": "https://arxiv.org/abs/2503.09567",
    "authors": "Q Chen,L Qin, J Liu,D Peng, J Guan,P Wangâ€¦Â - arXiv preprint arXivÂ â€¦, 2025",
    "publication": "arxiv.org",
    "abstract": "Recent advancements in reasoning with large language models (RLLMs), such as OpenAI-O1 and DeepSeek-R1, have demonstrated their impressive capabilities in complex domainsÂ â€¦",
    "cited_by_count": 228,
    "benchmark_name": "Goal Misgeneralization (Procgen)",
    "benchmark_paper": "Goal Misgeneralization in Deep Reinforcement Learning"
  },
  {
    "pdf_url": "",
    "title": "Survey on large language model-enhanced reinforcement learning: Concept, taxonomy, and methods",
    "url": "https://ieeexplore.ieee.org/abstract/document/10766898/",
    "authors": "Y Cao,H Zhao,Y Cheng,T Shu,Y Chenâ€¦Â - â€¦Â on Neural NetworksÂ â€¦, 2024",
    "publication": "ieeexplore.ieee.org",
    "abstract": "With extensive pretrained knowledge and high-level general capabilities, large languagemodels (LLMs) emerge as a promising avenue to augment reinforcement learning (RL) inÂ â€¦",
    "cited_by_count": 175,
    "benchmark_name": "Goal Misgeneralization (Procgen)",
    "benchmark_paper": "Goal Misgeneralization in Deep Reinforcement Learning"
  },
  {
    "pdf_url": "",
    "title": "Eight things to know about large language models",
    "url": "https://read.dukeupress.edu/critical-ai/article-abstract/doi/10.1215/2834703X-11556011/400182",
    "authors": "SR Bowman- Critical AI, 2024",
    "publication": "read.dukeupress.edu",
    "abstract": "The widespread public deployment of large language models (LLMs) in recent months hasprompted a wave of new attention and engagement from advocates, policymakers, andÂ â€¦",
    "cited_by_count": 243,
    "benchmark_name": "Goal Misgeneralization (Procgen)",
    "benchmark_paper": "Goal Misgeneralization in Deep Reinforcement Learning"
  },
  {
    "pdf_url": "",
    "title": "The alignment problem from a deep learning perspective",
    "url": "https://arxiv.org/abs/2209.00626",
    "authors": "R Ngo,L Chan,S Mindermann- arXiv preprint arXiv:2209.00626, 2022",
    "publication": "arxiv.org",
    "abstract": "In coming years or decades, artificial general intelligence (AGI) may surpass humancapabilities at many critical tasks. We argue that, without substantial effort to prevent it, AGIsÂ â€¦",
    "cited_by_count": 295,
    "benchmark_name": "Goal Misgeneralization (Procgen)",
    "benchmark_paper": "Goal Misgeneralization in Deep Reinforcement Learning"
  },
  {
    "pdf_url": "",
    "title": "AI deception: A survey of examples, risks, and potential solutions",
    "url": "https://www.cell.com/patterns/fulltext/S2666-3899(24)00103-X?ref=aiexec.whitegloveai.com",
    "authors": "PS Park,S Goldstein,A O'Gara,M Chen,D Hendrycks- Patterns, 2024",
    "publication": "cell.com",
    "abstract": "This paper argues that a range of current AI systems have learned how to deceive humans.We define deception as the systematic inducement of false beliefs in the pursuit of someÂ â€¦",
    "cited_by_count": 341,
    "benchmark_name": "Goal Misgeneralization (Analysis)",
    "benchmark_paper": "Goal Misgeneralization: Why Correct Specifications Aren't Enough For Correct Goals"
  },
  {
    "pdf_url": "",
    "title": "Open problems and fundamental limitations of reinforcement learning from human feedback",
    "url": "https://arxiv.org/abs/2307.15217",
    "authors": "S Casper,X Davies,C Shi,TK Gilbertâ€¦Â - arXiv preprint arXivÂ â€¦, 2023",
    "publication": "arxiv.org",
    "abstract": "Reinforcement learning from human feedback (RLHF) is a technique for training AI systemsto align with human goals. RLHF has emerged as the central method used to finetune stateÂ â€¦",
    "cited_by_count": 777,
    "benchmark_name": "Goal Misgeneralization (Analysis)",
    "benchmark_paper": "Goal Misgeneralization: Why Correct Specifications Aren't Enough For Correct Goals"
  },
  {
    "pdf_url": "",
    "title": "Managing extreme AI risks amid rapid progress",
    "url": "https://www.science.org/doi/abs/10.1126/science.adn0117",
    "authors": "Y Bengio,G Hinton,A Yao,D Song,P Abbeel,T Darrellâ€¦Â - Science, 2024",
    "publication": "science.org",
    "abstract": "Artificial intelligence (AI) is progressing rapidly, and companies are shifting their focus todeveloping generalist AI systems that can autonomously act and pursue goals. Increases inÂ â€¦",
    "cited_by_count": 435,
    "benchmark_name": "Goal Misgeneralization (Analysis)",
    "benchmark_paper": "Goal Misgeneralization: Why Correct Specifications Aren't Enough For Correct Goals"
  },
  {
    "pdf_url": "",
    "title": "Large language model alignment: A survey",
    "url": "https://arxiv.org/abs/2309.15025",
    "authors": "T Shen,R Jin,Y Huang,C Liu,W Dong, Z Guoâ€¦Â - arXiv preprint arXivÂ â€¦, 2023",
    "publication": "arxiv.org",
    "abstract": "Recent years have witnessed remarkable progress made in large language models (LLMs).Such advancements, while garnering significant attention, have concurrently elicited variousÂ â€¦",
    "cited_by_count": 289,
    "benchmark_name": "Goal Misgeneralization (Analysis)",
    "benchmark_paper": "Goal Misgeneralization: Why Correct Specifications Aren't Enough For Correct Goals"
  },
  {
    "pdf_url": "",
    "title": "The alignment problem from a deep learning perspective",
    "url": "https://arxiv.org/abs/2209.00626",
    "authors": "R Ngo,L Chan,S Mindermann- arXiv preprint arXiv:2209.00626, 2022",
    "publication": "arxiv.org",
    "abstract": "In coming years or decades, artificial general intelligence (AGI) may surpass humancapabilities at many critical tasks. We argue that, without substantial effort to prevent it, AGIsÂ â€¦",
    "cited_by_count": 295,
    "benchmark_name": "Goal Misgeneralization (Analysis)",
    "benchmark_paper": "Goal Misgeneralization: Why Correct Specifications Aren't Enough For Correct Goals"
  },
  {
    "pdf_url": "",
    "title": "Safetywashing: Do AI Safety Benchmarks Actually Measure Safety Progress?",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2024/hash/7ebcdd0de471c027e67a11959c666d74-Abstract-Datasets_and_Benchmarks_Track.html",
    "authors": "R Ren,S Basart, A Khoja,A Gattiâ€¦Â - Advances inÂ â€¦, 2024",
    "publication": "proceedings.neurips.cc",
    "abstract": "Performance on popular ML benchmarks is highly correlated with model scale, suggestingthat most benchmarks tend to measure a similar underlying factor of general modelÂ â€¦",
    "cited_by_count": 48,
    "benchmark_name": "Goal Misgeneralization (Analysis)",
    "benchmark_paper": "Goal Misgeneralization: Why Correct Specifications Aren't Enough For Correct Goals"
  },
  {
    "pdf_url": "",
    "title": "Managing ai risks in an era of rapid progress",
    "url": "https://blog.biocomm.ai/wp-content/uploads/2023/11/Managing-AI-Risks-in-an-Era-of-Rapid-Progress.pdf",
    "authors": "Y Bengio,G Hinton,A Yao,D Songâ€¦Â - arXiv preprint arXivÂ â€¦, 2023",
    "publication": "blog.biocomm.ai",
    "abstract": "In this short consensus paper, we outline risks from upcoming, advanced AI systems. Weexamine large-scale social harms and malicious uses, as well as an irreversible loss ofÂ â€¦",
    "cited_by_count": 121,
    "benchmark_name": "Goal Misgeneralization (Analysis)",
    "benchmark_paper": "Goal Misgeneralization: Why Correct Specifications Aren't Enough For Correct Goals"
  },
  {
    "pdf_url": "",
    "title": "Harms from increasingly agentic algorithmic systems",
    "url": "https://dl.acm.org/doi/abs/10.1145/3593013.3594033",
    "authors": "A Chan, R Salganik,A Markelius, C Pangâ€¦Â - Proceedings of theÂ â€¦, 2023",
    "publication": "dl.acm.org",
    "abstract": "Research in Fairness, Accountability, Transparency, and Ethics (FATE) 1 has establishedmany sources and forms of algorithmic harm, in domains as diverse as health care, financeÂ â€¦",
    "cited_by_count": 181,
    "benchmark_name": "Goal Misgeneralization (Analysis)",
    "benchmark_paper": "Goal Misgeneralization: Why Correct Specifications Aren't Enough For Correct Goals"
  },
  {
    "pdf_url": "",
    "title": "Open-endedness is essential for artificial superhuman intelligence",
    "url": "https://arxiv.org/abs/2406.04268",
    "authors": "E Hughes,M Dennis,J Parker-Holderâ€¦Â - arXiv preprint arXivÂ â€¦, 2024",
    "publication": "arxiv.org",
    "abstract": "In recent years there has been a tremendous surge in the general capabilities of AI systems,mainly fuelled by training foundation models on internetscale data. Nevertheless, theÂ â€¦",
    "cited_by_count": 69,
    "benchmark_name": "Goal Misgeneralization (Analysis)",
    "benchmark_paper": "Goal Misgeneralization: Why Correct Specifications Aren't Enough For Correct Goals"
  },
  {
    "pdf_url": "",
    "title": "Characterizing manipulation from AI systems",
    "url": "https://dl.acm.org/doi/abs/10.1145/3617694.3623226",
    "authors": "M Carroll,A Chan, H Ashton,D Krueger- â€¦Â of the 3rd ACM Conference onÂ â€¦, 2023",
    "publication": "dl.acm.org",
    "abstract": "Manipulation is a concern in many domains, such as social media, advertising, andchatbots. As AI systems mediate more of our digital interactions, it is important to understandÂ â€¦",
    "cited_by_count": 124,
    "benchmark_name": "Goal Misgeneralization (Analysis)",
    "benchmark_paper": "Goal Misgeneralization: Why Correct Specifications Aren't Enough For Correct Goals"
  },
  {
    "pdf_url": "",
    "title": "Ai alignment: A comprehensive survey",
    "url": "https://arxiv.org/abs/2310.19852",
    "authors": "J Ji,T Qiu,B Chen, B Zhang,H Lou, K Wangâ€¦Â - arXiv preprint arXivÂ â€¦, 2023",
    "publication": "arxiv.org",
    "abstract": "AI alignment aims to make AI systems behave in line with human intentions and values. AsAI systems grow more capable, so do risks from misalignment. To provide a comprehensiveÂ â€¦",
    "cited_by_count": 459,
    "benchmark_name": "IPS Index",
    "benchmark_paper": "Instrumental Power-Seeking Index: A Novel Benchmark for Detecting Power-Seeking Behavior in LLMs"
  },
  {
    "pdf_url": "",
    "title": "AI deception: A survey of examples, risks, and potential solutions",
    "url": "https://www.cell.com/patterns/fulltext/S2666-3899(24)00103-X?ref=aiexec.whitegloveai.com",
    "authors": "PS Park,S Goldstein,A O'Gara,M Chen,D Hendrycks- Patterns, 2024",
    "publication": "cell.com",
    "abstract": "This paper argues that a range of current AI systems have learned how to deceive humans.We define deception as the systematic inducement of false beliefs in the pursuit of someÂ â€¦",
    "cited_by_count": 341,
    "benchmark_name": "IPS Index",
    "benchmark_paper": "Instrumental Power-Seeking Index: A Novel Benchmark for Detecting Power-Seeking Behavior in LLMs"
  },
  {
    "pdf_url": "",
    "title": "DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models.",
    "url": "https://blogs.qub.ac.uk/wp-content/uploads/sites/7/2024/01/A-comprehensive-Assessment-of-Trustworthiness-in-GPT-Models.pdf",
    "authors": "B Wang,W Chen,H Pei,C Xie,M Kang,C Zhang,C Xuâ€¦Â - NeurIPS, 2023",
    "publication": "blogs.qub.ac.uk",
    "abstract": "Abstract Generative Pre-trained Transformer (GPT) models have exhibited exciting progressin their capabilities, capturing the interest of practitioners and the public alike. Yet, while theÂ â€¦",
    "cited_by_count": 625,
    "benchmark_name": "IPS Index",
    "benchmark_paper": "Instrumental Power-Seeking Index: A Novel Benchmark for Detecting Power-Seeking Behavior in LLMs"
  },
  {
    "pdf_url": "",
    "title": "Representation engineering: A top-down approach to ai transparency",
    "url": "https://arxiv.org/abs/2310.01405",
    "authors": "A Zou,L Phan,S Chen,J Campbell,P Guoâ€¦Â - arXiv preprint arXivÂ â€¦, 2023",
    "publication": "arxiv.org",
    "abstract": "In this paper, we identify and characterize the emerging area of representation engineering(RepE), an approach to enhancing the transparency of AI systems that draws on insightsÂ â€¦",
    "cited_by_count": 567,
    "benchmark_name": "IPS Index",
    "benchmark_paper": "Instrumental Power-Seeking Index: A Novel Benchmark for Detecting Power-Seeking Behavior in LLMs"
  },
  {
    "pdf_url": "",
    "title": "Managing extreme AI risks amid rapid progress",
    "url": "https://www.science.org/doi/abs/10.1126/science.adn0117",
    "authors": "Y Bengio,G Hinton,A Yao,D Song,P Abbeel,T Darrellâ€¦Â - Science, 2024",
    "publication": "science.org",
    "abstract": "Artificial intelligence (AI) is progressing rapidly, and companies are shifting their focus todeveloping generalist AI systems that can autonomously act and pursue goals. Increases inÂ â€¦",
    "cited_by_count": 435,
    "benchmark_name": "IPS Index",
    "benchmark_paper": "Instrumental Power-Seeking Index: A Novel Benchmark for Detecting Power-Seeking Behavior in LLMs"
  },
  {
    "pdf_url": "",
    "title": "The wmdp benchmark: Measuring and reducing malicious use with unlearning",
    "url": "https://arxiv.org/abs/2403.03218",
    "authors": "N Li,A Pan,A Gopal, S Yue, D Berrios,A Gattiâ€¦Â - arXiv preprint arXivÂ â€¦, 2024",
    "publication": "arxiv.org",
    "abstract": "The White House Executive Order on Artificial Intelligence highlights the risks of largelanguage models (LLMs) empowering malicious actors in developing biological, cyber, andÂ â€¦",
    "cited_by_count": 284,
    "benchmark_name": "IPS Index",
    "benchmark_paper": "Instrumental Power-Seeking Index: A Novel Benchmark for Detecting Power-Seeking Behavior in LLMs"
  },
  {
    "pdf_url": "",
    "title": "Foundational challenges in assuring alignment and safety of large language models",
    "url": "https://arxiv.org/abs/2404.09932",
    "authors": "U Anwar,A Saparov,J Rando,D Palekaâ€¦Â - arXiv preprint arXivÂ â€¦, 2024",
    "publication": "arxiv.org",
    "abstract": "This work identifies 18 foundational challenges in assuring the alignment and safety of largelanguage models (LLMs). These challenges are organized into three different categoriesÂ â€¦",
    "cited_by_count": 254,
    "benchmark_name": "IPS Index",
    "benchmark_paper": "Instrumental Power-Seeking Index: A Novel Benchmark for Detecting Power-Seeking Behavior in LLMs"
  },
  {
    "pdf_url": "",
    "title": "An overview of catastrophic AI risks",
    "url": "https://arxiv.org/abs/2306.12001",
    "authors": "D Hendrycks,M Mazeika,T Woodside- arXiv preprint arXiv:2306.12001, 2023",
    "publication": "arxiv.org",
    "abstract": "Rapid advancements in artificial intelligence (AI) have sparked growing concerns amongexperts, policymakers, and world leaders regarding the potential for increasingly advancedÂ â€¦",
    "cited_by_count": 316,
    "benchmark_name": "IPS Index",
    "benchmark_paper": "Instrumental Power-Seeking Index: A Novel Benchmark for Detecting Power-Seeking Behavior in LLMs"
  },
  {
    "pdf_url": "",
    "title": "Review of large vision models and visual prompt engineering",
    "url": "https://www.sciencedirect.com/science/article/pii/S2950162823000474",
    "authors": "J Wang,Z Liu,L Zhao,Z Wu,C Ma, S Yu,H Daiâ€¦Â - Meta-Radiology, 2023",
    "publication": "Elsevier",
    "abstract": "Visual prompt engineering is a fundamental methodology in the field of visual and imageartificial general intelligence. As the development of large vision models progresses, theÂ â€¦",
    "cited_by_count": 244,
    "benchmark_name": "IPS Index",
    "benchmark_paper": "Instrumental Power-Seeking Index: A Novel Benchmark for Detecting Power-Seeking Behavior in LLMs"
  },
  {
    "pdf_url": "",
    "title": "Rewarded soups: towards pareto-optimal alignment by interpolating weights fine-tuned on diverse rewards",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/e12a3b98b67e8395f639fde4c2b03168-Abstract-Conference.html",
    "authors": "A Rame,G Couairon,C Dancetteâ€¦Â - Advances inÂ â€¦, 2023",
    "publication": "proceedings.neurips.cc",
    "abstract": "Foundation models are first pre-trained on vast unsupervised datasets and then fine-tunedon labeled data. Reinforcement learning, notably from human feedback (RLHF), can furtherÂ â€¦",
    "cited_by_count": 208,
    "benchmark_name": "IPS Index",
    "benchmark_paper": "Instrumental Power-Seeking Index: A Novel Benchmark for Detecting Power-Seeking Behavior in LLMs"
  },
  {
    "pdf_url": "",
    "title": "Cognitive bias in clinical large language models",
    "url": "https://www.nature.com/articles/s41746-025-01790-0",
    "authors": "A Mahajan,Z Obermeyer,R Daneshjou,J Lesterâ€¦Â - NPJ DigitalÂ â€¦, 2025",
    "publication": "nature.com",
    "abstract": "Cognitive bias accounts for a significant portion of preventable errors in healthcare,contributing to significant patient morbidity and mortality each year. As large languageÂ â€¦",
    "cited_by_count": 5,
    "benchmark_name": "SycEval",
    "benchmark_paper": "SycEval: Evaluating LLM Sycophancy"
  },
  {
    "pdf_url": "",
    "title": "Large language models outperform humans in identifying neuromyths but show sycophantic behavior in applied contexts",
    "url": "https://www.sciencedirect.com/science/article/pii/S2211949325000092",
    "authors": "E Richter,MWH Spitzer, A Morgan,L Fredeâ€¦Â - Trends in NeuroscienceÂ â€¦, 2025",
    "publication": "Elsevier",
    "abstract": "Background: Neuromyths are widespread among educators, which raises concerns aboutmisconceptions regarding the (neural) principles underlying learning in the educatorÂ â€¦",
    "cited_by_count": 6,
    "benchmark_name": "SycEval",
    "benchmark_paper": "SycEval: Evaluating LLM Sycophancy"
  },
  {
    "pdf_url": "",
    "title": "Measuring sycophancy of language models in multi-turn dialogues",
    "url": "https://arxiv.org/abs/2505.23840",
    "authors": "J Hong,G Byun,S Kim,K Shu,JD Choi- arXiv preprint arXiv:2505.23840, 2025",
    "publication": "arxiv.org",
    "abstract": "Large Language Models (LLMs) are expected to provide helpful and harmless responses,yet they often exhibit sycophancy--conforming to user beliefs regardless of factual accuracyÂ â€¦",
    "cited_by_count": 7,
    "benchmark_name": "SycEval",
    "benchmark_paper": "SycEval: Evaluating LLM Sycophancy"
  },
  {
    "pdf_url": "",
    "title": "Selective agreement, not sycophancy: investigating opinion dynamics in LLM interactions",
    "url": "https://link.springer.com/content/pdf/10.1140/epjds/s13688-025-00579-1.pdf",
    "authors": "E Cau,V Pansanella,D Pedreschi,G Rossetti- EPJ Data Science, 2025",
    "publication": "Springer",
    "abstract": "Understanding how opinions evolve is essential for addressing phenomena such aspolarization, radicalization, and consensus formation. In this work, we investigate howÂ â€¦",
    "cited_by_count": 4,
    "benchmark_name": "SycEval",
    "benchmark_paper": "SycEval: Evaluating LLM Sycophancy"
  },
  {
    "pdf_url": "",
    "title": "Social sycophancy: A broader understanding of llm sycophancy",
    "url": "https://arxiv.org/abs/2505.13995",
    "authors": "M Cheng,S Yu,C Lee,P Khadpe,L Ibrahimâ€¦Â - arXiv preprint arXivÂ â€¦, 2025",
    "publication": "arxiv.org",
    "abstract": "A serious risk to the safety and utility of LLMs is sycophancy, ie, excessive agreement withand flattery of the user. Yet existing work focuses on only one aspect of sycophancyÂ â€¦",
    "cited_by_count": 31,
    "benchmark_name": "SycEval",
    "benchmark_paper": "SycEval: Evaluating LLM Sycophancy"
  },
  {
    "pdf_url": "",
    "title": "Sycophancy under pressure: Evaluating and mitigating sycophantic bias via adversarial dialogues in scientific qa",
    "url": "https://arxiv.org/abs/2508.13743",
    "authors": "K Zhang, Q Jia, Z Chen,W Sun,X Zhu,C Liâ€¦Â - arXiv preprint arXivÂ â€¦, 2025",
    "publication": "arxiv.org",
    "abstract": "Large language models (LLMs), while increasingly used in domains requiring factual rigor,often display a troubling behavior: sycophancy, the tendency to align with user beliefsÂ â€¦",
    "cited_by_count": 3,
    "benchmark_name": "SycEval",
    "benchmark_paper": "SycEval: Evaluating LLM Sycophancy"
  },
  {
    "pdf_url": "",
    "title": "EchoBench: Benchmarking Sycophancy in Medical Large Vision-Language Models",
    "url": "https://arxiv.org/abs/2509.20146",
    "authors": "B Yuan, Y Zhou, Y Wang,F Huo, Y Jing, L Shenâ€¦Â - arXiv preprint arXivÂ â€¦, 2025",
    "publication": "arxiv.org",
    "abstract": "Recent benchmarks for medical Large Vision-Language Models (LVLMs) emphasizeleaderboard accuracy, overlooking reliability and safety. We study sycophancy--models'Â â€¦",
    "cited_by_count": 1,
    "benchmark_name": "SycEval",
    "benchmark_paper": "SycEval: Evaluating LLM Sycophancy"
  },
  {
    "pdf_url": "",
    "title": "Exploring the impact of personality traits on llm bias and toxicity",
    "url": "https://arxiv.org/abs/2502.12566",
    "authors": "S Wang,R Li,X Chen,Y Yuan,DF Wongâ€¦Â - arXiv preprint arXivÂ â€¦, 2025",
    "publication": "arxiv.org",
    "abstract": "With the different roles that AI is expected to play in human life, imbuing large languagemodels (LLMs) with different personalities has attracted increasing research interests. WhileÂ â€¦",
    "cited_by_count": 10,
    "benchmark_name": "SycEval",
    "benchmark_paper": "SycEval: Evaluating LLM Sycophancy"
  },
  {
    "pdf_url": "",
    "title": "Measuring AI\" Slop\" in Text",
    "url": "https://arxiv.org/abs/2509.19163",
    "authors": "C Shaib,T Chakrabarty,D Garcia-Olanoâ€¦Â - arXiv preprint arXivÂ â€¦, 2025",
    "publication": "arxiv.org",
    "abstract": "AI\" slop\" is an increasingly popular term used to describe low-quality AI-generated text, butthere is currently no agreed upon definition of this term nor a means to measure itsÂ â€¦",
    "cited_by_count": 1,
    "benchmark_name": "SycEval",
    "benchmark_paper": "SycEval: Evaluating LLM Sycophancy"
  },
  {
    "pdf_url": "",
    "title": "Explainability of Large Language Models: Opportunities and Challenges toward Generating Trustworthy Explanations",
    "url": "https://arxiv.org/abs/2510.17256",
    "authors": "S Atakishiyev,HKB Babiker, J Dai,N Farruqueâ€¦Â - arXiv preprint arXivÂ â€¦, 2025",
    "publication": "arxiv.org",
    "abstract": "Large language models have exhibited impressive performance across a broad range ofdownstream tasks in natural language processing. However, how a language modelÂ â€¦",
    "benchmark_name": "SycEval",
    "benchmark_paper": "SycEval: Evaluating LLM Sycophancy"
  },
  {
    "pdf_url": "",
    "title": "Safetyprompts: a systematic review of open datasets for evaluating and improving large language model safety",
    "url": "https://ojs.aaai.org/index.php/AAAI/article/view/34975",
    "authors": "P RÃ¶ttger,F Pernisi,B Vidgen,D Hovy- Proceedings of the AAAIÂ â€¦, 2025",
    "publication": "ojs.aaai.org",
    "abstract": "The last two years have seen a rapid growth in concerns around the safety of largelanguage models (LLMs). Researchers and practitioners have met these concerns byÂ â€¦",
    "cited_by_count": 52,
    "benchmark_name": "HELM Safety",
    "benchmark_paper": "HELM Safety: Towards Standardized Safety Evaluations of Language Models"
  },
  {
    "pdf_url": "",
    "title": "Evaluating the Sensitivity of LLMs to Harmful Contents in Long Input",
    "url": "https://arxiv.org/abs/2510.05864",
    "authors": "F Ghorbanpour,A Fraser- arXiv preprint arXiv:2510.05864, 2025",
    "publication": "arxiv.org",
    "abstract": "Large language models (LLMs) increasingly support applications that rely on extendedcontext, from document processing to retrieval-augmented generation. While their longÂ â€¦",
    "benchmark_name": "HELM Safety",
    "benchmark_paper": "HELM Safety: Towards Standardized Safety Evaluations of Language Models"
  },
  {
    "pdf_url": "",
    "title": "Are Large Language Models Actually Getting Safer?",
    "url": "https://www.cigionline.org/static/documents/DPH-paper-Ashley_Ferreira.pdf",
    "authors": "A Ferreira- Indirizzo: https://www. cigionline. org/static/documentsÂ â€¦, 2025",
    "publication": "cigionline.org",
    "abstract": "It is widely recognized that LLMs have risen to remarkable prominence in recent years,fundamentally transforming many aspects of our world. Their rise can be traced to the 2017Â â€¦",
    "cited_by_count": 1,
    "benchmark_name": "HELM Safety",
    "benchmark_paper": "HELM Safety: Towards Standardized Safety Evaluations of Language Models"
  },
  {
    "pdf_url": "",
    "title": "AIBENCH: TOWARDS TRUSTWORTHY EVALUA-TION UNDER THE 45 LAW",
    "url": "https://www.researchgate.net/profile/Zicheng-Zhang-9/publication/393362210_AIBENCH_TOWARDS_TRUSTWORTHY_EVALUA-_TION_UNDER_THE_45LAW/links/6867747be4632b045dc9b47c/AIBENCH-TOWARDS-TRUSTWORTHY-EVALUA-TION-UNDER-THE-45LAW.pdf",
    "authors": "Z Zhang,J Wang, Y Guo,F Wen,Z Chen,H Wang, W Liâ€¦",
    "publication": "researchgate.net",
    "abstract": "We present AIBench, a flexible and rapidly updating platform that aggregates evaluationresults from commercial platforms, popular open-source leaderboards, and internalÂ â€¦",
    "cited_by_count": 6,
    "benchmark_name": "HELM Safety",
    "benchmark_paper": "HELM Safety: Towards Standardized Safety Evaluations of Language Models"
  },
  {
    "pdf_url": "",
    "title": "The hidden risks of large reasoning models: A safety assessment of r1",
    "url": "https://arxiv.org/abs/2502.12659",
    "authors": "K Zhou,C Liu,X Zhao,S Jangam,J Srinivasaâ€¦Â - arXiv preprint arXivÂ â€¦, 2025",
    "publication": "arxiv.org",
    "abstract": "The rapid development of large reasoning models, such as OpenAI-o3 and DeepSeek-R1,has led to significant improvements in complex reasoning over non-reasoning largeÂ â€¦",
    "cited_by_count": 36,
    "benchmark_name": "AIR-Bench 2024",
    "benchmark_paper": "AIR-Bench 2024: A Safety Benchmark Based on Risk Categories from Regulations and Policies"
  },
  {
    "pdf_url": "",
    "title": "Acceptable use policies for foundation models",
    "url": "https://ojs.aaai.org/index.php/AIES/article/view/31677",
    "authors": "K Klyman- Proceedings of the AAAI/ACM Conference on AI, EthicsÂ â€¦, 2024",
    "publication": "ojs.aaai.org",
    "abstract": "As foundation models have accumulated hundreds of millions of users, developers havebegun to take steps to prevent harmful types of uses. One salient intervention that foundationÂ â€¦",
    "cited_by_count": 14,
    "benchmark_name": "AIR-Bench 2024",
    "benchmark_paper": "AIR-Bench 2024: A Safety Benchmark Based on Risk Categories from Regulations and Policies"
  },
  {
    "pdf_url": "",
    "title": "Refusal-trained llms are easily jailbroken as browser agents",
    "url": "https://arxiv.org/abs/2410.13886",
    "authors": "P Kumar,E Lau,S Vijayakumar,T Trinhâ€¦Â - arXiv preprint arXivÂ â€¦, 2024",
    "publication": "arxiv.org",
    "abstract": "For safety reasons, large language models (LLMs) are trained to refuse harmful userinstructions, such as assisting dangerous activities. We study an open question in this workÂ â€¦",
    "cited_by_count": 17,
    "benchmark_name": "AIR-Bench 2024",
    "benchmark_paper": "AIR-Bench 2024: A Safety Benchmark Based on Risk Categories from Regulations and Policies"
  },
  {
    "pdf_url": "",
    "title": "Agrail: A lifelong agent guardrail with effective and adaptive safety detection",
    "url": "https://arxiv.org/abs/2502.11448",
    "authors": "W Luo,S Dai,X Liu,S Banerjee,H Sun,M Chenâ€¦Â - arXiv preprint arXivÂ â€¦, 2025",
    "publication": "arxiv.org",
    "abstract": "The rapid advancements in Large Language Models (LLMs) have enabled their deploymentas autonomous agents for handling complex tasks in dynamic environments. These LLMsÂ â€¦",
    "cited_by_count": 18,
    "benchmark_name": "AIR-Bench 2024",
    "benchmark_paper": "AIR-Bench 2024: A Safety Benchmark Based on Risk Categories from Regulations and Policies"
  },
  {
    "pdf_url": "",
    "title": "Shieldagent: Shielding agents via verifiable safety policy reasoning",
    "url": "https://arxiv.org/abs/2503.22738",
    "authors": "Z Chen,M Kang,B Li- arXiv preprint arXiv:2503.22738, 2025",
    "publication": "arxiv.org",
    "abstract": "Autonomous agents powered by foundation models have seen widespread adoption acrossvarious real-world applications. However, they remain highly vulnerable to maliciousÂ â€¦",
    "cited_by_count": 21,
    "benchmark_name": "AIR-Bench 2024",
    "benchmark_paper": "AIR-Bench 2024: A Safety Benchmark Based on Risk Categories from Regulations and Policies"
  },
  {
    "pdf_url": "",
    "title": "Aligned LLMs are not aligned browser agents",
    "url": "https://openreview.net/forum?id=NsFZZU9gvk",
    "authors": "P Kumar,E Lau,S Vijayakumar,T Trinhâ€¦Â - The ThirteenthÂ â€¦, 2025",
    "publication": "openreview.net",
    "abstract": "For safety reasons, large language models (LLMs) are trained to refuse harmful userinstructions, such as assisting dangerous activities. We study an open question in this workÂ â€¦",
    "cited_by_count": 8,
    "benchmark_name": "AIR-Bench 2024",
    "benchmark_paper": "AIR-Bench 2024: A Safety Benchmark Based on Risk Categories from Regulations and Policies"
  },
  {
    "pdf_url": "",
    "title": "Differentially private kernel density estimation",
    "url": "https://arxiv.org/abs/2409.01688",
    "authors": "E Liu,JYC Hu, A Reneau,Z Song,H Liu- arXiv preprint arXiv:2409.01688, 2024",
    "publication": "arxiv.org",
    "abstract": "We introduce a refined differentially private (DP) data structure for kernel density estimation(KDE), offering not only improved privacy-utility tradeoff but also better efficiency over priorÂ â€¦",
    "cited_by_count": 5,
    "benchmark_name": "AIR-Bench 2024",
    "benchmark_paper": "AIR-Bench 2024: A Safety Benchmark Based on Risk Categories from Regulations and Policies"
  },
  {
    "pdf_url": "",
    "title": "Eaira: Establishing a methodology for evaluating ai models as scientific research assistants",
    "url": "https://arxiv.org/abs/2502.20309",
    "authors": "F Cappello,S Madireddy,R Underwoodâ€¦Â - arXiv preprint arXivÂ â€¦, 2025",
    "publication": "arxiv.org",
    "abstract": "Recent advancements have positioned AI, and particularly Large Language Models (LLMs),as transformative tools for scientific research, capable of addressing complex tasks thatÂ â€¦",
    "cited_by_count": 7,
    "benchmark_name": "AIR-Bench 2024",
    "benchmark_paper": "AIR-Bench 2024: A Safety Benchmark Based on Risk Categories from Regulations and Policies"
  },
  {
    "pdf_url": "",
    "title": "Breaking Down Bias: On The Limits of Generalizable Pruning Strategies",
    "url": "https://dl.acm.org/doi/abs/10.1145/3715275.3732161",
    "authors": "S Ma,A Salinas,J Nyarko,P Henderson- Proceedings of the 2025 ACMÂ â€¦, 2025",
    "publication": "dl.acm.org",
    "abstract": "We employ model pruning to examine how LLMs conceptualize racial biases, and whether ageneralizable mitigation strategy for such biases appears feasible. Our analysis yieldsÂ â€¦",
    "cited_by_count": 2,
    "benchmark_name": "AIR-Bench 2024",
    "benchmark_paper": "AIR-Bench 2024: A Safety Benchmark Based on Risk Categories from Regulations and Policies"
  },
  {
    "pdf_url": "",
    "title": "A Comprehensive Survey on Trustworthiness in Reasoning with Large Language Models",
    "url": "https://arxiv.org/abs/2509.03871",
    "authors": "Y Wang,Y Yu,J Liang, R HeÂ - arXiv preprint arXiv:2509.03871, 2025",
    "publication": "arxiv.org",
    "abstract": "The development of Long-CoT reasoning has advanced LLM performance across varioustasks, including language understanding, complex problem solving, and code generationÂ â€¦",
    "benchmark_name": "AIR-Bench 2024",
    "benchmark_paper": "AIR-Bench 2024: A Safety Benchmark Based on Risk Categories from Regulations and Policies"
  },
  {
    "pdf_url": "",
    "title": "Security and privacy challenges of large language models: A survey",
    "url": "https://dl.acm.org/doi/abs/10.1145/3712001",
    "authors": "BC Das,MH Amini,Y Wu- ACM Computing Surveys, 2025",
    "publication": "dl.acm.org",
    "abstract": "Large language models (LLMs) have demonstrated extraordinary capabilities andcontributed to multiple fields, such as generating and summarizing text, languageÂ â€¦",
    "cited_by_count": 388,
    "benchmark_name": "TrustLLM",
    "benchmark_paper": "TrustLLM: Trustworthiness in Large Language Models"
  },
  {
    "pdf_url": "",
    "title": "Empowering biomedical discovery with AI agents",
    "url": "https://www.cell.com/cell/fulltext/S0092-8674(24)01070-5?&target=_blank",
    "authors": "S Gao,A Fang, Y Huang,V Giunchiglia,A Nooriâ€¦Â - Cell, 2024",
    "publication": "cell.com",
    "abstract": "We envision\" AI scientists\" as systems capable of skeptical learning and reasoning thatempower biomedical research through collaborative agents that integrate AI models andÂ â€¦",
    "cited_by_count": 232,
    "benchmark_name": "TrustLLM",
    "benchmark_paper": "TrustLLM: Trustworthiness in Large Language Models"
  },
  {
    "pdf_url": "",
    "title": "Large language models: A survey",
    "url": "https://arxiv.org/abs/2402.06196",
    "authors": "S Minaee,T Mikolov,N Nikzad,M Chenaghluâ€¦Â - arXiv preprint arXivÂ â€¦, 2024",
    "publication": "arxiv.org",
    "abstract": "Large Language Models (LLMs) have drawn a lot of attention due to their strongperformance on a wide range of natural language tasks, since the release of ChatGPT inÂ â€¦",
    "cited_by_count": 1504,
    "benchmark_name": "TrustLLM",
    "benchmark_paper": "TrustLLM: Trustworthiness in Large Language Models"
  },
  {
    "pdf_url": "",
    "title": "Jailbreakbench: An open robustness benchmark for jailbreaking large language models",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2024/hash/63092d79154adebd7305dfd498cbff70-Abstract-Datasets_and_Benchmarks_Track.html",
    "authors": "P Chao,E Debenedetti,A Robeyâ€¦Â - Advances inÂ â€¦, 2024",
    "publication": "proceedings.neurips.cc",
    "abstract": "Jailbreak attacks cause large language models (LLMs) to generate harmful, unethical, orotherwise objectionable content. Evaluating these attacks presents a number of challengesÂ â€¦",
    "cited_by_count": 350,
    "benchmark_name": "TrustLLM",
    "benchmark_paper": "TrustLLM: Trustworthiness in Large Language Models"
  },
  {
    "pdf_url": "",
    "title": "Large language models: a comprehensive survey of its applications, challenges, limitations, and future prospects",
    "url": "https://www.researchgate.net/profile/Muhammad-Shaikh-9/publication/383818024_Large_Language_Models_A_Comprehensive_Survey_of_its_Applications_Challenges_Limitations_and_Future_Prospects/links/66dffb06b1606e24c21d8936/Large-Language-Models-A-Comprehensive-Survey-of-its-Applications-Challenges-Limitations-and-Future-Prospects.pdf",
    "authors": "MU Hadi,R Qureshi,A Shah,M Irfan,A Zafarâ€¦Â - AuthoreaÂ â€¦, 2023",
    "publication": "researchgate.net",
    "abstract": "Within the vast expanse of computerized language processing, a revolutionary entity knownas Large Language Models (LLMs) has emerged, wielding immense power in its capacity toÂ â€¦",
    "cited_by_count": 601,
    "benchmark_name": "TrustLLM",
    "benchmark_paper": "TrustLLM: Trustworthiness in Large Language Models"
  },
  {
    "pdf_url": "",
    "title": "Combating misinformation in the age of llms: Opportunities and challenges",
    "url": "https://onlinelibrary.wiley.com/doi/abs/10.1002/aaai.12188",
    "authors": "C Chen,K Shu- AI Magazine, 2024",
    "publication": "Wiley Online Library",
    "abstract": "Misinformation such as fake news and rumors is a serious threat for information ecosystemsand public trust. The emergence of large language models (LLMs) has great potential toÂ â€¦",
    "cited_by_count": 279,
    "benchmark_name": "TrustLLM",
    "benchmark_paper": "TrustLLM: Trustworthiness in Large Language Models"
  },
  {
    "pdf_url": "",
    "title": "A comprehensive survey of small language models in the era of large language models: Techniques, enhancements, applications, collaboration with llms, andÂ â€¦",
    "url": "https://dl.acm.org/doi/abs/10.1145/3768165",
    "authors": "F Wang,Z Zhang,X Zhang,Z Wu, T Mo,Q Luâ€¦Â - ACM Transactions onÂ â€¦, 2024",
    "publication": "dl.acm.org",
    "abstract": "Large language models (LLMs) have demonstrated emergent abilities in text generation,question answering, and reasoning, facilitating various tasks and domains. Despite theirÂ â€¦",
    "cited_by_count": 111,
    "benchmark_name": "TrustLLM",
    "benchmark_paper": "TrustLLM: Trustworthiness in Large Language Models"
  },
  {
    "pdf_url": "",
    "title": "Factuality challenges in the era of large language models and opportunities for fact-checking",
    "url": "https://www.nature.com/articles/s42256-024-00881-z",
    "authors": "I Augenstein,T Baldwin,M Chaâ€¦Â - Nature MachineÂ â€¦, 2024",
    "publication": "nature.com",
    "abstract": "The emergence of tools based on large language models (LLMs), such as OpenAI'sChatGPT and Google's Gemini, has garnered immense public attention owing to theirÂ â€¦",
    "cited_by_count": 225,
    "benchmark_name": "TrustLLM",
    "benchmark_paper": "TrustLLM: Trustworthiness in Large Language Models"
  },
  {
    "pdf_url": "",
    "title": "Mllm-as-a-judge: Assessing multimodal llm-as-a-judge with vision-language benchmark",
    "url": "https://openreview.net/forum?id=dbFEFHAD79",
    "authors": "D Chen,R Chen, S Zhang, Y Wang,Y Liuâ€¦Â - â€¦Â on Machine Learning, 2024",
    "publication": "openreview.net",
    "abstract": "Multimodal Large Language Models (MLLMs) have gained significant attention recently,showing remarkable potential in artificial general intelligence. However, assessing the utilityÂ â€¦",
    "cited_by_count": 210,
    "benchmark_name": "TrustLLM",
    "benchmark_paper": "TrustLLM: Trustworthiness in Large Language Models"
  },
  {
    "pdf_url": "",
    "title": "Can llm-generated misinformation be detected?",
    "url": "https://arxiv.org/abs/2309.13788",
    "authors": "C Chen,K Shu- arXiv preprint arXiv:2309.13788, 2023",
    "publication": "arxiv.org",
    "abstract": "The advent of Large Language Models (LLMs) has made a transformative impact. However,the potential that LLMs such as ChatGPT can be exploited to generate misinformation hasÂ â€¦",
    "cited_by_count": 310,
    "benchmark_name": "TrustLLM",
    "benchmark_paper": "TrustLLM: Trustworthiness in Large Language Models"
  },
  {
    "pdf_url": "",
    "title": "Who is responsible? the data, models, users or regulations? a comprehensive survey on responsible generative ai for a sustainable future",
    "url": "https://arxiv.org/abs/2502.08650",
    "authors": "S Raza,R Qureshi, A Zahid, S Kamawalâ€¦Â - arXiv preprint arXivÂ â€¦, 2025",
    "publication": "arxiv.org",
    "abstract": "Generative AI is moving rapidly from research into real world deployment across sectors,which elevates the need for responsible development, deployment, evaluation, andÂ â€¦",
    "cited_by_count": 13,
    "benchmark_name": "MLCommons AILuminate",
    "benchmark_paper": "AILuminate: Introducing v1.0 of the AI Risk and Reliability Benchmark from MLCommons"
  },
  {
    "pdf_url": "",
    "title": "Automated red teaming with goat: the generative offensive agent tester",
    "url": "https://arxiv.org/abs/2410.01606",
    "authors": "M Pavlova,E Brinkman,K Iyer,V Albieroâ€¦Â - arXiv preprint arXivÂ â€¦, 2024",
    "publication": "arxiv.org",
    "abstract": "Red teaming assesses how large language models (LLMs) can produce content thatviolates norms, policies, and rules set during their safety training. However, most existingÂ â€¦",
    "cited_by_count": 21,
    "benchmark_name": "MLCommons AILuminate",
    "benchmark_paper": "AILuminate: Introducing v1.0 of the AI Risk and Reliability Benchmark from MLCommons"
  },
  {
    "pdf_url": "",
    "title": "Are all prompt components value-neutral? understanding the heterogeneous adversarial robustness of dissected prompt in large language models",
    "url": "https://arxiv.org/abs/2508.01554",
    "authors": "Y Zheng,T Li, H Huang, T Zeng, J Lu,C Chuâ€¦Â - arXiv preprint arXivÂ â€¦, 2025",
    "publication": "arxiv.org",
    "abstract": "Prompt-based adversarial attacks have become an effective means to assess therobustness of large language models (LLMs). However, existing approaches often treatÂ â€¦",
    "cited_by_count": 3,
    "benchmark_name": "MLCommons AILuminate",
    "benchmark_paper": "AILuminate: Introducing v1.0 of the AI Risk and Reliability Benchmark from MLCommons"
  },
  {
    "pdf_url": "",
    "title": "D-rex: A benchmark for detecting deceptive reasoning in large language models",
    "url": "https://arxiv.org/abs/2509.17938",
    "authors": "S Krishna,A Zou,R Gupta,EK Jones, N Winterâ€¦Â - arXiv preprint arXivÂ â€¦, 2025",
    "publication": "arxiv.org",
    "abstract": "The safety and alignment of Large Language Models (LLMs) are critical for their responsibledeployment. Current evaluation methods predominantly focus on identifying and preventingÂ â€¦",
    "cited_by_count": 1,
    "benchmark_name": "MLCommons AILuminate",
    "benchmark_paper": "AILuminate: Introducing v1.0 of the AI Risk and Reliability Benchmark from MLCommons"
  },
  {
    "pdf_url": "",
    "title": "Contemplative Superalignment",
    "url": "https://link.springer.com/chapter/10.1007/978-3-032-00686-8_31",
    "authors": "RE Laukkonen, F Inglis, S Chandariaâ€¦Â - â€¦Â Conference on ArtificialÂ â€¦, 2025",
    "publication": "Springer",
    "abstract": "As artificial intelligence (AI) improves, current alignment strategies may falter in the face ofunpredictable self-improvement and the sheer complexity of AI. Rather than trying to controlÂ â€¦",
    "cited_by_count": 1,
    "benchmark_name": "MLCommons AILuminate",
    "benchmark_paper": "AILuminate: Introducing v1.0 of the AI Risk and Reliability Benchmark from MLCommons"
  },
  {
    "pdf_url": "",
    "title": "Is Perceptual Encryption Secure? A Security Benchmark for Perceptual Encryption Methods",
    "url": "https://ieeexplore.ieee.org/abstract/document/10974570/",
    "authors": "U Kashyap,SK Padhi,SS Ali- IEEE Transactions on ArtificialÂ â€¦, 2025",
    "publication": "ieeexplore.ieee.org",
    "abstract": "Perceptual encryption methods are the key enablers for protecting image privacy for deeplearning-based applications in the cloud. In perceptual encryption, the image content isÂ â€¦",
    "benchmark_name": "MLCommons AILuminate",
    "benchmark_paper": "AILuminate: Introducing v1.0 of the AI Risk and Reliability Benchmark from MLCommons"
  },
  {
    "pdf_url": "",
    "title": "Phare: A Safety Probe for Large Language Models",
    "url": "https://arxiv.org/abs/2505.11365",
    "authors": "PL Jeune, B MalÃ©zieux,W Xiao,M Dora- arXiv preprint arXiv:2505.11365, 2025",
    "publication": "arxiv.org",
    "abstract": "Ensuring the safety of large language models (LLMs) is critical for responsible deployment,yet existing evaluations often prioritize performance over identifying failure modes. WeÂ â€¦",
    "benchmark_name": "MLCommons AILuminate",
    "benchmark_paper": "AILuminate: Introducing v1.0 of the AI Risk and Reliability Benchmark from MLCommons"
  },
  {
    "pdf_url": "",
    "title": "Decoding Federated Learning: The FedNAM+ Conformal Revolution",
    "url": "https://arxiv.org/abs/2506.17872",
    "authors": "SB Balija,A Nanda,D Sahoo- arXiv preprint arXiv:2506.17872, 2025",
    "publication": "arxiv.org",
    "abstract": "Federated learning has significantly advanced distributed training of machine learningmodels across decentralized data sources. However, existing frameworks often lackÂ â€¦",
    "cited_by_count": 1,
    "benchmark_name": "MLCommons AILuminate",
    "benchmark_paper": "AILuminate: Introducing v1.0 of the AI Risk and Reliability Benchmark from MLCommons"
  },
  {
    "pdf_url": "",
    "title": "Beyond Uniform Criteria: Scenario-Adaptive Multi-Dimensional Jailbreak Evaluation",
    "url": "https://arxiv.org/abs/2508.06194",
    "authors": "L Jiang, Y Li, X Zhang,Y Ding, L PanÂ - arXiv preprint arXiv:2508.06194, 2025",
    "publication": "arxiv.org",
    "abstract": "Precise jailbreak evaluation is vital for LLM red teaming and jailbreak research. Currentapproaches employ binary classification (eg, string matching, toxic text classifiers, LLMÂ â€¦",
    "benchmark_name": "MLCommons AILuminate",
    "benchmark_paper": "AILuminate: Introducing v1.0 of the AI Risk and Reliability Benchmark from MLCommons"
  },
  {
    "pdf_url": "",
    "title": "The Trust Fabric: Decentralized Interoperability and Economic Coordination for the Agentic Web",
    "url": "https://arxiv.org/abs/2507.07901",
    "authors": "SB Balija, R Singal,R Raskar,E Darzi, R Balaâ€¦Â - arXiv preprint arXivÂ â€¦, 2025",
    "publication": "arxiv.org",
    "abstract": "The fragmentation of AI agent ecosystems has created urgent demands for interoperability,trust, and economic coordination that current protocols--including MCP (Hou et al., 2025)Â â€¦",
    "benchmark_name": "MLCommons AILuminate",
    "benchmark_paper": "AILuminate: Introducing v1.0 of the AI Risk and Reliability Benchmark from MLCommons"
  },
  {
    "pdf_url": "",
    "title": "Safetyprompts: a systematic review of open datasets for evaluating and improving large language model safety",
    "url": "https://ojs.aaai.org/index.php/AAAI/article/view/34975",
    "authors": "P RÃ¶ttger,F Pernisi,B Vidgen,D Hovy- Proceedings of the AAAIÂ â€¦, 2025",
    "publication": "ojs.aaai.org",
    "abstract": "The last two years have seen a rapid growth in concerns around the safety of largelanguage models (LLMs). Researchers and practitioners have met these concerns byÂ â€¦",
    "cited_by_count": 52,
    "benchmark_name": "ALERT",
    "benchmark_paper": "ALERT: A Comprehensive Benchmark for Assessing Large Language Models' Safety through Red Teaming"
  },
  {
    "pdf_url": "",
    "title": "Wildguard: Open one-stop moderation tools for safety risks, jailbreaks, and refusals of llms",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2024/hash/0f69b4b96a46f284b726fbd70f74fb3b-Abstract-Datasets_and_Benchmarks_Track.html",
    "authors": "S Han,K Rao,A Ettinger,L Jiangâ€¦Â - Advances inÂ â€¦, 2024",
    "publication": "proceedings.neurips.cc",
    "abstract": "We introduce WildGuard---an open, light-weight moderation tool for LLM safety thatachieves three goals:(1) identifying malicious intent in user prompts,(2) detecting safety risksÂ â€¦",
    "cited_by_count": 169,
    "benchmark_name": "ALERT",
    "benchmark_paper": "ALERT: A Comprehensive Benchmark for Assessing Large Language Models' Safety through Red Teaming"
  },
  {
    "pdf_url": "",
    "title": "Sorry-bench: Systematically evaluating large language model safety refusal",
    "url": "https://arxiv.org/abs/2406.14598",
    "authors": "T Xie,X Qi,Y Zeng,Y Huang,UM Sehwagâ€¦Â - arXiv preprint arXivÂ â€¦, 2024",
    "publication": "arxiv.org",
    "abstract": "Evaluating aligned large language models'(LLMs) ability to recognize and reject unsafe userrequests is crucial for safe, policy-compliant deployments. Existing evaluation effortsÂ â€¦",
    "cited_by_count": 141,
    "benchmark_name": "ALERT",
    "benchmark_paper": "ALERT: A Comprehensive Benchmark for Assessing Large Language Models' Safety through Red Teaming"
  },
  {
    "pdf_url": "",
    "title": "Or-bench: An over-refusal benchmark for large language models",
    "url": "https://arxiv.org/abs/2405.20947",
    "authors": "J Cui,WL Chiang,I Stoica,CJ Hsieh- arXiv preprint arXiv:2405.20947, 2024",
    "publication": "arxiv.org",
    "abstract": "Large Language Models (LLMs) require careful safety alignment to prevent maliciousoutputs. While significant research focuses on mitigating harmful content generation, theÂ â€¦",
    "cited_by_count": 104,
    "benchmark_name": "ALERT",
    "benchmark_paper": "ALERT: A Comprehensive Benchmark for Assessing Large Language Models' Safety through Red Teaming"
  },
  {
    "pdf_url": "",
    "title": "Who is responsible? the data, models, users or regulations? a comprehensive survey on responsible generative ai for a sustainable future",
    "url": "https://arxiv.org/abs/2502.08650",
    "authors": "S Raza,R Qureshi, A Zahid, S Kamawalâ€¦Â - arXiv preprint arXivÂ â€¦, 2025",
    "publication": "arxiv.org",
    "abstract": "Generative AI is moving rapidly from research into real world deployment across sectors,which elevates the need for responsible development, deployment, evaluation, andÂ â€¦",
    "cited_by_count": 13,
    "benchmark_name": "ALERT",
    "benchmark_paper": "ALERT: A Comprehensive Benchmark for Assessing Large Language Models' Safety through Red Teaming"
  },
  {
    "pdf_url": "",
    "title": "On the trustworthiness of generative foundation models: Guideline, assessment, and perspective",
    "url": "https://arxiv.org/abs/2502.14296",
    "authors": "Y Huang,C Gao, S Wu,H Wang,X Wangâ€¦Â - arXiv preprint arXivÂ â€¦, 2025",
    "publication": "arxiv.org",
    "abstract": "Generative Foundation Models (GenFMs) have emerged as transformative tools. However,their widespread adoption raises critical concerns regarding trustworthiness acrossÂ â€¦",
    "cited_by_count": 32,
    "benchmark_name": "ALERT",
    "benchmark_paper": "ALERT: A Comprehensive Benchmark for Assessing Large Language Models' Safety through Red Teaming"
  },
  {
    "pdf_url": "",
    "title": "ART: automatic red-teaming for text-to-image models to protect benign users",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2024/hash/a5c7206fd66e8314bb21a04492359353-Abstract-Conference.html",
    "authors": "G Li,K Chen,S Zhang,J Zhangâ€¦Â - Advances in NeuralÂ â€¦, 2024",
    "publication": "proceedings.neurips.cc",
    "abstract": "Large-scale pre-trained generative models are taking the world by storm, due to theirabilities in generating creative content. Meanwhile, safeguards for these generative modelsÂ â€¦",
    "cited_by_count": 27,
    "benchmark_name": "ALERT",
    "benchmark_paper": "ALERT: A Comprehensive Benchmark for Assessing Large Language Models' Safety through Red Teaming"
  },
  {
    "pdf_url": "",
    "title": "Benchmarking adversarial robustness to bias elicitation in large language models: Scalable automated assessment with llm-as-a-judge",
    "url": "https://link.springer.com/article/10.1007/s10994-025-06862-6",
    "authors": "R Cantini,A Orsino, M Ruggiero,D Talia- Machine Learning, 2025",
    "publication": "Springer",
    "abstract": "The growing integration of Large Language Models (LLMs) into critical societal domains hasraised concerns about embedded biases that can perpetuate stereotypes and undermineÂ â€¦",
    "cited_by_count": 13,
    "benchmark_name": "ALERT",
    "benchmark_paper": "ALERT: A Comprehensive Benchmark for Assessing Large Language Models' Safety through Red Teaming"
  },
  {
    "pdf_url": "",
    "title": "Bi-factorial preference optimization: Balancing safety-helpfulness in language models",
    "url": "https://arxiv.org/abs/2408.15313",
    "authors": "W Zhang,PHS Torr,M Elhoseiny,A Bibi- arXiv preprint arXiv:2408.15313, 2024",
    "publication": "arxiv.org",
    "abstract": "Fine-tuning large language models (LLMs) on human preferences, typically throughreinforcement learning from human feedback (RLHF), has proven successful in enhancingÂ â€¦",
    "cited_by_count": 15,
    "benchmark_name": "ALERT",
    "benchmark_paper": "ALERT: A Comprehensive Benchmark for Assessing Large Language Models' Safety through Red Teaming"
  },
  {
    "pdf_url": "",
    "title": "Blockchain for large language model security and safety: A holistic survey",
    "url": "https://dl.acm.org/doi/abs/10.1145/3715073.3715075",
    "authors": "C Geren,A Board,GG Dagher,T Andersenâ€¦Â - ACM SIGKDDÂ â€¦, 2025",
    "publication": "dl.acm.org",
    "abstract": "With the growing development and deployment of large language models (LLMs) in bothindustrial and academic fields, their security and safety concerns have become increasinglyÂ â€¦",
    "cited_by_count": 21,
    "benchmark_name": "ALERT",
    "benchmark_paper": "ALERT: A Comprehensive Benchmark for Assessing Large Language Models' Safety through Red Teaming"
  },
  {
    "pdf_url": "",
    "title": "A comprehensive survey in llm (-agent) full stack safety: Data, training and deployment",
    "url": "https://arxiv.org/abs/2504.15585",
    "authors": "K Wang,G Zhang,Z Zhou,J Wu,M Yu,S Zhaoâ€¦Â - arXiv preprint arXivÂ â€¦, 2025",
    "publication": "arxiv.org",
    "abstract": "The remarkable success of Large Language Models (LLMs) has illuminated a promisingpathway toward achieving Artificial General Intelligence for both academic and industrialÂ â€¦",
    "cited_by_count": 61,
    "benchmark_name": "AgentHarm",
    "benchmark_paper": "AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents"
  },
  {
    "pdf_url": "",
    "title": "A survey on llm-as-a-judge",
    "url": "https://arxiv.org/abs/2411.15594",
    "authors": "J Gu,X Jiang,Z Shi,H Tan,X Zhai,C Xu,W Liâ€¦Â - arXiv preprint arXivÂ â€¦, 2024",
    "publication": "arxiv.org",
    "abstract": "Accurate and consistent evaluation is crucial for decision-making across numerous fields,yet it remains a challenging task due to inherent subjectivity, variability, and scale. LargeÂ â€¦",
    "cited_by_count": 714,
    "benchmark_name": "AgentHarm",
    "benchmark_paper": "AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents"
  },
  {
    "pdf_url": "",
    "title": "Jailbreaking leading safety-aligned llms with simple adaptive attacks",
    "url": "https://arxiv.org/abs/2404.02151",
    "authors": "M Andriushchenko,F Croce,N Flammarion- arXiv preprint arXivÂ â€¦, 2024",
    "publication": "arxiv.org",
    "abstract": "We show that even the most recent safety-aligned LLMs are not robust to simple adaptivejailbreaking attacks. First, we demonstrate how to successfully leverage access to logprobsÂ â€¦",
    "cited_by_count": 284,
    "benchmark_name": "AgentHarm",
    "benchmark_paper": "AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents"
  },
  {
    "pdf_url": "",
    "title": "Large language model agent: A survey on methodology, applications and challenges",
    "url": "https://arxiv.org/abs/2503.21460",
    "authors": "J Luo,W Zhang,Y Yuan,Y Zhao,J Yang,Y Guâ€¦Â - arXiv preprint arXivÂ â€¦, 2025",
    "publication": "arxiv.org",
    "abstract": "The era of intelligent agents is upon us, driven by revolutionary advancements in largelanguage models. Large Language Model (LLM) agents, with goal-driven behaviors andÂ â€¦",
    "cited_by_count": 43,
    "benchmark_name": "AgentHarm",
    "benchmark_paper": "AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents"
  },
  {
    "pdf_url": "",
    "title": "A survey on trustworthy llm agents: Threats and countermeasures",
    "url": "https://dl.acm.org/doi/abs/10.1145/3711896.3736561",
    "authors": "M Yu, F Meng,X Zhou,S Wang,J Mao, L Panâ€¦Â - Proceedings of the 31stÂ â€¦, 2025",
    "publication": "dl.acm.org",
    "abstract": "With the rapid evolution of Large Language Models (LLMs), LLM-based agents and Multi-agent Systems (MAS) have significantly expanded the capabilities of LLM ecosystems. ThisÂ â€¦",
    "cited_by_count": 43,
    "benchmark_name": "AgentHarm",
    "benchmark_paper": "AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents"
  },
  {
    "pdf_url": "",
    "title": "A survey of self-evolving agents: On path to artificial super intelligence",
    "url": "https://arxiv.org/abs/2507.21046",
    "authors": "H Gao,J Geng,W Hua,M Hu,X Juan,H Liuâ€¦Â - arXiv preprint arXivÂ â€¦, 2025",
    "publication": "arxiv.org",
    "abstract": "Large Language Models (LLMs) have demonstrated strong capabilities but remainfundamentally static, unable to adapt their internal parameters to novel tasks, evolvingÂ â€¦",
    "cited_by_count": 33,
    "benchmark_name": "AgentHarm",
    "benchmark_paper": "AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents"
  },
  {
    "pdf_url": "",
    "title": "The hidden risks of large reasoning models: A safety assessment of r1",
    "url": "https://arxiv.org/abs/2502.12659",
    "authors": "K Zhou,C Liu,X Zhao,S Jangam,J Srinivasaâ€¦Â - arXiv preprint arXivÂ â€¦, 2025",
    "publication": "arxiv.org",
    "abstract": "The rapid development of large reasoning models, such as OpenAI-o3 and DeepSeek-R1,has led to significant improvements in complex reasoning over non-reasoning largeÂ â€¦",
    "cited_by_count": 36,
    "benchmark_name": "AgentHarm",
    "benchmark_paper": "AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents"
  },
  {
    "pdf_url": "",
    "title": "On the trustworthiness of generative foundation models: Guideline, assessment, and perspective",
    "url": "https://arxiv.org/abs/2502.14296",
    "authors": "Y Huang,C Gao, S Wu,H Wang,X Wangâ€¦Â - arXiv preprint arXivÂ â€¦, 2025",
    "publication": "arxiv.org",
    "abstract": "Generative Foundation Models (GenFMs) have emerged as transformative tools. However,their widespread adoption raises critical concerns regarding trustworthiness acrossÂ â€¦",
    "cited_by_count": 32,
    "benchmark_name": "AgentHarm",
    "benchmark_paper": "AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents"
  },
  {
    "pdf_url": "",
    "title": "A comprehensive survey of self-evolving ai agents: A new paradigm bridging foundation models and lifelong agentic systems",
    "url": "https://arxiv.org/abs/2508.07407",
    "authors": "J Fang,Y Peng,X Zhang,Y Wang, X Yiâ€¦Â - arXiv preprint arXivÂ â€¦, 2025",
    "publication": "arxiv.org",
    "abstract": "Recent advances in large language models have sparked growing interest in AI agentscapable of solving complex, real-world tasks. However, most existing agent systems rely onÂ â€¦",
    "cited_by_count": 18,
    "benchmark_name": "AgentHarm",
    "benchmark_paper": "AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents"
  },
  {
    "pdf_url": "",
    "title": "From llm reasoning to autonomous ai agents: A comprehensive review",
    "url": "https://arxiv.org/abs/2504.19678",
    "authors": "MA Ferrag,N Tihanyi,M Debbah- arXiv preprint arXiv:2504.19678, 2025",
    "publication": "arxiv.org",
    "abstract": "Large language models and autonomous AI agents have evolved rapidly, resulting in adiverse array of evaluation benchmarks, frameworks, and collaboration protocols. HoweverÂ â€¦",
    "cited_by_count": 61,
    "benchmark_name": "AgentHarm",
    "benchmark_paper": "AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents"
  },
  {
    "pdf_url": "",
    "title": "Safetyprompts: a systematic review of open datasets for evaluating and improving large language model safety",
    "url": "https://ojs.aaai.org/index.php/AAAI/article/view/34975",
    "authors": "P RÃ¶ttger,F Pernisi,B Vidgen,D Hovy- Proceedings of the AAAIÂ â€¦, 2025",
    "publication": "ojs.aaai.org",
    "abstract": "The last two years have seen a rapid growth in concerns around the safety of largelanguage models (LLMs). Researchers and practitioners have met these concerns byÂ â€¦",
    "cited_by_count": 52,
    "benchmark_name": "SimpleSafetyTest",
    "benchmark_paper": "SimpleSafetyTest: Universal Safety Testing for Language Models"
  },
  {
    "pdf_url": "",
    "title": "Jailbreaking leading safety-aligned llms with simple adaptive attacks",
    "url": "https://arxiv.org/abs/2404.02151",
    "authors": "M Andriushchenko,F Croce,N Flammarion- arXiv preprint arXivÂ â€¦, 2024",
    "publication": "arxiv.org",
    "abstract": "We show that even the most recent safety-aligned LLMs are not robust to simple adaptivejailbreaking attacks. First, we demonstrate how to successfully leverage access to logprobsÂ â€¦",
    "cited_by_count": 284,
    "benchmark_name": "SimpleSafetyTest",
    "benchmark_paper": "SimpleSafetyTest: Universal Safety Testing for Language Models"
  },
  {
    "pdf_url": "",
    "title": "Wildguard: Open one-stop moderation tools for safety risks, jailbreaks, and refusals of llms",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2024/hash/0f69b4b96a46f284b726fbd70f74fb3b-Abstract-Datasets_and_Benchmarks_Track.html",
    "authors": "S Han,K Rao,A Ettinger,L Jiangâ€¦Â - Advances inÂ â€¦, 2024",
    "publication": "proceedings.neurips.cc",
    "abstract": "We introduce WildGuard---an open, light-weight moderation tool for LLM safety thatachieves three goals:(1) identifying malicious intent in user prompts,(2) detecting safety risksÂ â€¦",
    "cited_by_count": 169,
    "benchmark_name": "SimpleSafetyTest",
    "benchmark_paper": "SimpleSafetyTest: Universal Safety Testing for Language Models"
  },
  {
    "pdf_url": "",
    "title": "Xstest: A test suite for identifying exaggerated safety behaviours in large language models",
    "url": "https://arxiv.org/abs/2308.01263",
    "authors": "P RÃ¶ttger,HR Kirk,B Vidgen,G Attanasioâ€¦Â - arXiv preprint arXivÂ â€¦, 2023",
    "publication": "arxiv.org",
    "abstract": "Without proper safeguards, large language models will readily follow malicious instructionsand generate toxic content. This risk motivates safety efforts such as red-teaming and largeÂ â€¦",
    "cited_by_count": 349,
    "benchmark_name": "SimpleSafetyTest",
    "benchmark_paper": "SimpleSafetyTest: Universal Safety Testing for Language Models"
  },
  {
    "pdf_url": "",
    "title": "Sorry-bench: Systematically evaluating large language model safety refusal",
    "url": "https://arxiv.org/abs/2406.14598",
    "authors": "T Xie,X Qi,Y Zeng,Y Huang,UM Sehwagâ€¦Â - arXiv preprint arXivÂ â€¦, 2024",
    "publication": "arxiv.org",
    "abstract": "Evaluating aligned large language models'(LLMs) ability to recognize and reject unsafe userrequests is crucial for safe, policy-compliant deployments. Existing evaluation effortsÂ â€¦",
    "cited_by_count": 141,
    "benchmark_name": "SimpleSafetyTest",
    "benchmark_paper": "SimpleSafetyTest: Universal Safety Testing for Language Models"
  },
  {
    "pdf_url": "",
    "title": "The responsible foundation model development cheatsheet: A review of tools & resources",
    "url": "https://arxiv.org/abs/2406.16746",
    "authors": "S Longpre,S Biderman,A Albalakâ€¦Â - arXiv preprint arXivÂ â€¦, 2024",
    "publication": "arxiv.org",
    "abstract": "Foundation model development attracts a rapidly expanding body of contributors, scientists,and applications. To help shape responsible development practices, we introduce theÂ â€¦",
    "cited_by_count": 15,
    "benchmark_name": "SimpleSafetyTest",
    "benchmark_paper": "SimpleSafetyTest: Universal Safety Testing for Language Models"
  },
  {
    "pdf_url": "",
    "title": "Aegis: Online adaptive ai content safety moderation with ensemble of llm experts",
    "url": "https://arxiv.org/abs/2404.05993",
    "authors": "S Ghosh,P Varshney,E Galinkin,C Parisien- arXiv preprint arXivÂ â€¦, 2024",
    "publication": "arxiv.org",
    "abstract": "As Large Language Models (LLMs) and generative AI become more widespread, thecontent safety risks associated with their use also increase. We find a notable deficiency inÂ â€¦",
    "cited_by_count": 68,
    "benchmark_name": "SimpleSafetyTest",
    "benchmark_paper": "SimpleSafetyTest: Universal Safety Testing for Language Models"
  },
  {
    "pdf_url": "",
    "title": "Latent adversarial training improves robustness to persistent harmful behaviors in llms",
    "url": "https://arxiv.org/abs/2407.15549",
    "authors": "A Sheshadri,A Ewart,P Guo,A Lynch,C Wuâ€¦Â - arXiv preprint arXivÂ â€¦, 2024",
    "publication": "arxiv.org",
    "abstract": "Large language models (LLMs) can often be made to behave in undesirable ways that theyare explicitly fine-tuned not to. For example, the LLM red-teaming literature has produced aÂ â€¦",
    "cited_by_count": 39,
    "benchmark_name": "SimpleSafetyTest",
    "benchmark_paper": "SimpleSafetyTest: Universal Safety Testing for Language Models"
  },
  {
    "pdf_url": "",
    "title": "Against The Achilles' Heel: A Survey on Red Teaming for Generative Models",
    "url": "https://www.jair.org/index.php/jair/article/view/17654",
    "authors": "L Lin,H Mu,Z Zhai,M Wang,Y Wang,R Wangâ€¦Â - Journal of ArtificialÂ â€¦, 2025",
    "publication": "jair.org",
    "abstract": "Generative models are rapidly gaining popularity and being integrated into everydayapplications, raising concerns over their safe use as various vulnerabilities are exposed. InÂ â€¦",
    "cited_by_count": 41,
    "benchmark_name": "SimpleSafetyTest",
    "benchmark_paper": "SimpleSafetyTest: Universal Safety Testing for Language Models"
  },
  {
    "pdf_url": "",
    "title": "Guardreasoner: Towards reasoning-based llm safeguards",
    "url": "https://arxiv.org/abs/2501.18492",
    "authors": "Y Liu,H Gao,S Zhai,J Xia,T Wu, Z Xueâ€¦Â - arXiv preprint arXivÂ â€¦, 2025",
    "publication": "arxiv.org",
    "abstract": "As LLMs increasingly impact safety-critical applications, ensuring their safety usingguardrails remains a key challenge. This paper proposes GuardReasoner, a new safeguardÂ â€¦",
    "cited_by_count": 49,
    "benchmark_name": "SimpleSafetyTest",
    "benchmark_paper": "SimpleSafetyTest: Universal Safety Testing for Language Models"
  },
  {
    "pdf_url": "",
    "title": "Adversarial attacks of vision tasks in the past 10 years: A survey",
    "url": "https://dl.acm.org/doi/abs/10.1145/3743126",
    "authors": "C Zhang, L Zhou,X Xu, J Wu,Z Liu- ACM Computing Surveys, 2025",
    "publication": "dl.acm.org",
    "abstract": "With the advent of Large Vision-Language Models (LVLMs), new attack vectors, such ascognitive bias, prompt injection, and jailbreaking, have emerged. Understanding theseÂ â€¦",
    "cited_by_count": 17,
    "benchmark_name": "XSTest",
    "benchmark_paper": "XSTest: A Test Suite for Identifying Exaggerated Safety Behaviors in Large Language Models"
  },
  {
    "pdf_url": "",
    "title": "Safetyprompts: a systematic review of open datasets for evaluating and improving large language model safety",
    "url": "https://ojs.aaai.org/index.php/AAAI/article/view/34975",
    "authors": "P RÃ¶ttger,F Pernisi,B Vidgen,D Hovy- Proceedings of the AAAIÂ â€¦, 2025",
    "publication": "ojs.aaai.org",
    "abstract": "The last two years have seen a rapid growth in concerns around the safety of largelanguage models (LLMs). Researchers and practitioners have met these concerns byÂ â€¦",
    "cited_by_count": 52,
    "benchmark_name": "XSTest",
    "benchmark_paper": "XSTest: A Test Suite for Identifying Exaggerated Safety Behaviors in Large Language Models"
  },
  {
    "pdf_url": "",
    "title": "The llama 3 herd of models",
    "url": "https://arxiv.org/abs/2407.21783",
    "authors": "A Grattafiori,A Dubey, A Jauhri, A Pandeyâ€¦Â - arXiv preprint arXivÂ â€¦, 2024",
    "publication": "arxiv.org",
    "abstract": "Modern artificial intelligence (AI) systems are powered by foundation models. This paperpresents a new set of foundation models, called Llama 3. It is a herd of language modelsÂ â€¦",
    "cited_by_count": 3144,
    "benchmark_name": "XSTest",
    "benchmark_paper": "XSTest: A Test Suite for Identifying Exaggerated Safety Behaviors in Large Language Models"
  },
  {
    "pdf_url": "",
    "title": "Fine-tuning aligned language models compromises safety, even when users do not intend to!",
    "url": "https://arxiv.org/abs/2310.03693",
    "authors": "X Qi,Y Zeng,T Xie,PY Chen,R Jia,P Mittalâ€¦Â - arXiv preprint arXivÂ â€¦, 2023",
    "publication": "arxiv.org",
    "abstract": "Optimizing large language models (LLMs) for downstream use cases often involves thecustomization of pre-trained LLMs through further fine-tuning. Meta's open release of LlamaÂ â€¦",
    "cited_by_count": 841,
    "benchmark_name": "XSTest",
    "benchmark_paper": "XSTest: A Test Suite for Identifying Exaggerated Safety Behaviors in Large Language Models"
  },
  {
    "pdf_url": "",
    "title": "Jailbreakbench: An open robustness benchmark for jailbreaking large language models",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2024/hash/63092d79154adebd7305dfd498cbff70-Abstract-Datasets_and_Benchmarks_Track.html",
    "authors": "P Chao,E Debenedetti,A Robeyâ€¦Â - Advances inÂ â€¦, 2024",
    "publication": "proceedings.neurips.cc",
    "abstract": "Jailbreak attacks cause large language models (LLMs) to generate harmful, unethical, orotherwise objectionable content. Evaluating these attacks presents a number of challengesÂ â€¦",
    "cited_by_count": 350,
    "benchmark_name": "XSTest",
    "benchmark_paper": "XSTest: A Test Suite for Identifying Exaggerated Safety Behaviors in Large Language Models"
  },
  {
    "pdf_url": "",
    "title": "Openai o1 system card",
    "url": "https://arxiv.org/abs/2412.16720",
    "authors": "A Jaech,A Kalai,A Lerer, A Richardsonâ€¦Â - arXiv preprint arXivÂ â€¦, 2024",
    "publication": "arxiv.org",
    "abstract": "The o1 model series is trained with large-scale reinforcement learning to reason using chainof thought. These advanced reasoning capabilities provide new avenues for improving theÂ â€¦",
    "cited_by_count": 1203,
    "benchmark_name": "XSTest",
    "benchmark_paper": "XSTest: A Test Suite for Identifying Exaggerated Safety Behaviors in Large Language Models"
  },
  {
    "pdf_url": "",
    "title": "Rewardbench: Evaluating reward models for language modeling",
    "url": "https://arxiv.org/abs/2403.13787",
    "authors": "N Lambert,V Pyatkin,J Morrison,LJ Mirandaâ€¦Â - arXiv preprint arXivÂ â€¦, 2024",
    "publication": "arxiv.org",
    "abstract": "Reward models (RMs) are at the crux of successfully using RLHF to align pretrained modelsto human preferences, yet there has been relatively little study that focuses on evaluation ofÂ â€¦",
    "cited_by_count": 393,
    "benchmark_name": "XSTest",
    "benchmark_paper": "XSTest: A Test Suite for Identifying Exaggerated Safety Behaviors in Large Language Models"
  },
  {
    "pdf_url": "",
    "title": "Gptfuzzer: Red teaming large language models with auto-generated jailbreak prompts",
    "url": "https://arxiv.org/abs/2309.10253",
    "authors": "J Yu,X Lin,Z Yu,X Xing- arXiv preprint arXiv:2309.10253, 2023",
    "publication": "arxiv.org",
    "abstract": "Large language models (LLMs) have recently experienced tremendous popularity and arewidely used from casual conversations to AI-driven programming. However, despite theirÂ â€¦",
    "cited_by_count": 446,
    "benchmark_name": "XSTest",
    "benchmark_paper": "XSTest: A Test Suite for Identifying Exaggerated Safety Behaviors in Large Language Models"
  },
  {
    "pdf_url": "",
    "title": "Improving alignment and robustness with circuit breakers",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2024/hash/97ca7168c2c333df5ea61ece3b3276e1-Abstract-Conference.html",
    "authors": "A Zou,L Phan,J Wang, D Duenasâ€¦Â - Advances inÂ â€¦, 2024",
    "publication": "proceedings.neurips.cc",
    "abstract": "AI systems can take harmful actions and are highly vulnerable to adversarial attacks. Wepresent an approach, inspired by recent advances in representation engineering, thatÂ â€¦",
    "cited_by_count": 139,
    "benchmark_name": "XSTest",
    "benchmark_paper": "XSTest: A Test Suite for Identifying Exaggerated Safety Behaviors in Large Language Models"
  },
  {
    "pdf_url": "",
    "title": "Wildguard: Open one-stop moderation tools for safety risks, jailbreaks, and refusals of llms",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2024/hash/0f69b4b96a46f284b726fbd70f74fb3b-Abstract-Datasets_and_Benchmarks_Track.html",
    "authors": "S Han,K Rao,A Ettinger,L Jiangâ€¦Â - Advances inÂ â€¦, 2024",
    "publication": "proceedings.neurips.cc",
    "abstract": "We introduce WildGuard---an open, light-weight moderation tool for LLM safety thatachieves three goals:(1) identifying malicious intent in user prompts,(2) detecting safety risksÂ â€¦",
    "cited_by_count": 169,
    "benchmark_name": "XSTest",
    "benchmark_paper": "XSTest: A Test Suite for Identifying Exaggerated Safety Behaviors in Large Language Models"
  }
]