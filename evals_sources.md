# Sources for techniques evaluation
## DPO AND PPO
- Evals: Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study: https://arxiv.org/pdf/2404.10719v1
## CAI (Constitutional AI)

- Benchmark TrustLM, on 7B LLama : https://arxiv.org/html/2502.15861v1
- HarmBench, multiple small scale models : https://arxiv.org/html/2503.17365v1

## DPO

- Beaver 7B model, SafeDPO method, see Appendix : https://arxiv.org/pdf/2505.20065
- Tulu, multiple sizes, ToxiGen dataset : https://arxiv.org/pdf/2311.10702

## Finetuning

- Jailbreak attaacks, evaluate on 7B and slightly 13B model, also a lot of tables with evaluations: https://arxiv.org/pdf/2406.18510

- 



## Misc

- Wildguard model, this has some table with models evaluated : https://proceedings.neurips.cc/paper_files/paper/2024/file/0f69b4b96a46f284b726fbd70f74fb3b-Paper-Datasets_and_Benchmarks_Track.pdf
