benchmark_name,benchmark_paper,title,authors,publication,year,url,pdf_url,abstract,cited_by_count,paper_type
TruthfulQA,TruthfulQA: Measuring How Models Mimic Human Falsehoods,Simpo: Simple preference optimization with a reference-free reward,"Y Meng,M Xia,D Chen- Advances in Neural Information …, 2024",proceedings.neurips.cc,,https://proceedings.neurips.cc/paper_files/paper/2024/hash/e099c1c9699814af0be873a175361713-Abstract-Conference.html,,Abstract Direct Preference Optimization (DPO) is a widely used offline preferenceoptimization algorithm that reparameterizes reward functions in reinforcement learning from …,657.0,Methodology
TruthfulQA,TruthfulQA: Measuring How Models Mimic Human Falsehoods,Llama 2: Open foundation and fine-tuned chat models,"H Touvron,L Martin,K Stone, P Albert… - arXiv preprint arXiv …, 2023",arxiv.org,,https://arxiv.org/abs/2307.09288,,"In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned largelanguage models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine …",19070.0,Methodology
HaluEval,HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models,Ragas: Automated evaluation of retrieval augmented generation,"S Es,J James,LE Anke… - Proceedings of the 18th …, 2024",aclanthology.org,,https://aclanthology.org/2024.eacl-demo.16/,,"Abstract We introduce RAGAs (Retrieval Augmented Generation Assessment), a frameworkfor reference-free evaluation of Retrieval Augmented Generation (RAG) pipelines. RAGAs is …",795.0,Methodology
TRUE,TRUE: Re-evaluating Factual Consistency Evaluation,Detecting hallucinations in large language models using semantic entropy,"S Farquhar,J Kossen,L Kuhn,Y Gal- Nature, 2024",nature.com,,https://www.nature.com/articles/s41586-024-07421-0,,"Large language model (LLM) systems, such as ChatGPT or Gemini, can show impressivereasoning and question-answering capabilities but often 'hallucinate'false outputs and …",772.0,Methodology
TRUE,TRUE: Re-evaluating Factual Consistency Evaluation,Enabling large language models to generate text with citations,"T Gao,H Yen,J Yu,D Chen- arXiv preprint arXiv:2305.14627, 2023",arxiv.org,,https://arxiv.org/abs/2305.14627,,"Large language models (LLMs) have emerged as a widely-used tool for informationseeking, but their generated outputs are prone to hallucination. In this work, our aim is to …",437.0,Methodology
TRUE,TRUE: Re-evaluating Factual Consistency Evaluation,"Rarr: Researching and revising what language models say, using language models","L Gao,Z Dai,P Pasupat,A Chen,AT Chaganty… - arXiv preprint arXiv …, 2022",arxiv.org,,https://arxiv.org/abs/2210.08726,,"Language models (LMs) now excel at many tasks such as few-shot learning, questionanswering, reasoning, and dialog. However, they sometimes generate unsupported or …",395.0,Methodology
TRUE,TRUE: Re-evaluating Factual Consistency Evaluation,Making retrieval-augmented language models robust to irrelevant context,"O Yoran,T Wolfson,O Ram,J Berant- arXiv preprint arXiv:2310.01558, 2023",arxiv.org,,https://arxiv.org/abs/2310.01558,,"Retrieval-augmented language models (RALMs) hold promise to produce languageunderstanding systems that are are factual, efficient, and up-to-date. An important …",259.0,Methodology
TRUE,TRUE: Re-evaluating Factual Consistency Evaluation,Does fine-tuning llms on new knowledge encourage hallucinations?,"Z Gekhman,G Yona,R Aharoni,M Eyal… - arXiv preprint arXiv …, 2024",arxiv.org,,https://arxiv.org/abs/2405.05904,,"When large language models are aligned via supervised fine-tuning, they may encounternew factual information that was not acquired through pre-training. It is often conjectured that …",198.0,Methodology
TRUE,TRUE: Re-evaluating Factual Consistency Evaluation,AlignScore: Evaluating factual consistency with a unified alignment function,"Y Zha,Y Yang,R Li,Z Hu- arXiv preprint arXiv:2305.16739, 2023",arxiv.org,,https://arxiv.org/abs/2305.16739,,Many text generation applications require the generated text to be factually consistent withinput information. Automatic evaluation of factual consistency is challenging. Previous work …,271.0,Methodology
FIB,Evaluating the Factual Consistency of Large Language Models Through News Summarization,Trustllm: Trustworthiness in large language models,"Y Huang,L Sun,H Wang, S Wu,Q Zhang, Y Li… - arXiv preprint arXiv …, 2024",arxiv.org,,https://arxiv.org/abs/2401.05561,,"Large language models (LLMs), exemplified by ChatGPT, have gained considerableattention for their excellent natural language processing capabilities. Nonetheless, these …",487.0,Methodology
FIB,Evaluating the Factual Consistency of Large Language Models Through News Summarization,Position: Trustllm: Trustworthiness in large language models,"Y Huang,L Sun,H Wang, S Wu… - International …, 2024",proceedings.mlr.press,,http://proceedings.mlr.press/v235/huang24x.html,,"Large language models (LLMs) have gained considerable attention for their excellentnatural language processing capabilities. Nonetheless, these LLMs present many …",88.0,Methodology
FIB,Evaluating the Factual Consistency of Large Language Models Through News Summarization,Lm vs lm: Detecting factual errors via cross examination,"R Cohen, M Hamri,M Geva,A Globerson- arXiv preprint arXiv …, 2023",arxiv.org,,https://arxiv.org/abs/2305.13281,,"A prominent weakness of modern language models (LMs) is their tendency to generatefactually incorrect text, which hinders their usability. A natural question is whether such …",180.0,Methodology
FEVER,FEVER: a Large-scale Dataset for Fact Extraction and VERification,React: Synergizing reasoning and acting in language models,"S Yao,J Zhao,D Yu,N Du,I Shafran… - The eleventh …, 2022",openreview.net,,https://openreview.net/forum?id=WE_vluYUL-X,,"While large language models (LLMs) have demonstrated impressive capabilities acrosstasks in language understanding and interactive decision making, their abilities for …",5155.0,Methodology
FEVER,FEVER: a Large-scale Dataset for Fact Extraction and VERification,Factscore: Fine-grained atomic evaluation of factual precision in long form text generation,"S Min,K Krishna,X Lyu,M Lewis,W Yih… - arXiv preprint arXiv …, 2023",arxiv.org,,https://arxiv.org/abs/2305.14251,,Evaluating the factuality of long-form text generated by large language models (LMs) is non-trivial because (1) generations often contain a mixture of supported and unsupported pieces …,875.0,Methodology
FEVER,FEVER: a Large-scale Dataset for Fact Extraction and VERification,Beyond the imitation game: Quantifying and extrapolating the capabilities of language models,"A Srivastava, A Rastogi, A Rao,AAM Shoeb… - … on machine learning …, 2023",openreview.net,,https://openreview.net/forum?id=uyTL5Bvosj&nesting=2&sort=date-desc,,"Language models demonstrate both quantitative improvement and new qualitativecapabilities with increasing scale. Despite their potentially transformative impact, these new …",2020.0,Methodology
FEVER,FEVER: a Large-scale Dataset for Fact Extraction and VERification,Improving text embeddings with large language models,"L Wang,N Yang,X Huang,L Yang… - arXiv preprint arXiv …, 2023",arxiv.org,,https://arxiv.org/abs/2401.00368,,"In this paper, we introduce a novel and simple method for obtaining high-quality textembeddings using only synthetic data and less than 1k training steps. Unlike existing …",579.0,Methodology
FEVER,FEVER: a Large-scale Dataset for Fact Extraction and VERification,Text embeddings by weakly-supervised contrastive pre-training,"L Wang,N Yang,X Huang,B Jiao,L Yang… - arXiv preprint arXiv …, 2022",arxiv.org,,https://arxiv.org/abs/2212.03533,,"This paper presents E5, a family of state-of-the-art text embeddings that transfer well to awide range of tasks. The model is trained in a contrastive manner with weak supervision …",876.0,Methodology
HalluLens,HalluLens: LLM Hallucination Benchmark,Stress testing deliberative alignment for anti-scheming training,"B Schoen, E Nitishinskaya,M Balesni… - arXiv preprint arXiv …, 2025",arxiv.org,,https://arxiv.org/abs/2509.15541,,"Highly capable AI systems could secretly pursue misaligned goals--what we call"" scheming"".Because a scheming AI would deliberately try to hide its misaligned goals and actions …",4.0,Methodology
HalluLens,HalluLens: LLM Hallucination Benchmark,The hallucination tax of reinforcement finetuning,"L Song,T Shi,J Zhao- arXiv preprint arXiv:2505.13988, 2025",arxiv.org,,https://arxiv.org/abs/2505.13988,,"Reinforcement finetuning (RFT) has become a standard approach for enhancing thereasoning capabilities of large language models (LLMs). However, its impact on model …",8.0,Methodology
FreshQA,FreshLLMs: Refreshing Large Language Models with Search Engine Augmentation,Trustllm: Trustworthiness in large language models,"Y Huang,L Sun,H Wang, S Wu,Q Zhang, Y Li… - arXiv preprint arXiv …, 2024",arxiv.org,,https://arxiv.org/abs/2401.05561,,"Large language models (LLMs), exemplified by ChatGPT, have gained considerableattention for their excellent natural language processing capabilities. Nonetheless, these …",487.0,Methodology
FreshQA,FreshLLMs: Refreshing Large Language Models with Search Engine Augmentation,Raft: Adapting language model to domain specific rag,"T Zhang,SG Patil,N Jain,S Shen,M Zaharia… - arXiv preprint arXiv …, 2024",arxiv.org,,https://arxiv.org/abs/2403.10131,,"Pretraining Large Language Models (LLMs) on large corpora of textual data is now astandard paradigm. When using these LLMs for many downstream applications, it is …",296.0,Methodology
ETHICS,Aligning AI With Shared Human Values,Universal and transferable adversarial attacks on aligned language models,"A Zou,Z Wang,N Carlini,M Nasr,JZ Kolter… - arXiv preprint arXiv …, 2023",arxiv.org,,https://arxiv.org/abs/2307.15043,,"Because"" out-of-the-box"" large language models are capable of generating a great deal ofobjectionable content, recent work has focused on aligning these models in an attempt to …",2181.0,Methodology
ETHICS,Aligning AI With Shared Human Values,Beyond the imitation game: Quantifying and extrapolating the capabilities of language models,"A Srivastava, A Rastogi, A Rao,AAM Shoeb… - … on machine learning …, 2023",openreview.net,,https://openreview.net/forum?id=uyTL5Bvosj&nesting=2&sort=date-desc,,"Language models demonstrate both quantitative improvement and new qualitativecapabilities with increasing scale. Despite their potentially transformative impact, these new …",2020.0,Methodology
ETHICS,Aligning AI With Shared Human Values,Tree of attacks: Jailbreaking black-box llms automatically,"A Mehrotra,M Zampetakis… - Advances in …, 2024",proceedings.neurips.cc,,https://proceedings.neurips.cc/paper_files/paper/2024/hash/70702e8cbb4890b4a467b984ae59828a-Abstract-Conference.html,,"Abstract While Large Language Models (LLMs) display versatile functionality, they continueto generate harmful, biased, and toxic content, as demonstrated by the prevalence of human …",424.0,Methodology
ETHICS,Aligning AI With Shared Human Values,Scaling and evaluating sparse autoencoders,"L Gao,TD la Tour, H Tillman,G Goh, R Troll… - arXiv preprint arXiv …, 2024",arxiv.org,,https://arxiv.org/abs/2406.04093,,Sparse autoencoders provide a promising unsupervised approach for extractinginterpretable features from a language model by reconstructing activations from a sparse …,303.0,Methodology
ETHICS,Aligning AI With Shared Human Values,Trustllm: Trustworthiness in large language models,"Y Huang,L Sun,H Wang, S Wu,Q Zhang, Y Li… - arXiv preprint arXiv …, 2024",arxiv.org,,https://arxiv.org/abs/2401.05561,,"Large language models (LLMs), exemplified by ChatGPT, have gained considerableattention for their excellent natural language processing capabilities. Nonetheless, these …",487.0,Methodology
Moral Stories,Moral Stories: Situated Reasoning about Norms Intents Actions and their Consequences,Refiner: Reasoning feedback on intermediate representations,"D Paul,M Ismayilzada,M Peyrard,B Borges… - arXiv preprint arXiv …, 2023",arxiv.org,,https://arxiv.org/abs/2304.01904,,"Language models (LMs) have recently shown remarkable performance on reasoning tasksby explicitly generating intermediate inferences, eg, chain-of-thought prompting. However …",242.0,Methodology
Moral Stories,Moral Stories: Situated Reasoning about Norms Intents Actions and their Consequences,Training socially aligned language models in simulated human society,"R Liu,R Yang,C Jia,G Zhang,D Zhou… - arXiv preprint arXiv …, 2023",proceedings.iclr.cc,,https://proceedings.iclr.cc/paper_files/paper/2024/file/d763b4a2dde0ae7b77498516ce9f439e-Paper-Conference.pdf,,"Social alignment in AI systems aims to ensure that these models behave according toestablished societal values. However, unlike humans, who derive consensus on value …",142.0,Methodology
Jiminy Cricket,What Would Jiminy Cricket Do? Towards Agents That Behave Morally,Trustllm: Trustworthiness in large language models,"Y Huang,L Sun,H Wang, S Wu,Q Zhang, Y Li… - arXiv preprint arXiv …, 2024",arxiv.org,,https://arxiv.org/abs/2401.05561,,"Large language models (LLMs), exemplified by ChatGPT, have gained considerableattention for their excellent natural language processing capabilities. Nonetheless, these …",487.0,Methodology
Jiminy Cricket,What Would Jiminy Cricket Do? Towards Agents That Behave Morally,Position: Trustllm: Trustworthiness in large language models,"Y Huang,L Sun,H Wang, S Wu… - International …, 2024",proceedings.mlr.press,,http://proceedings.mlr.press/v235/huang24x.html,,"Large language models (LLMs) have gained considerable attention for their excellentnatural language processing capabilities. Nonetheless, these LLMs present many …",88.0,Methodology
Jiminy Cricket,What Would Jiminy Cricket Do? Towards Agents That Behave Morally,Multi-turn reinforcement learning with preference human feedback,"L Shani,A Rosenberg,A Cassel… - Advances in …, 2024",proceedings.neurips.cc,,https://proceedings.neurips.cc/paper_files/paper/2024/hash/d77a7b289361abff82bdd2fb537ae152-Abstract-Conference.html,,"Abstract Reinforcement Learning from Human Feedback (RLHF) has become the standardapproach for aligning Large Language Models (LLMs) with human preferences, allowing …",51.0,Methodology
Jiminy Cricket,What Would Jiminy Cricket Do? Towards Agents That Behave Morally,Moca: Measuring human-language model alignment on causal and moral judgment tasks,"A Nie,Y Zhang,AS Amdekar,C Piech… - Advances in …, 2023",proceedings.neurips.cc,,https://proceedings.neurips.cc/paper_files/paper/2023/hash/f751c6f8bfb52c60f43942896fe65904-Abstract-Conference.html,,Human commonsense understanding of the physical and social world is organized aroundintuitive theories. These theories support making causal and moral judgments. When …,69.0,Methodology
SCRUPLES,Scruples: A Corpus of Community Ethical Judgments on 32000 Real-Life Anecdotes,Moca: Measuring human-language model alignment on causal and moral judgment tasks,"A Nie,Y Zhang,AS Amdekar,C Piech… - Advances in …, 2023",proceedings.neurips.cc,,https://proceedings.neurips.cc/paper_files/paper/2023/hash/f751c6f8bfb52c60f43942896fe65904-Abstract-Conference.html,,Human commonsense understanding of the physical and social world is organized aroundintuitive theories. These theories support making causal and moral judgments. When …,69.0,Methodology
Social Chemistry 101,Social Chemistry 101: Learning to Reason about Social and Moral Norms,Trustllm: Trustworthiness in large language models,"Y Huang,L Sun,H Wang, S Wu,Q Zhang, Y Li… - arXiv preprint arXiv …, 2024",arxiv.org,,https://arxiv.org/abs/2401.05561,,"Large language models (LLMs), exemplified by ChatGPT, have gained considerableattention for their excellent natural language processing capabilities. Nonetheless, these …",487.0,Methodology
Social Chemistry 101,Social Chemistry 101: Learning to Reason about Social and Moral Norms,Position: Trustllm: Trustworthiness in large language models,"Y Huang,L Sun,H Wang, S Wu… - International …, 2024",proceedings.mlr.press,,http://proceedings.mlr.press/v235/huang24x.html,,"Large language models (LLMs) have gained considerable attention for their excellentnatural language processing capabilities. Nonetheless, these LLMs present many …",88.0,Methodology
Delphi,Delphi: Towards Machine Ethics and Norms,Collective constitutional ai: Aligning a language model with public input,"S Huang,D Siddarth, L Lovitt,TI Liao… - Proceedings of the …, 2024",dl.acm.org,,https://dl.acm.org/doi/abs/10.1145/3630106.3658979,,"There is growing consensus that language model (LM) developers should not be the soledeciders of LM behavior, creating a need for methods that enable the broader public to …",102.0,Methodology
STORAL,A Corpus for Understanding and Generating Moral Stories,Slam-omni: Timbre-controllable voice interaction system with single-stage training,"W Chen,Z Ma,R Yan,Y Liang,X Li, R Xu,Z Niu… - arXiv preprint arXiv …, 2024",arxiv.org,,https://arxiv.org/abs/2412.15649,,"Recent advancements highlight the potential of end-to-end real-time spoken dialoguesystems, showcasing their low latency and high quality. In this paper, we introduce SLAM …",36.0,Methodology
Moral Integrity Corpus,The Moral Integrity Corpus: A Benchmark for Ethical Dialogue Systems,Training socially aligned language models in simulated human society,"R Liu,R Yang,C Jia,G Zhang,D Zhou… - arXiv preprint arXiv …, 2023",proceedings.iclr.cc,,https://proceedings.iclr.cc/paper_files/paper/2024/file/d763b4a2dde0ae7b77498516ce9f439e-Paper-Conference.pdf,,"Social alignment in AI systems aims to ensure that these models behave according toestablished societal values. However, unlike humans, who derive consensus on value …",142.0,Methodology
Moral Integrity Corpus,The Moral Integrity Corpus: A Benchmark for Ethical Dialogue Systems,Moca: Measuring human-language model alignment on causal and moral judgment tasks,"A Nie,Y Zhang,AS Amdekar,C Piech… - Advances in …, 2023",proceedings.neurips.cc,,https://proceedings.neurips.cc/paper_files/paper/2023/hash/f751c6f8bfb52c60f43942896fe65904-Abstract-Conference.html,,Human commonsense understanding of the physical and social world is organized aroundintuitive theories. These theories support making causal and moral judgments. When …,69.0,Methodology
RobustBench,RobustBench: a standardized adversarial robustness benchmark,Better diffusion models further improve adversarial training,"Z Wang,T Pang,C Du,M Lin… - … on machine learning, 2023",proceedings.mlr.press,,http://proceedings.mlr.press/v202/wang23ad.html,,It has been recognized that the data generated by the denoising diffusion probabilisticmodel (DDPM) improves adversarial training. After two years of rapid development in …,347.0,Methodology
RobustBench,RobustBench: a standardized adversarial robustness benchmark,Visual adversarial examples jailbreak aligned large language models,"X Qi,K Huang,A Panda,P Henderson… - Proceedings of the …, 2024",ojs.aaai.org,,https://ojs.aaai.org/index.php/AAAI/article/view/30150,,"Warning: this paper contains data, prompts, and model outputs that are offensive in nature.Recently, there has been a surge of interest in integrating vision into Large Language …",331.0,Methodology
RobustBench,RobustBench: a standardized adversarial robustness benchmark,Diffusion models for adversarial purification,"W Nie,B Guo,Y Huang,C Xiao,A Vahdat… - arXiv preprint arXiv …, 2022",arxiv.org,,https://arxiv.org/abs/2205.07460,,Adversarial purification refers to a class of defense methods that remove adversarialperturbations using a generative model. These methods do not make assumptions on the …,743.0,Methodology
RobustBench,RobustBench: a standardized adversarial robustness benchmark,Continual test-time domain adaptation,"Q Wang,O Fink,L Van Gool… - Proceedings of the IEEE …, 2022",openaccess.thecvf.com,,http://openaccess.thecvf.com/content/CVPR2022/html/Wang_Continual_Test-Time_Domain_Adaptation_CVPR_2022_paper.html,,Test-time domain adaptation aims to adapt a source pre-trained model to a target domainwithout using any source data. Existing works mainly consider the case where the target …,709.0,Methodology
RobustBench,RobustBench: a standardized adversarial robustness benchmark,Jailbreaking leading safety-aligned llms with simple adaptive attacks,"M Andriushchenko,F Croce,N Flammarion- arXiv preprint arXiv …, 2024",arxiv.org,,https://arxiv.org/abs/2404.02151,,"We show that even the most recent safety-aligned LLMs are not robust to simple adaptivejailbreaking attacks. First, we demonstrate how to successfully leverage access to logprobs …",284.0,Methodology
AutoAttack,Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks,Better diffusion models further improve adversarial training,"Z Wang,T Pang,C Du,M Lin… - … on machine learning, 2023",proceedings.mlr.press,,http://proceedings.mlr.press/v202/wang23ad.html,,It has been recognized that the data generated by the denoising diffusion probabilisticmodel (DDPM) improves adversarial training. After two years of rapid development in …,347.0,Methodology
AutoAttack,Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks,Diffusion models for adversarial purification,"W Nie,B Guo,Y Huang,C Xiao,A Vahdat… - arXiv preprint arXiv …, 2022",arxiv.org,,https://arxiv.org/abs/2205.07460,,Adversarial purification refers to a class of defense methods that remove adversarialperturbations using a generative model. These methods do not make assumptions on the …,743.0,Methodology
AutoAttack,Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks,Square attack: a query-efficient black-box adversarial attack via random search,"M Andriushchenko,F Croce,N Flammarion… - European conference on …, 2020",Springer,,https://link.springer.com/chapter/10.1007/978-3-030-58592-1_29,,"Abstract We propose the Square Attack, a score-based black-box l 2-and l∞-adversarialattack that does not rely on local gradient information and thus is not affected by gradient …",1428.0,Methodology
AutoAttack,Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks,Data augmentation can improve robustness,"SA Rebuffi,S Gowal,DA Calian… - Advances in neural …, 2021",proceedings.neurips.cc,,https://proceedings.neurips.cc/paper/2021/hash/fb4c48608ce8825b558ccf07169a3421-Abstract.html,,"Adversarial training suffers from robust overfitting, a phenomenon where the robust testaccuracy starts to decrease during training. In this paper, we focus on reducing robust …",524.0,Methodology
AutoAttack,Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks,Defensive unlearning with adversarial training for robust concept erasure in diffusion models,"Y Zhang,X Chen,J Jia,Y Zhang… - Advances in neural …, 2024",proceedings.neurips.cc,,https://proceedings.neurips.cc/paper_files/paper/2024/hash/40954ac18a457dd5f11145bae6454cdf-Abstract-Conference.html,,"Diffusion models (DMs) have achieved remarkable success in text-to-image generation, butthey also pose safety risks, such as the potential generation of harmful content and copyright …",103.0,Methodology
ImageNet-C / ImageNet-P,Benchmarking Neural Network Robustness to Common Corruptions and Perturbations,Dinov2: Learning robust visual features without supervision,"M Oquab,T Darcet,T Moutakanni,H Vo… - arXiv preprint arXiv …, 2023",arxiv.org,,https://arxiv.org/abs/2304.07193,,The recent breakthroughs in natural language processing for model pretraining on largequantities of data have opened the way for similar foundation models in computer vision …,5336.0,Methodology
ImageNet-C / ImageNet-P,Benchmarking Neural Network Robustness to Common Corruptions and Perturbations,Reproducible scaling laws for contrastive language-image learning,"M Cherti, R Beaumont,R Wightman… - Proceedings of the …, 2023",openaccess.thecvf.com,,https://openaccess.thecvf.com/content/CVPR2023/html/Cherti_Reproducible_Scaling_Laws_for_Contrastive_Language-Image_Learning_CVPR_2023_paper,,"Scaling up neural networks has led to remarkable performance across a wide range oftasks. Moreover, performance often follows reliable scaling laws as a function of training set …",1238.0,Methodology
ImageNet-C / ImageNet-P,Benchmarking Neural Network Robustness to Common Corruptions and Perturbations,Scaling vision transformers to 22 billion parameters,"M Dehghani,J Djolonga,B Mustafa… - International …, 2023",proceedings.mlr.press,,http://proceedings.mlr.press/v202/dehghani23a.html,,"The scaling of Transformers has driven breakthrough capabilities for language models. Atpresent, the largest large language models (LLMs) contain upwards of 100B parameters …",824.0,Methodology
ImageNet-C / ImageNet-P,Benchmarking Neural Network Robustness to Common Corruptions and Perturbations,Cellpose 2.0: how to train your own model,"M Pachitariu,C Stringer- Nature methods, 2022",nature.com,,https://www.nature.com/articles/s41592-022-01663-4,,"Pretrained neural network models for biological segmentation can provide good out-of-the-box results for many image types. However, such models do not allow users to adapt the …",1063.0,Methodology
ImageNet-C / ImageNet-P,Benchmarking Neural Network Robustness to Common Corruptions and Perturbations,A convnet for the 2020s,"Z Liu,H Mao,CY Wu,C Feichtenhofer… - Proceedings of the …, 2022",openaccess.thecvf.com,,http://openaccess.thecvf.com/content/CVPR2022/html/Liu_A_ConvNet_for_the_2020s_CVPR_2022_paper.html,,"The"" Roaring 20s"" of visual recognition began with the introduction of Vision Transformers(ViTs), which quickly superseded ConvNets as the state-of-the-art image classification …",10224.0,Methodology
ImageNet-C / ImageNet-P,Benchmarking Neural Network Robustness to Common Corruptions and Perturbations,Repvit: Revisiting mobile cnn from vit perspective,"A Wang,H Chen,Z Lin,J Han… - Proceedings of the IEEE …, 2024",openaccess.thecvf.com,,http://openaccess.thecvf.com/content/CVPR2024/html/Wang_RepViT_Revisiting_Mobile_CNN_From_ViT_Perspective_CVPR_2024_paper.html,,Abstract Recently lightweight Vision Transformers (ViTs) demonstrate superior performanceand lower latency compared with lightweight Convolutional Neural Networks (CNNs) on …,580.0,Methodology
ImageNet-C / ImageNet-P,Benchmarking Neural Network Robustness to Common Corruptions and Perturbations,Masked autoencoders are scalable vision learners,"K He,X Chen,S Xie,Y Li,P Dollár… - Proceedings of the …, 2022",openaccess.thecvf.com,,https://openaccess.thecvf.com/content/CVPR2022/html/He_Masked_Autoencoders_Are_Scalable_Vision_Learners_CVPR_2022_paper,,This paper shows that masked autoencoders (MAE) are scalable self-supervised learnersfor computer vision. Our MAE approach is simple: we mask random patches of the input …,12162.0,Methodology
ImageNet-C / ImageNet-P,Benchmarking Neural Network Robustness to Common Corruptions and Perturbations,Learning transferable visual models from natural language supervision,"A Radford,JW Kim, C Hallacy… - International …, 2021",proceedings.mlr.press,,http://proceedings.mlr.press/v139/radford21a,,State-of-the-art computer vision systems are trained to predict a fixed set of predeterminedobject categories. This restricted form of supervision limits their generality and usability since …,46323.0,Methodology
WILDS,WILDS: A Benchmark of in-the-Wild Distribution Shifts,Towards a general-purpose foundation model for computational pathology,"RJ Chen,T Ding,MY Lu,DFK Williamson,G Jaume… - Nature medicine, 2024",nature.com,,https://www.nature.com/articles/s41591-024-02857-3,,"Quantitative evaluation of tissue images is crucial for computational pathology (CPath) tasks,requiring the objective characterization of histopathological entities from whole-slide images …",1031.0,Methodology
WILDS,WILDS: A Benchmark of in-the-Wild Distribution Shifts,Transfer learning in environmental remote sensing,"Y Ma,S Chen,S Ermon,DB Lobell- Remote Sensing of Environment, 2024",Elsevier,,https://www.sciencedirect.com/science/article/pii/S0034425723004765,,Abstract Machine learning (ML) has proven to be a powerful tool for utilizing the rapidlyincreasing amounts of remote sensing data for environmental monitoring. Yet ML models …,291.0,Methodology
OoD-Bench,OoD-Bench: Quantifying and Understanding Two Dimensions of Out-of-Distribution Generalization,A sentence speaks a thousand images: Domain generalization through distilling clip with language guidance,"Z Huang,A Zhou,Z Ling,M Cai… - Proceedings of the …, 2023",openaccess.thecvf.com,,http://openaccess.thecvf.com/content/ICCV2023/html/Huang_A_Sentence_Speaks_a_Thousand_Images_Domain_Generalization_through_Distilling_ICCV_2023_paper.html,,Abstract Domain generalization studies the problem of training a model with samples fromseveral domains (or distributions) and then testing the model with samples from a new …,58.0,Methodology
OoD-Bench,OoD-Bench: Quantifying and Understanding Two Dimensions of Out-of-Distribution Generalization,Improved test-time adaptation for domain generalization,"L Chen, Y Zhang,Y Song,Y Shan… - Proceedings of the …, 2023",openaccess.thecvf.com,,http://openaccess.thecvf.com/content/CVPR2023/html/Chen_Improved_Test-Time_Adaptation_for_Domain_Generalization_CVPR_2023_paper.html,,The main challenge in domain generalization (DG) is to handle the distribution shift problemthat lies between the training and test data. Recent studies suggest that test-time training …,84.0,Methodology
OoD-Bench,OoD-Bench: Quantifying and Understanding Two Dimensions of Out-of-Distribution Generalization,Feed two birds with one scone: Exploiting wild data for both out-of-distribution generalization and detection,"H Bai,G Canal,X Du,J Kwon… - … on Machine Learning, 2023",proceedings.mlr.press,,https://proceedings.mlr.press/v202/bai23a.html,,"Modern machine learning models deployed in the wild can encounter both covariate andsemantic shifts, giving rise to the problems of out-of-distribution (OOD) generalization and …",65.0,Methodology
OoD-Bench,OoD-Bench: Quantifying and Understanding Two Dimensions of Out-of-Distribution Generalization,Sparse invariant risk minimization,"X Zhou,Y Lin,W Zhang… - … Conference on Machine …, 2022",proceedings.mlr.press,,https://proceedings.mlr.press/v162/zhou22e.html,,"Abstract Invariant Risk Minimization (IRM) is an emerging invariant feature extractingtechnique to help generalization with distributional shift. However, we find that there exists a …",92.0,Methodology
OoD-Bench,OoD-Bench: Quantifying and Understanding Two Dimensions of Out-of-Distribution Generalization,Domain generalization via rationale invariance,"L Chen, Y Zhang,Y Song… - Proceedings of the …, 2023",openaccess.thecvf.com,,http://openaccess.thecvf.com/content/ICCV2023/html/Chen_Domain_Generalization_via_Rationale_Invariance_ICCV_2023_paper.html,,"This paper offers a new perspective to ease the challenge of domain generalization, whichinvolves maintaining robust results even in unseen environments. Our design focuses on the …",44.0,Methodology
SoK: Certified Robustness,SoK: Certified Robustness for Deep Neural Networks,Universal and transferable adversarial attacks on aligned language models,"A Zou,Z Wang,N Carlini,M Nasr,JZ Kolter… - arXiv preprint arXiv …, 2023",arxiv.org,,https://arxiv.org/abs/2307.15043,,"Because"" out-of-the-box"" large language models are capable of generating a great deal ofobjectionable content, recent work has focused on aligning these models in an attempt to …",2181.0,Methodology
SoK: Certified Robustness,SoK: Certified Robustness for Deep Neural Networks,Visual adversarial examples jailbreak aligned large language models,"X Qi,K Huang,A Panda,P Henderson… - Proceedings of the …, 2024",ojs.aaai.org,,https://ojs.aaai.org/index.php/AAAI/article/view/30150,,"Warning: this paper contains data, prompts, and model outputs that are offensive in nature.Recently, there has been a surge of interest in integrating vision into Large Language …",331.0,Methodology
SoK: Certified Robustness,SoK: Certified Robustness for Deep Neural Networks,Attention-enhancing backdoor attacks against bert-based models,"W Lyu,S Zheng,L Pang,H Ling,C Chen- arXiv preprint arXiv:2310.14480, 2023",arxiv.org,,https://arxiv.org/abs/2310.14480,,Recent studies have revealed that\textit {Backdoor Attacks} can threaten the safety of naturallanguage processing (NLP) models. Investigating the strategies of backdoor attacks will help …,82.0,Methodology
SoK: Certified Robustness,SoK: Certified Robustness for Deep Neural Networks,Rab: Provable robustness against backdoor attacks,"M Weber,X Xu,B Karlaš,C Zhang… - 2023 IEEE Symposium …, 2023",ieeexplore.ieee.org,,https://ieeexplore.ieee.org/abstract/document/10179451/,,"Recent studies have shown that deep neural net-works (DNNs) are vulnerable toadversarial attacks, including evasion and backdoor (poisoning) attacks. On the defense …",223.0,Methodology
SoK: Certified Robustness,SoK: Certified Robustness for Deep Neural Networks,Text-crs: A generalized certified robustness framework against textual adversarial attacks,"X Zhang,H Hong,Y Hong,P Huang… - … IEEE Symposium on …, 2024",ieeexplore.ieee.org,,https://ieeexplore.ieee.org/abstract/document/10646716/,,"The language models, especially the basic text classification models, have been shown tobe susceptible to textual adversarial attacks such as synonym substitution and word …",37.0,Methodology
Adversarial Robustness Benchmark,Benchmarking Adversarial Robustness on Image Classification,Better diffusion models further improve adversarial training,"Z Wang,T Pang,C Du,M Lin… - … on machine learning, 2023",proceedings.mlr.press,,http://proceedings.mlr.press/v202/wang23ad.html,,It has been recognized that the data generated by the denoising diffusion probabilisticmodel (DDPM) improves adversarial training. After two years of rapid development in …,347.0,Methodology
Adversarial Robustness Benchmark,Benchmarking Adversarial Robustness on Image Classification,Robustness in deep learning models for medical diagnostics: security and adversarial challenges towards robust AI applications,"H Javed,S El-Sappagh,T Abuhmed- Artificial Intelligence Review, 2024",Springer,,https://link.springer.com/article/10.1007/s10462-024-11005-9,,The current study investigates the robustness of deep learning models for accurate medicaldiagnosis systems with a specific focus on their ability to maintain performance in the …,85.0,Methodology
Adversarial Robustness Benchmark,Benchmarking Adversarial Robustness on Image Classification,On adversarial robustness of trajectory prediction for autonomous vehicles,"Q Zhang,S Hu,J Sun,QA Chen… - Proceedings of the …, 2022",openaccess.thecvf.com,,http://openaccess.thecvf.com/content/CVPR2022/html/Zhang_On_Adversarial_Robustness_of_Trajectory_Prediction_for_Autonomous_Vehicles_CVPR_2022_paper.html,,"Trajectory prediction is a critical component for autonomous vehicles (AVs) to perform safeplanning and navigation. However, few studies have analyzed the adversarial robustness of …",209.0,Methodology
Adversarial Robustness Benchmark,Benchmarking Adversarial Robustness on Image Classification,Understanding the robustness of 3D object detection with bird's-eye-view representations in autonomous driving,"Z Zhu,Y Zhang,H Chen,Y Dong… - Proceedings of the …, 2023",openaccess.thecvf.com,,http://openaccess.thecvf.com/content/CVPR2023/html/Zhu_Understanding_the_Robustness_of_3D_Object_Detection_With_Birds-Eye-View_Representations_CVPR_2023_paper.html,,Abstract 3D object detection is an essential perception task in autonomous driving tounderstand the environments. The Bird's-Eye-View (BEV) representations have significantly …,86.0,Methodology
GRB,Graph Robustness Benchmark: Benchmarking the Adversarial Robustness of Graph Machine Learning,Evaluating explainability for graph neural networks,"C Agarwal,O Queen,H Lakkaraju,M Zitnik- Scientific Data, 2023",nature.com,,https://www.nature.com/articles/s41597-023-01974-x,,"As explanations are increasingly used to understand the behavior of graph neural networks(GNNs), evaluating the quality and reliability of GNN explanations is crucial. However …",214.0,Methodology
GRB,Graph Robustness Benchmark: Benchmarking the Adversarial Robustness of Graph Machine Learning,Adversarial robustness in graph neural networks: A hamiltonian approach,"K Zhao,Q Kang,Y Song,R She… - Advances in Neural …, 2023",proceedings.neurips.cc,,https://proceedings.neurips.cc/paper_files/paper/2023/hash/0a443a000e1cb2281480b3bac395b3b8-Abstract-Conference.html,,"Graph neural networks (GNNs) are vulnerable to adversarial perturbations, including thosethat affect both node features and graph topology. This paper investigates GNNs derived …",44.0,Methodology
GRB,Graph Robustness Benchmark: Benchmarking the Adversarial Robustness of Graph Machine Learning,Can large language models improve the adversarial robustness of graph neural networks?,"Z Zhang,X Wang,H Zhou,Y Yu, M Zhang… - Proceedings of the 31st …, 2025",dl.acm.org,,https://dl.acm.org/doi/abs/10.1145/3690624.3709256,,"Graph neural networks (GNNs) are vulnerable to adversarial attacks, especially for topologyperturbations, and many methods that improve the robustness of GNNs have received …",24.0,Methodology
RealToxicityPrompts,RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models,Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context,"G Team,P Georgiev, VI Lei, R Burnell, L Bai… - arXiv preprint arXiv …, 2024",arxiv.org,,https://arxiv.org/abs/2403.05530,,"In this report, we introduce the Gemini 1.5 family of models, representing the next generationof highly compute-efficient multimodal models capable of recalling and reasoning over fine …",2793.0,Methodology
RealToxicityPrompts,RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models,Palm 2 technical report,"R Anil,AM Dai,O Firat,M Johnson,D Lepikhin… - arXiv preprint arXiv …, 2023",arxiv.org,,https://arxiv.org/abs/2305.10403,,"We introduce PaLM 2, a new state-of-the-art language model that has better multilingual andreasoning capabilities and is more compute-efficient than its predecessor PaLM. PaLM 2 is …",2120.0,Methodology
RealToxicityPrompts,RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models,Gpt-4 technical report,"J Achiam,S Adler,S Agarwal,L Ahmad… - arXiv preprint arXiv …, 2023",arxiv.org,,https://arxiv.org/abs/2303.08774,,"We report the development of GPT-4, a large-scale, multimodal model which can acceptimage and text inputs and produce text outputs. While less capable than humans in many …",19617.0,Methodology
RealToxicityPrompts,RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models,Starcoder: may the source be with you!,"R Li,LB Allal,Y Zi,N Muennighoff,D Kocetkov… - arXiv preprint arXiv …, 2023",arxiv.org,,https://arxiv.org/abs/2305.06161,,"The BigCode community, an open-scientific collaboration working on the responsibledevelopment of Large Language Models for Code (Code LLMs), introduces StarCoder and …",1296.0,Methodology
SafetyBench,SafetyBench: Evaluating the Safety of Large Language Models with Multiple Choice Questions,Igniting language intelligence: The hitchhiker's guide from chain-of-thought reasoning to language agents,"Z Zhang,Y Yao,A Zhang,X Tang,X Ma,Z He… - ACM Computing …, 2025",dl.acm.org,,https://dl.acm.org/doi/abs/10.1145/3719341,,"Large language models (LLMs) have dramatically enhanced the field of languageintelligence, as demonstrably evidenced by their formidable empirical performance across a …",100.0,Methodology
SafetyBench,SafetyBench: Evaluating the Safety of Large Language Models with Multiple Choice Questions,The art of saying no: Contextual noncompliance in language models,"F Brahman,S Kumar,V Balachandran… - Advances in …, 2024",proceedings.neurips.cc,,https://proceedings.neurips.cc/paper_files/paper/2024/hash/58e79894267cf72c66202228ad9c6057-Abstract-Datasets_and_Benchmarks_Track.html,,"Chat-based language models are designed to be helpful, yet they should not comply withevery user request. While most existing work primarily focuses on refusal of``unsafe''queries …",58.0,Methodology
SafetyBench,SafetyBench: Evaluating the Safety of Large Language Models with Multiple Choice Questions,Big5-chat: Shaping llm personalities through training on human-grounded data,"W Li,J Liu,A Liu,X Zhou,M Diab,M Sap- arXiv preprint arXiv:2410.16491, 2024",arxiv.org,,https://arxiv.org/abs/2410.16491,,"In this work, we tackle the challenge of embedding realistic human personality traits intoLLMs. Previous approaches have primarily focused on prompt-based methods that describe …",24.0,Methodology
SafetyBench,SafetyBench: Evaluating the Safety of Large Language Models with Multiple Choice Questions,Machine Against the {RAG}: Jamming {Retrieval-Augmented} Generation with Blocker Documents,"A Shafran,R Schuster,V Shmatikov- 34th USENIX Security Symposium …, 2025",usenix.org,,https://www.usenix.org/conference/usenixsecurity25/presentation/shafran,,Retrieval-augmented generation (RAG) systems respond to queries by retrieving relevantdocuments from a knowledge database and applying an LLM to the retrieved documents …,15.0,Methodology
ToxiGen,ToxiGen: A Large-Scale Machine-Generated Dataset for Adversarial and Implicit Hate Speech Detection,Llama 2: Open foundation and fine-tuned chat models,"H Touvron,L Martin,K Stone, P Albert… - arXiv preprint arXiv …, 2023",arxiv.org,,https://arxiv.org/abs/2307.09288,,"In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned largelanguage models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine …",19070.0,Methodology
ToxiGen,ToxiGen: A Large-Scale Machine-Generated Dataset for Adversarial and Implicit Hate Speech Detection,The llama 3 herd of models,"A Grattafiori,A Dubey, A Jauhri, A Pandey… - arXiv preprint arXiv …, 2024",arxiv.org,,https://arxiv.org/abs/2407.21783,,"Modern artificial intelligence (AI) systems are powered by foundation models. This paperpresents a new set of foundation models, called Llama 3. It is a herd of language models …",3144.0,Methodology
ToxiGen,ToxiGen: A Large-Scale Machine-Generated Dataset for Adversarial and Implicit Hate Speech Detection,Trustllm: Trustworthiness in large language models,"Y Huang,L Sun,H Wang, S Wu,Q Zhang, Y Li… - arXiv preprint arXiv …, 2024",arxiv.org,,https://arxiv.org/abs/2401.05561,,"Large language models (LLMs), exemplified by ChatGPT, have gained considerableattention for their excellent natural language processing capabilities. Nonetheless, these …",487.0,Methodology
ToxiGen,ToxiGen: A Large-Scale Machine-Generated Dataset for Adversarial and Implicit Hate Speech Detection,Textbooks are all you need ii: phi-1.5 technical report,"Y Li,S Bubeck,R Eldan,A Del Giorno… - arXiv preprint arXiv …, 2023",arxiv.org,,https://arxiv.org/abs/2309.05463,,We continue the investigation into the power of smaller Transformer-based languagemodels as initiated by\textbf {TinyStories}--a 10 million parameter model that can produce …,597.0,Methodology
ToxiGen,ToxiGen: A Large-Scale Machine-Generated Dataset for Adversarial and Implicit Hate Speech Detection,Orca: Progressive learning from complex explanation traces of gpt-4,"S Mukherjee,A Mitra,G Jawahar,S Agarwal… - arXiv preprint arXiv …, 2023",arxiv.org,,https://arxiv.org/abs/2306.02707,,"Recent research has focused on enhancing the capability of smaller models throughimitation learning, drawing on the outputs generated by large foundation models (LFMs). A …",482.0,Methodology
ToxiGen,ToxiGen: A Large-Scale Machine-Generated Dataset for Adversarial and Implicit Hate Speech Detection,Almanac—retrieval-augmented language models for clinical medicine,"C Zakka,R Shad,A Chaurasia,AR Dalal, JL Kim… - Nejm ai, 2024",ai.nejm.org,,https://ai.nejm.org/doi/abs/10.1056/AIoa2300068,,"Abstract Background Large language models (LLMs) have recently shown impressive zero-shot capabilities, whereby they can use auxiliary data, without the availability of task-specific …",386.0,Methodology
ToxiGen,ToxiGen: A Large-Scale Machine-Generated Dataset for Adversarial and Implicit Hate Speech Detection,Meditron-70b: Scaling medical pretraining for large language models,"Z Chen,AH Cano,A Romanou, A Bonnet… - arXiv preprint arXiv …, 2023",arxiv.org,,https://arxiv.org/abs/2311.16079,,Large language models (LLMs) can potentially democratize access to medical knowledge.While many efforts have been made to harness and improve LLMs' medical knowledge and …,461.0,Methodology
BBQ,BBQ: A Hand-Built Bias Benchmark for Question Answering,Gemini: a family of highly capable multimodal models,"G Team, R Anil, S Borgeaud, JB Alayrac, J Yu… - arXiv preprint arXiv …, 2023",arxiv.org,,https://arxiv.org/abs/2312.11805,,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkablecapabilities across image, audio, video, and text understanding. The Gemini family consists …",6181.0,Methodology
BBQ,BBQ: A Hand-Built Bias Benchmark for Question Answering,Mixtral of experts,"AQ Jiang,A Sablayrolles, A Roux,A Mensch… - arXiv preprint arXiv …, 2024",arxiv.org,,https://arxiv.org/abs/2401.04088,,"We introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral hasthe same architecture as Mistral 7B, with the difference that each layer is composed of 8 …",2440.0,Methodology
BBQ,BBQ: A Hand-Built Bias Benchmark for Question Answering,Palm 2 technical report,"R Anil,AM Dai,O Firat,M Johnson,D Lepikhin… - arXiv preprint arXiv …, 2023",arxiv.org,,https://arxiv.org/abs/2305.10403,,"We introduce PaLM 2, a new state-of-the-art language model that has better multilingual andreasoning capabilities and is more compute-efficient than its predecessor PaLM. PaLM 2 is …",2120.0,Methodology
BBQ,BBQ: A Hand-Built Bias Benchmark for Question Answering,Training a helpful and harmless assistant with reinforcement learning from human feedback,"Y Bai,A Jones,K Ndousse,A Askell, A Chen… - arXiv preprint arXiv …, 2022",arxiv.org,,https://arxiv.org/abs/2204.05862,,We apply preference modeling and reinforcement learning from human feedback (RLHF) tofinetune language models to act as helpful and harmless assistants. We find this alignment …,2994.0,Methodology
BBQ,BBQ: A Hand-Built Bias Benchmark for Question Answering,Language models don't always say what they think: Unfaithful explanations in chain-of-thought prompting,"M Turpin,J Michael,E Perez… - Advances in Neural …, 2023",proceedings.neurips.cc,,https://proceedings.neurips.cc/paper_files/paper/2023/hash/ed3fea9033a80fea1376299fa7863f4a-Abstract-Conference.html,,"Abstract Large Language Models (LLMs) can achieve strong performance on many tasks byproducing step-by-step reasoning before giving a final output, often referred to as chain-of …",668.0,Methodology
BBQ,BBQ: A Hand-Built Bias Benchmark for Question Answering,Beyond the imitation game: Quantifying and extrapolating the capabilities of language models,"A Srivastava, A Rastogi, A Rao,AAM Shoeb… - … on machine learning …, 2023",openreview.net,,https://openreview.net/forum?id=uyTL5Bvosj&nesting=2&sort=date-desc,,"Language models demonstrate both quantitative improvement and new qualitativecapabilities with increasing scale. Despite their potentially transformative impact, these new …",2020.0,Methodology
BOLD,BOLD: Dataset and Metrics for Measuring Biases in Open-Ended Language Generation,Mixtral of experts,"AQ Jiang,A Sablayrolles, A Roux,A Mensch… - arXiv preprint arXiv …, 2024",arxiv.org,,https://arxiv.org/abs/2401.04088,,"We introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral hasthe same architecture as Mistral 7B, with the difference that each layer is composed of 8 …",2440.0,Methodology
BOLD,BOLD: Dataset and Metrics for Measuring Biases in Open-Ended Language Generation,Llama 2: Open foundation and fine-tuned chat models,"H Touvron,L Martin,K Stone, P Albert… - arXiv preprint arXiv …, 2023",arxiv.org,,https://arxiv.org/abs/2307.09288,,"In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned largelanguage models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine …",19070.0,Methodology
BOLD,BOLD: Dataset and Metrics for Measuring Biases in Open-Ended Language Generation,Training language models to follow instructions with human feedback,"L Ouyang,J Wu, X Jiang, D Almeida… - Advances in neural …, 2022",proceedings.neurips.cc,,https://proceedings.neurips.cc/paper_files/paper/2022/hash/b1efde53be364a73914f58805a001731-Abstract-Conference.html,,"Making language models bigger does not inherently make them better at following a user'sintent. For example, large language models can generate outputs that are untruthful, toxic, or …",19069.0,Methodology
BOLD,BOLD: Dataset and Metrics for Measuring Biases in Open-Ended Language Generation,Starcoder 2 and the stack v2: The next generation,"A Lozhkov,R Li,LB Allal,F Cassano… - arXiv preprint arXiv …, 2024",arxiv.org,,https://arxiv.org/abs/2402.19173,,"The BigCode project, an open-scientific collaboration focused on the responsibledevelopment of Large Language Models for Code (Code LLMs), introduces StarCoder2. In …",456.0,Methodology
JailbreakBench,JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models,Jailbreaking black box large language models in twenty queries,"P Chao,A Robey,E Dobriban… - … IEEE Conference on …, 2025",ieeexplore.ieee.org,,https://ieeexplore.ieee.org/abstract/document/10992337/,,"There is growing interest in ensuring that large language models (LLMs) align with humanvalues. However, the alignment of such models is vulnerable to adversarial jailbreaks, which …",953.0,Methodology
JailbreakBench,JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models,Refusal in language models is mediated by a single direction,"A Arditi,O Obeso,A Syed,D Paleka… - Advances in …, 2024",proceedings.neurips.cc,,https://proceedings.neurips.cc/paper_files/paper/2024/hash/f545448535dfde4f9786555403ab7c49-Abstract-Conference.html,,"Conversational large language models are fine-tuned for both instruction-following andsafety, resulting in models that obey benign requests but refuse harmful ones. While this …",265.0,Methodology
JailbreakBench,JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models,Jailbreaking leading safety-aligned llms with simple adaptive attacks,"M Andriushchenko,F Croce,N Flammarion- arXiv preprint arXiv …, 2024",arxiv.org,,https://arxiv.org/abs/2404.02151,,"We show that even the most recent safety-aligned LLMs are not robust to simple adaptivejailbreaking attacks. First, we demonstrate how to successfully leverage access to logprobs …",284.0,Methodology
JailbreakBench,JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models,Deepinception: Hypnotize large language model to be jailbreaker,"X Li,Z Zhou,J Zhu,J Yao,T Liu,B Han- arXiv preprint arXiv:2311.03191, 2023",arxiv.org,,https://arxiv.org/abs/2311.03191,,"Despite remarkable success in various applications, large language models (LLMs) arevulnerable to adversarial jailbreaks that make the safety guardrails void. However, previous …",282.0,Methodology
JailbreakBench,JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models,Rainbow teaming: Open-ended generation of diverse adversarial prompts,"M Samvelyan, SC Raparthy,A Lupu… - Advances in …, 2024",proceedings.neurips.cc,,https://proceedings.neurips.cc/paper_files/paper/2024/hash/8147a43d030b43a01020774ae1d3e3bb-Abstract-Conference.html,,"As large language models (LLMs) become increasingly prevalent across many real-worldapplications, understanding and enhancing their robustness to adversarial attacks is of …",118.0,Methodology
JailbreakBench,JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models,Deliberative alignment: Reasoning enables safer language models,"MY Guan,M Joglekar,E Wallace,S Jain… - arXiv preprint arXiv …, 2024",arxiv.org,,https://arxiv.org/abs/2412.16339,,"As large-scale language models increasingly impact safety-critical domains, ensuring theirreliable adherence to well-defined principles remains a fundamental challenge. We …",153.0,Methodology
HarmBench,HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal,Tree of attacks: Jailbreaking black-box llms automatically,"A Mehrotra,M Zampetakis… - Advances in …, 2024",proceedings.neurips.cc,,https://proceedings.neurips.cc/paper_files/paper/2024/hash/70702e8cbb4890b4a467b984ae59828a-Abstract-Conference.html,,"Abstract While Large Language Models (LLMs) display versatile functionality, they continueto generate harmful, biased, and toxic content, as demonstrated by the prevalence of human …",424.0,Methodology
HarmBench,HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal,Refusal in language models is mediated by a single direction,"A Arditi,O Obeso,A Syed,D Paleka… - Advances in …, 2024",proceedings.neurips.cc,,https://proceedings.neurips.cc/paper_files/paper/2024/hash/f545448535dfde4f9786555403ab7c49-Abstract-Conference.html,,"Conversational large language models are fine-tuned for both instruction-following andsafety, resulting in models that obey benign requests but refuse harmful ones. While this …",265.0,Methodology
HarmBench,HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal,Jailbreaking leading safety-aligned llms with simple adaptive attacks,"M Andriushchenko,F Croce,N Flammarion- arXiv preprint arXiv …, 2024",arxiv.org,,https://arxiv.org/abs/2404.02151,,"We show that even the most recent safety-aligned LLMs are not robust to simple adaptivejailbreaking attacks. First, we demonstrate how to successfully leverage access to logprobs …",284.0,Methodology
HarmBench,HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal,Figstep: Jailbreaking large vision-language models via typographic visual prompts,"Y Gong, D Ran,J Liu, C Wang,T Cong… - Proceedings of the …, 2025",ojs.aaai.org,,https://ojs.aaai.org/index.php/AAAI/article/view/34568,,"Abstract Large Vision-Language Models (LVLMs) signify a groundbreaking paradigm shiftwithin the Artificial Intelligence (AI) community, extending beyond the capabilities of Large …",264.0,Methodology
HarmBench,HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal,"Wildguard: Open one-stop moderation tools for safety risks, jailbreaks, and refusals of llms","S Han,K Rao,A Ettinger,L Jiang… - Advances in …, 2024",proceedings.neurips.cc,,https://proceedings.neurips.cc/paper_files/paper/2024/hash/0f69b4b96a46f284b726fbd70f74fb3b-Abstract-Datasets_and_Benchmarks_Track.html,,"We introduce WildGuard---an open, light-weight moderation tool for LLM safety thatachieves three goals:(1) identifying malicious intent in user prompts,(2) detecting safety risks …",169.0,Methodology
HarmBench,HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal,Tulu 3: Pushing frontiers in open language model post-training,"N Lambert,J Morrison,V Pyatkin,S Huang… - arXiv preprint arXiv …, 2024",arxiv.org,,https://arxiv.org/abs/2411.15124,,"Language model post-training is applied to refine behaviors and unlock new skills across awide range of recent language models, but open recipes for applying these techniques lag …",252.0,Methodology
DecodingTrust,DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models,Simpo: Simple preference optimization with a reference-free reward,"Y Meng,M Xia,D Chen- Advances in Neural Information …, 2024",proceedings.neurips.cc,,https://proceedings.neurips.cc/paper_files/paper/2024/hash/e099c1c9699814af0be873a175361713-Abstract-Conference.html,,Abstract Direct Preference Optimization (DPO) is a widely used offline preferenceoptimization algorithm that reparameterizes reward functions in reinforcement learning from …,657.0,Methodology
DecodingTrust,DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models,Trustllm: Trustworthiness in large language models,"Y Huang,L Sun,H Wang, S Wu,Q Zhang, Y Li… - arXiv preprint arXiv …, 2024",arxiv.org,,https://arxiv.org/abs/2401.05561,,"Large language models (LLMs), exemplified by ChatGPT, have gained considerableattention for their excellent natural language processing capabilities. Nonetheless, these …",487.0,Methodology
DecodingTrust,DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models,Gptfuzzer: Red teaming large language models with auto-generated jailbreak prompts,"J Yu,X Lin,Z Yu,X Xing- arXiv preprint arXiv:2309.10253, 2023",arxiv.org,,https://arxiv.org/abs/2309.10253,,"Large language models (LLMs) have recently experienced tremendous popularity and arewidely used from casual conversations to AI-driven programming. However, despite their …",446.0,Methodology
AdvBench,Universal and Transferable Adversarial Attacks on Aligned Language Models,Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context,"G Team,P Georgiev, VI Lei, R Burnell, L Bai… - arXiv preprint arXiv …, 2024",arxiv.org,,https://arxiv.org/abs/2403.05530,,"In this report, we introduce the Gemini 1.5 family of models, representing the next generationof highly compute-efficient multimodal models capable of recalling and reasoning over fine …",2793.0,Methodology
AdvBench,Universal and Transferable Adversarial Attacks on Aligned Language Models,Jailbreaking black box large language models in twenty queries,"P Chao,A Robey,E Dobriban… - … IEEE Conference on …, 2025",ieeexplore.ieee.org,,https://ieeexplore.ieee.org/abstract/document/10992337/,,"There is growing interest in ensuring that large language models (LLMs) align with humanvalues. However, the alignment of such models is vulnerable to adversarial jailbreaks, which …",953.0,Methodology
AdvBench,Universal and Transferable Adversarial Attacks on Aligned Language Models,Tree of attacks: Jailbreaking black-box llms automatically,"A Mehrotra,M Zampetakis… - Advances in …, 2024",proceedings.neurips.cc,,https://proceedings.neurips.cc/paper_files/paper/2024/hash/70702e8cbb4890b4a467b984ae59828a-Abstract-Conference.html,,"Abstract While Large Language Models (LLMs) display versatile functionality, they continueto generate harmful, biased, and toxic content, as demonstrated by the prevalence of human …",424.0,Methodology
HarmfulQA,Red-Teaming Large Language Models using Chain of Utterances for Safety-Alignment,Trustllm: Trustworthiness in large language models,"Y Huang,L Sun,H Wang, S Wu,Q Zhang, Y Li… - arXiv preprint arXiv …, 2024",arxiv.org,,https://arxiv.org/abs/2401.05561,,"Large language models (LLMs), exemplified by ChatGPT, have gained considerableattention for their excellent natural language processing capabilities. Nonetheless, these …",487.0,Methodology
HarmfulQA,Red-Teaming Large Language Models using Chain of Utterances for Safety-Alignment,Smoothllm: Defending large language models against jailbreaking attacks,"A Robey,E Wong,H Hassani,GJ Pappas- arXiv preprint arXiv …, 2023",arxiv.org,,https://arxiv.org/abs/2310.03684,,"Despite efforts to align large language models (LLMs) with human intentions, widely-usedLLMs such as GPT, Llama, and Claude are susceptible to jailbreaking attacks, wherein an …",417.0,Methodology
DoNotAnswer,Do-Not-Answer: A Dataset for Evaluating Safeguards in LLMs,Trustllm: Trustworthiness in large language models,"Y Huang,L Sun,H Wang, S Wu,Q Zhang, Y Li… - arXiv preprint arXiv …, 2024",arxiv.org,,https://arxiv.org/abs/2401.05561,,"Large language models (LLMs), exemplified by ChatGPT, have gained considerableattention for their excellent natural language processing capabilities. Nonetheless, these …",487.0,Methodology
DoNotAnswer,Do-Not-Answer: A Dataset for Evaluating Safeguards in LLMs,Position: Trustllm: Trustworthiness in large language models,"Y Huang,L Sun,H Wang, S Wu… - International …, 2024",proceedings.mlr.press,,http://proceedings.mlr.press/v235/huang24x.html,,"Large language models (LLMs) have gained considerable attention for their excellentnatural language processing capabilities. Nonetheless, these LLMs present many …",88.0,Methodology
SALAD-Bench,SALAD-Bench: A Hierarchical and Comprehensive Safety Benchmark for Large Language Models,"Wildguard: Open one-stop moderation tools for safety risks, jailbreaks, and refusals of llms","S Han,K Rao,A Ettinger,L Jiang… - Advances in …, 2024",proceedings.neurips.cc,,https://proceedings.neurips.cc/paper_files/paper/2024/hash/0f69b4b96a46f284b726fbd70f74fb3b-Abstract-Datasets_and_Benchmarks_Track.html,,"We introduce WildGuard---an open, light-weight moderation tool for LLM safety thatachieves three goals:(1) identifying malicious intent in user prompts,(2) detecting safety risks …",169.0,Methodology
SALAD-Bench,SALAD-Bench: A Hierarchical and Comprehensive Safety Benchmark for Large Language Models,Optimization-based prompt injection attack to llm-as-a-judge,"J Shi,Z Yuan,Y Liu,Y Huang,P Zhou,L Sun… - Proceedings of the …, 2024",dl.acm.org,,https://dl.acm.org/doi/abs/10.1145/3658644.3690291,,LLM-as-a-Judge uses a large language model (LLM) to select the best response from a setof candidates for a given question. LLM-as-a-Judge has many applications such as LLM …,108.0,Methodology
SALAD-Bench,SALAD-Bench: A Hierarchical and Comprehensive Safety Benchmark for Large Language Models,Shieldgemma: Generative ai content moderation based on gemma,"W Zeng, Y Liu,R Mullins,L Peran, J Fernandez… - arXiv preprint arXiv …, 2024",arxiv.org,,https://arxiv.org/abs/2407.21772,,"We present ShieldGemma, a comprehensive suite of LLM-based safety content moderationmodels built upon Gemma2. These models provide robust, state-of-the-art predictions of …",85.0,Methodology
MACHIAVELLI,Do the Rewards Justify the Means? Measuring Trade-Offs Between Rewards and Ethical Behavior in the MACHIAVELLI Benchmark,Rewarded soups: towards pareto-optimal alignment by interpolating weights fine-tuned on diverse rewards,"A Rame,G Couairon,C Dancette… - Advances in …, 2023",proceedings.neurips.cc,,https://proceedings.neurips.cc/paper_files/paper/2023/hash/e12a3b98b67e8395f639fde4c2b03168-Abstract-Conference.html,,"Foundation models are first pre-trained on vast unsupervised datasets and then fine-tunedon labeled data. Reinforcement learning, notably from human feedback (RLHF), can further …",208.0,Methodology
ELEPHANT,ELEPHANT: Measuring and Understanding Social Sycophancy in LLMs,Sycophancy under pressure: Evaluating and mitigating sycophantic bias via adversarial dialogues in scientific qa,"K Zhang, Q Jia, Z Chen,W Sun,X Zhu,C Li… - arXiv preprint arXiv …, 2025",arxiv.org,,https://arxiv.org/abs/2508.13743,,"Large language models (LLMs), while increasingly used in domains requiring factual rigor,often display a troubling behavior: sycophancy, the tendency to align with user beliefs …",3.0,Methodology
ELEPHANT,ELEPHANT: Measuring and Understanding Social Sycophancy in LLMs,The pimmur principles: Ensuring validity in collective behavior of llm societies,"J Zhou,J Huang,X Zhou,MH Lam,X Wang… - arXiv preprint arXiv …, 2025",arxiv.org,,https://arxiv.org/abs/2509.18052,,"Large Language Models (LLMs) are increasingly used for social simulation, wherepopulations of agents are expected to reproduce human-like collective behavior. However …",1.0,Methodology
ELEPHANT,ELEPHANT: Measuring and Understanding Social Sycophancy in LLMs,When truth is overridden: Uncovering the internal origins of sycophancy in large language models,"K Wang, J Li, S Yang,Z Zhang,D Wang- arXiv preprint arXiv:2508.02087, 2025",arxiv.org,,https://arxiv.org/abs/2508.02087,,"Large Language Models (LLMs) often exhibit sycophantic behavior, agreeing with user-stated opinions even when those contradict factual knowledge. While prior work has …",1.0,Methodology
OpenDeception,OpenDeception: Benchmarking and Investigating AI Deceptive Behaviors via Open-ended Interaction Simulation,Mitigating deceptive alignment via self-monitoring,"J Ji, W Chen, K Wang,D Hong,S Fang,B Chen… - arXiv preprint arXiv …, 2025",arxiv.org,,https://arxiv.org/abs/2505.18807,,"Modern large language models rely on chain-of-thought (CoT) reasoning to achieveimpressive performance, yet the same mechanism can amplify deceptive alignment …",7.0,Methodology
OpenDeception,OpenDeception: Benchmarking and Investigating AI Deceptive Behaviors via Open-ended Interaction Simulation,Quantized but Deceptive? A Multi-Dimensional Truthfulness Evaluation of Quantized LLMs,"Y Fu,X Long, R Li, H Yu, M Sheng,X Han… - arXiv preprint arXiv …, 2025",arxiv.org,,https://arxiv.org/abs/2508.19432,,Quantization enables efficient deployment of large language models (LLMs) in resource-constrained environments by significantly reducing memory and computation costs. While …,1.0,Methodology
Goal Misgeneralization (Procgen),Goal Misgeneralization in Deep Reinforcement Learning,Rewarded soups: towards pareto-optimal alignment by interpolating weights fine-tuned on diverse rewards,"A Rame,G Couairon,C Dancette… - Advances in …, 2023",proceedings.neurips.cc,,https://proceedings.neurips.cc/paper_files/paper/2023/hash/e12a3b98b67e8395f639fde4c2b03168-Abstract-Conference.html,,"Foundation models are first pre-trained on vast unsupervised datasets and then fine-tunedon labeled data. Reinforcement learning, notably from human feedback (RLHF), can further …",208.0,Methodology
IPS Index,Instrumental Power-Seeking Index: A Novel Benchmark for Detecting Power-Seeking Behavior in LLMs,Rewarded soups: towards pareto-optimal alignment by interpolating weights fine-tuned on diverse rewards,"A Rame,G Couairon,C Dancette… - Advances in …, 2023",proceedings.neurips.cc,,https://proceedings.neurips.cc/paper_files/paper/2023/hash/e12a3b98b67e8395f639fde4c2b03168-Abstract-Conference.html,,"Foundation models are first pre-trained on vast unsupervised datasets and then fine-tunedon labeled data. Reinforcement learning, notably from human feedback (RLHF), can further …",208.0,Methodology
SycEval,SycEval: Evaluating LLM Sycophancy,Measuring sycophancy of language models in multi-turn dialogues,"J Hong,G Byun,S Kim,K Shu,JD Choi- arXiv preprint arXiv:2505.23840, 2025",arxiv.org,,https://arxiv.org/abs/2505.23840,,"Large Language Models (LLMs) are expected to provide helpful and harmless responses,yet they often exhibit sycophancy--conforming to user beliefs regardless of factual accuracy …",7.0,Methodology
SycEval,SycEval: Evaluating LLM Sycophancy,Sycophancy under pressure: Evaluating and mitigating sycophantic bias via adversarial dialogues in scientific qa,"K Zhang, Q Jia, Z Chen,W Sun,X Zhu,C Li… - arXiv preprint arXiv …, 2025",arxiv.org,,https://arxiv.org/abs/2508.13743,,"Large language models (LLMs), while increasingly used in domains requiring factual rigor,often display a troubling behavior: sycophancy, the tendency to align with user beliefs …",3.0,Methodology
HELM Safety,HELM Safety: Towards Standardized Safety Evaluations of Language Models,Evaluating the Sensitivity of LLMs to Harmful Contents in Long Input,"F Ghorbanpour,A Fraser- arXiv preprint arXiv:2510.05864, 2025",arxiv.org,,https://arxiv.org/abs/2510.05864,,"Large language models (LLMs) increasingly support applications that rely on extendedcontext, from document processing to retrieval-augmented generation. While their long …",,Methodology
AIR-Bench 2024,AIR-Bench 2024: A Safety Benchmark Based on Risk Categories from Regulations and Policies,Refusal-trained llms are easily jailbroken as browser agents,"P Kumar,E Lau,S Vijayakumar,T Trinh… - arXiv preprint arXiv …, 2024",arxiv.org,,https://arxiv.org/abs/2410.13886,,"For safety reasons, large language models (LLMs) are trained to refuse harmful userinstructions, such as assisting dangerous activities. We study an open question in this work …",17.0,Methodology
AIR-Bench 2024,AIR-Bench 2024: A Safety Benchmark Based on Risk Categories from Regulations and Policies,Agrail: A lifelong agent guardrail with effective and adaptive safety detection,"W Luo,S Dai,X Liu,S Banerjee,H Sun,M Chen… - arXiv preprint arXiv …, 2025",arxiv.org,,https://arxiv.org/abs/2502.11448,,The rapid advancements in Large Language Models (LLMs) have enabled their deploymentas autonomous agents for handling complex tasks in dynamic environments. These LLMs …,18.0,Methodology
AIR-Bench 2024,AIR-Bench 2024: A Safety Benchmark Based on Risk Categories from Regulations and Policies,Shieldagent: Shielding agents via verifiable safety policy reasoning,"Z Chen,M Kang,B Li- arXiv preprint arXiv:2503.22738, 2025",arxiv.org,,https://arxiv.org/abs/2503.22738,,"Autonomous agents powered by foundation models have seen widespread adoption acrossvarious real-world applications. However, they remain highly vulnerable to malicious …",21.0,Methodology
AIR-Bench 2024,AIR-Bench 2024: A Safety Benchmark Based on Risk Categories from Regulations and Policies,Differentially private kernel density estimation,"E Liu,JYC Hu, A Reneau,Z Song,H Liu- arXiv preprint arXiv:2409.01688, 2024",arxiv.org,,https://arxiv.org/abs/2409.01688,,"We introduce a refined differentially private (DP) data structure for kernel density estimation(KDE), offering not only improved privacy-utility tradeoff but also better efficiency over prior …",5.0,Methodology
AIR-Bench 2024,AIR-Bench 2024: A Safety Benchmark Based on Risk Categories from Regulations and Policies,Eaira: Establishing a methodology for evaluating ai models as scientific research assistants,"F Cappello,S Madireddy,R Underwood… - arXiv preprint arXiv …, 2025",arxiv.org,,https://arxiv.org/abs/2502.20309,,"Recent advancements have positioned AI, and particularly Large Language Models (LLMs),as transformative tools for scientific research, capable of addressing complex tasks that …",7.0,Methodology
AIR-Bench 2024,AIR-Bench 2024: A Safety Benchmark Based on Risk Categories from Regulations and Policies,Breaking Down Bias: On The Limits of Generalizable Pruning Strategies,"S Ma,A Salinas,J Nyarko,P Henderson- Proceedings of the 2025 ACM …, 2025",dl.acm.org,,https://dl.acm.org/doi/abs/10.1145/3715275.3732161,,"We employ model pruning to examine how LLMs conceptualize racial biases, and whether ageneralizable mitigation strategy for such biases appears feasible. Our analysis yields …",2.0,Methodology
TrustLLM,TrustLLM: Trustworthiness in Large Language Models,Empowering biomedical discovery with AI agents,"S Gao,A Fang, Y Huang,V Giunchiglia,A Noori… - Cell, 2024",cell.com,,https://www.cell.com/cell/fulltext/S0092-8674(24)01070-5?&target=_blank,,"We envision"" AI scientists"" as systems capable of skeptical learning and reasoning thatempower biomedical research through collaborative agents that integrate AI models and …",232.0,Methodology
MLCommons AILuminate,AILuminate: Introducing v1.0 of the AI Risk and Reliability Benchmark from MLCommons,Automated red teaming with goat: the generative offensive agent tester,"M Pavlova,E Brinkman,K Iyer,V Albiero… - arXiv preprint arXiv …, 2024",arxiv.org,,https://arxiv.org/abs/2410.01606,,"Red teaming assesses how large language models (LLMs) can produce content thatviolates norms, policies, and rules set during their safety training. However, most existing …",21.0,Methodology
MLCommons AILuminate,AILuminate: Introducing v1.0 of the AI Risk and Reliability Benchmark from MLCommons,Are all prompt components value-neutral? understanding the heterogeneous adversarial robustness of dissected prompt in large language models,"Y Zheng,T Li, H Huang, T Zeng, J Lu,C Chu… - arXiv preprint arXiv …, 2025",arxiv.org,,https://arxiv.org/abs/2508.01554,,"Prompt-based adversarial attacks have become an effective means to assess therobustness of large language models (LLMs). However, existing approaches often treat …",3.0,Methodology
MLCommons AILuminate,AILuminate: Introducing v1.0 of the AI Risk and Reliability Benchmark from MLCommons,Phare: A Safety Probe for Large Language Models,"PL Jeune, B Malézieux,W Xiao,M Dora- arXiv preprint arXiv:2505.11365, 2025",arxiv.org,,https://arxiv.org/abs/2505.11365,,"Ensuring the safety of large language models (LLMs) is critical for responsible deployment,yet existing evaluations often prioritize performance over identifying failure modes. We …",,Methodology
MLCommons AILuminate,AILuminate: Introducing v1.0 of the AI Risk and Reliability Benchmark from MLCommons,Decoding Federated Learning: The FedNAM+ Conformal Revolution,"SB Balija,A Nanda,D Sahoo- arXiv preprint arXiv:2506.17872, 2025",arxiv.org,,https://arxiv.org/abs/2506.17872,,"Federated learning has significantly advanced distributed training of machine learningmodels across decentralized data sources. However, existing frameworks often lack …",1.0,Methodology
MLCommons AILuminate,AILuminate: Introducing v1.0 of the AI Risk and Reliability Benchmark from MLCommons,Beyond Uniform Criteria: Scenario-Adaptive Multi-Dimensional Jailbreak Evaluation,"L Jiang, Y Li, X Zhang,Y Ding, L Pan - arXiv preprint arXiv:2508.06194, 2025",arxiv.org,,https://arxiv.org/abs/2508.06194,,"Precise jailbreak evaluation is vital for LLM red teaming and jailbreak research. Currentapproaches employ binary classification (eg, string matching, toxic text classifiers, LLM …",,Methodology
ALERT,ALERT: A Comprehensive Benchmark for Assessing Large Language Models' Safety through Red Teaming,"Wildguard: Open one-stop moderation tools for safety risks, jailbreaks, and refusals of llms","S Han,K Rao,A Ettinger,L Jiang… - Advances in …, 2024",proceedings.neurips.cc,,https://proceedings.neurips.cc/paper_files/paper/2024/hash/0f69b4b96a46f284b726fbd70f74fb3b-Abstract-Datasets_and_Benchmarks_Track.html,,"We introduce WildGuard---an open, light-weight moderation tool for LLM safety thatachieves three goals:(1) identifying malicious intent in user prompts,(2) detecting safety risks …",169.0,Methodology
ALERT,ALERT: A Comprehensive Benchmark for Assessing Large Language Models' Safety through Red Teaming,ART: automatic red-teaming for text-to-image models to protect benign users,"G Li,K Chen,S Zhang,J Zhang… - Advances in Neural …, 2024",proceedings.neurips.cc,,https://proceedings.neurips.cc/paper_files/paper/2024/hash/a5c7206fd66e8314bb21a04492359353-Abstract-Conference.html,,"Large-scale pre-trained generative models are taking the world by storm, due to theirabilities in generating creative content. Meanwhile, safeguards for these generative models …",27.0,Methodology
ALERT,ALERT: A Comprehensive Benchmark for Assessing Large Language Models' Safety through Red Teaming,Bi-factorial preference optimization: Balancing safety-helpfulness in language models,"W Zhang,PHS Torr,M Elhoseiny,A Bibi- arXiv preprint arXiv:2408.15313, 2024",arxiv.org,,https://arxiv.org/abs/2408.15313,,"Fine-tuning large language models (LLMs) on human preferences, typically throughreinforcement learning from human feedback (RLHF), has proven successful in enhancing …",15.0,Methodology
AgentHarm,AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents,Jailbreaking leading safety-aligned llms with simple adaptive attacks,"M Andriushchenko,F Croce,N Flammarion- arXiv preprint arXiv …, 2024",arxiv.org,,https://arxiv.org/abs/2404.02151,,"We show that even the most recent safety-aligned LLMs are not robust to simple adaptivejailbreaking attacks. First, we demonstrate how to successfully leverage access to logprobs …",284.0,Methodology
SimpleSafetyTest,SimpleSafetyTest: Universal Safety Testing for Language Models,Jailbreaking leading safety-aligned llms with simple adaptive attacks,"M Andriushchenko,F Croce,N Flammarion- arXiv preprint arXiv …, 2024",arxiv.org,,https://arxiv.org/abs/2404.02151,,"We show that even the most recent safety-aligned LLMs are not robust to simple adaptivejailbreaking attacks. First, we demonstrate how to successfully leverage access to logprobs …",284.0,Methodology
SimpleSafetyTest,SimpleSafetyTest: Universal Safety Testing for Language Models,"Wildguard: Open one-stop moderation tools for safety risks, jailbreaks, and refusals of llms","S Han,K Rao,A Ettinger,L Jiang… - Advances in …, 2024",proceedings.neurips.cc,,https://proceedings.neurips.cc/paper_files/paper/2024/hash/0f69b4b96a46f284b726fbd70f74fb3b-Abstract-Datasets_and_Benchmarks_Track.html,,"We introduce WildGuard---an open, light-weight moderation tool for LLM safety thatachieves three goals:(1) identifying malicious intent in user prompts,(2) detecting safety risks …",169.0,Methodology
SimpleSafetyTest,SimpleSafetyTest: Universal Safety Testing for Language Models,Aegis: Online adaptive ai content safety moderation with ensemble of llm experts,"S Ghosh,P Varshney,E Galinkin,C Parisien- arXiv preprint arXiv …, 2024",arxiv.org,,https://arxiv.org/abs/2404.05993,,"As Large Language Models (LLMs) and generative AI become more widespread, thecontent safety risks associated with their use also increase. We find a notable deficiency in …",68.0,Methodology
SimpleSafetyTest,SimpleSafetyTest: Universal Safety Testing for Language Models,Latent adversarial training improves robustness to persistent harmful behaviors in llms,"A Sheshadri,A Ewart,P Guo,A Lynch,C Wu… - arXiv preprint arXiv …, 2024",arxiv.org,,https://arxiv.org/abs/2407.15549,,"Large language models (LLMs) can often be made to behave in undesirable ways that theyare explicitly fine-tuned not to. For example, the LLM red-teaming literature has produced a …",39.0,Methodology
SimpleSafetyTest,SimpleSafetyTest: Universal Safety Testing for Language Models,Guardreasoner: Towards reasoning-based llm safeguards,"Y Liu,H Gao,S Zhai,J Xia,T Wu, Z Xue… - arXiv preprint arXiv …, 2025",arxiv.org,,https://arxiv.org/abs/2501.18492,,"As LLMs increasingly impact safety-critical applications, ensuring their safety usingguardrails remains a key challenge. This paper proposes GuardReasoner, a new safeguard …",49.0,Methodology
XSTest,XSTest: A Test Suite for Identifying Exaggerated Safety Behaviors in Large Language Models,The llama 3 herd of models,"A Grattafiori,A Dubey, A Jauhri, A Pandey… - arXiv preprint arXiv …, 2024",arxiv.org,,https://arxiv.org/abs/2407.21783,,"Modern artificial intelligence (AI) systems are powered by foundation models. This paperpresents a new set of foundation models, called Llama 3. It is a herd of language models …",3144.0,Methodology
XSTest,XSTest: A Test Suite for Identifying Exaggerated Safety Behaviors in Large Language Models,Openai o1 system card,"A Jaech,A Kalai,A Lerer, A Richardson… - arXiv preprint arXiv …, 2024",arxiv.org,,https://arxiv.org/abs/2412.16720,,The o1 model series is trained with large-scale reinforcement learning to reason using chainof thought. These advanced reasoning capabilities provide new avenues for improving the …,1203.0,Methodology
XSTest,XSTest: A Test Suite for Identifying Exaggerated Safety Behaviors in Large Language Models,Gptfuzzer: Red teaming large language models with auto-generated jailbreak prompts,"J Yu,X Lin,Z Yu,X Xing- arXiv preprint arXiv:2309.10253, 2023",arxiv.org,,https://arxiv.org/abs/2309.10253,,"Large language models (LLMs) have recently experienced tremendous popularity and arewidely used from casual conversations to AI-driven programming. However, despite their …",446.0,Methodology
XSTest,XSTest: A Test Suite for Identifying Exaggerated Safety Behaviors in Large Language Models,Improving alignment and robustness with circuit breakers,"A Zou,L Phan,J Wang, D Duenas… - Advances in …, 2024",proceedings.neurips.cc,,https://proceedings.neurips.cc/paper_files/paper/2024/hash/97ca7168c2c333df5ea61ece3b3276e1-Abstract-Conference.html,,"AI systems can take harmful actions and are highly vulnerable to adversarial attacks. Wepresent an approach, inspired by recent advances in representation engineering, that …",139.0,Methodology
XSTest,XSTest: A Test Suite for Identifying Exaggerated Safety Behaviors in Large Language Models,"Wildguard: Open one-stop moderation tools for safety risks, jailbreaks, and refusals of llms","S Han,K Rao,A Ettinger,L Jiang… - Advances in …, 2024",proceedings.neurips.cc,,https://proceedings.neurips.cc/paper_files/paper/2024/hash/0f69b4b96a46f284b726fbd70f74fb3b-Abstract-Datasets_and_Benchmarks_Track.html,,"We introduce WildGuard---an open, light-weight moderation tool for LLM safety thatachieves three goals:(1) identifying malicious intent in user prompts,(2) detecting safety risks …",169.0,Methodology
