{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Safety Benchmark Scaling Analysis - Comprehensive Visualizations\n",
    "\n",
    "This notebook creates clear visualizations to analyze:\n",
    "1. **Baseline Scaling**: Which benchmarks improve with model scale?\n",
    "2. **Technique Effectiveness**: Which safety techniques work best?\n",
    "3. **Scaling Behavior**: Do techniques become more/less effective at larger scales?\n",
    "\n",
    "**Following SafetyWashing methodology**: Individual data points, Spearman correlations, slope-based categorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import hashlib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (16, 10)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "print('âœ“ All imports successful')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Merged Data\n",
    "\n",
    "**Note**: Run `preprocess_and_merge.ipynb` first to create `merged_data.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load merged dataset\n",
    "df = pd.read_csv('../data/merged_data.csv')\n",
    "\n",
    "print(f'Loaded {len(df)} data points')\n",
    "print(f'\\nColumns: {list(df.columns)}')\n",
    "print(f'\\nFirst few rows:')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing: Critical Corrections\n",
    "\n",
    "**Two critical preprocessing steps:**\n",
    "\n",
    "1. **Unique Technique Identifiers**: Same technique name from different papers = different implementations\n",
    "2. **Benchmark Directionality**: Some benchmarks (WMDP: BIO, CHEM, CYBER) have inverted scoring (higher = worse safety)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: Create unique technique identifiers (Technique + Source Paper)\n",
    "def create_paper_id(source_paper):\n",
    "    \"\"\"Create a short hash ID from the source paper URL\"\"\"\n",
    "    if pd.isna(source_paper):\n",
    "        return 'unknown'\n",
    "    return hashlib.md5(str(source_paper).encode()).hexdigest()[:8]\n",
    "\n",
    "df['paper_id'] = df['Source paper'].apply(create_paper_id)\n",
    "df['technique_id'] = df['Technique'] + '_' + df['paper_id']\n",
    "\n",
    "print(f\"âœ“ Created unique technique identifiers\")\n",
    "print(f\"  Original technique names: {df['Technique'].nunique()}\")\n",
    "print(f\"  Unique technique_id count: {df['technique_id'].nunique()}\")\n",
    "\n",
    "# STEP 2: Identify and handle benchmark directionality\n",
    "# WMDP benchmarks (BIO, CHEM, CYBER): higher score = worse safety (need inversion)\n",
    "# Other benchmarks: higher score = better safety (already correct)\n",
    "\n",
    "INVERTED_BENCHMARKS = ['BIO', 'CHEM', 'CYBER']\n",
    "\n",
    "print(f\"\\nâœ“ Benchmark directionality identified:\")\n",
    "print(f\"  Inverted (higher = worse): {INVERTED_BENCHMARKS}\")\n",
    "\n",
    "# STEP 3: Invert performance for WMDP benchmarks to make higher = better consistently\n",
    "df['Performance_normalized'] = df['Performance'].copy()\n",
    "\n",
    "for benchmark in INVERTED_BENCHMARKS:\n",
    "    mask = df['Benchmark'] == benchmark\n",
    "    if mask.sum() > 0:\n",
    "        # Invert: new_score = 1 - old_score (assuming scores are 0-1)\n",
    "        # If scores are percentages (0-100), use: 100 - old_score\n",
    "        max_val = df.loc[mask, 'Performance'].max()\n",
    "        if max_val > 1.5:  # Likely percentage scale\n",
    "            df.loc[mask, 'Performance_normalized'] = 100 - df.loc[mask, 'Performance']\n",
    "        else:  # Likely 0-1 scale\n",
    "            df.loc[mask, 'Performance_normalized'] = 1 - df.loc[mask, 'Performance']\n",
    "        \n",
    "        print(f\"  Inverted {benchmark}: {mask.sum()} data points\")\n",
    "\n",
    "print(f\"\\nâœ“ All benchmarks now normalized: higher = better safety\")\n",
    "print(f\"\\nExample - BIO benchmark before/after:\")\n",
    "bio_sample = df[df['Benchmark'] == 'BIO'][['Technique', 'Model', 'Performance', 'Performance_normalized']].head(5)\n",
    "print(bio_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Phase 1: Data Overview\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Dataset Statistics Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(20, 12))\n",
    "\n",
    "# Plot 1: Data points per benchmark\n",
    "ax1 = axes[0, 0]\n",
    "benchmark_counts = df['Benchmark'].value_counts().sort_values(ascending=True)\n",
    "benchmark_counts.plot(kind='barh', ax=ax1, color='steelblue', edgecolor='black')\n",
    "ax1.set_xlabel('Number of Data Points', fontweight='bold')\n",
    "ax1.set_title('Data Coverage by Benchmark', fontsize=14, fontweight='bold')\n",
    "ax1.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Plot 2: Data points per technique (top 15)\n",
    "ax2 = axes[0, 1]\n",
    "technique_counts = df['Technique'].value_counts().head(15).sort_values(ascending=True)\n",
    "technique_counts.plot(kind='barh', ax=ax2, color='coral', edgecolor='black')\n",
    "ax2.set_xlabel('Number of Data Points', fontweight='bold')\n",
    "ax2.set_title('Data Coverage by Technique (Top 15)', fontsize=14, fontweight='bold')\n",
    "ax2.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Plot 3: Distribution of model scales\n",
    "ax3 = axes[1, 0]\n",
    "df['scale_numeric'].hist(bins=30, ax=ax3, color='lightgreen', edgecolor='black')\n",
    "ax3.set_xlabel('Model Scale (Billions of Parameters)', fontweight='bold')\n",
    "ax3.set_ylabel('Frequency', fontweight='bold')\n",
    "ax3.set_title('Distribution of Model Scales', fontsize=14, fontweight='bold')\n",
    "ax3.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Plot 4: Coverage matrix\n",
    "ax4 = axes[1, 1]\n",
    "coverage_matrix = df.groupby(['Technique', 'Benchmark']).size().unstack(fill_value=0)\n",
    "top_techniques = df['Technique'].value_counts().head(10).index\n",
    "coverage_subset = coverage_matrix.loc[top_techniques]\n",
    "sns.heatmap(coverage_subset, annot=True, fmt='d', cmap='YlOrRd', \n",
    "            linewidths=0.5, cbar_kws={'label': 'Data Points'}, ax=ax4)\n",
    "ax4.set_title('Coverage Matrix: Top 10 Techniques Ã— Benchmarks', fontsize=14, fontweight='bold')\n",
    "ax4.set_xlabel('Benchmark', fontweight='bold')\n",
    "ax4.set_ylabel('Technique', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'\\nðŸ“Š Dataset Summary:')\n",
    "print(f'  Total data points: {len(df)}')\n",
    "print(f'  Unique benchmarks: {df[\"Benchmark\"].nunique()}')\n",
    "print(f'  Unique techniques (by name): {df[\"Technique\"].nunique()}')\n",
    "print(f'  Unique techniques (by ID): {df[\"technique_id\"].nunique()}')\n",
    "print(f'  Unique models: {df[\"Model\"].nunique()}')\n",
    "print(f'  Scale range: {df[\"scale_numeric\"].min():.0f}B - {df[\"scale_numeric\"].max():.0f}B')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Phase 2: Baseline Scaling Analysis\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Baseline Performance vs Scale\n",
    "\n",
    "**Goal**: Identify which benchmarks are \"saturated\" (improve with scale) vs \"not saturated\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract baseline data - using normalized performance\n",
    "baseline_df = df[df['Technique'] == 'Baseline'].copy()\n",
    "\n",
    "print(f'Baseline data points: {len(baseline_df)}')\n",
    "print(f'Benchmarks with baseline data: {sorted(baseline_df[\"Benchmark\"].unique())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation and slope for each benchmark using normalized performance\n",
    "benchmark_stats = []\n",
    "\n",
    "for benchmark in baseline_df['Benchmark'].unique():\n",
    "    bench_data = baseline_df[baseline_df['Benchmark'] == benchmark]\n",
    "    \n",
    "    if len(bench_data) >= 2:\n",
    "        scales = bench_data['scale_numeric'].values\n",
    "        perfs = bench_data['Performance_normalized'].values  # UPDATED: use normalized\n",
    "        \n",
    "        # Spearman correlation\n",
    "        corr, p_value = spearmanr(scales, perfs)\n",
    "        \n",
    "        # Linear regression for slope\n",
    "        model = LinearRegression()\n",
    "        model.fit(scales.reshape(-1, 1), perfs)\n",
    "        slope = model.coef_[0]\n",
    "        r2 = model.score(scales.reshape(-1, 1), perfs)\n",
    "        \n",
    "        benchmark_stats.append({\n",
    "            'Benchmark': benchmark,\n",
    "            'Correlation': corr,\n",
    "            'P_value': p_value,\n",
    "            'Slope': slope,\n",
    "            'R2': r2,\n",
    "            'N_points': len(bench_data),\n",
    "            'Category': 'Saturated' if corr > 0.5 else 'Not Saturated'\n",
    "        })\n",
    "\n",
    "bench_stats_df = pd.DataFrame(benchmark_stats)\n",
    "\n",
    "print('\\nðŸ“ˆ Benchmark Scaling Statistics (with normalized scores):')\n",
    "print('='*80)\n",
    "print(bench_stats_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize baseline scaling with normalized performance\n",
    "benchmarks = baseline_df['Benchmark'].unique()\n",
    "n_benchmarks = len(benchmarks)\n",
    "n_cols = min(3, n_benchmarks)\n",
    "n_rows = (n_benchmarks + n_cols - 1) // n_cols\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(8*n_cols, 6*n_rows))\n",
    "if n_benchmarks == 1:\n",
    "    axes = [axes]\n",
    "else:\n",
    "    axes = axes.flatten() if n_benchmarks > 1 else [axes]\n",
    "\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, n_benchmarks))\n",
    "\n",
    "for idx, benchmark in enumerate(benchmarks):\n",
    "    if idx >= len(axes):\n",
    "        break\n",
    "    \n",
    "    ax = axes[idx]\n",
    "    bench_data = baseline_df[baseline_df['Benchmark'] == benchmark]\n",
    "    \n",
    "    # Scatter plot - using normalized performance\n",
    "    ax.scatter(bench_data['scale_numeric'], bench_data['Performance_normalized'],\n",
    "              s=150, alpha=0.7, color=colors[idx], edgecolors='black', linewidth=2)\n",
    "    \n",
    "    # Trend line\n",
    "    if len(bench_data) >= 2:\n",
    "        scales = bench_data['scale_numeric'].values.reshape(-1, 1)\n",
    "        perfs = bench_data['Performance_normalized'].values\n",
    "        model = LinearRegression()\n",
    "        model.fit(scales, perfs)\n",
    "        \n",
    "        scale_range = np.linspace(scales.min(), scales.max(), 100)\n",
    "        pred = model.predict(scale_range.reshape(-1, 1))\n",
    "        ax.plot(scale_range, pred, '--', color=colors[idx], linewidth=3, alpha=0.8)\n",
    "        \n",
    "        # Add statistics\n",
    "        stats = bench_stats_df[bench_stats_df['Benchmark'] == benchmark].iloc[0]\n",
    "        \n",
    "        # Mark if benchmark is inverted\n",
    "        inverted_marker = ' (INVERTED)' if benchmark in INVERTED_BENCHMARKS else ''\n",
    "        \n",
    "        stats_text = (\n",
    "            f\"Corr: {stats['Correlation']:.3f}\\n\"\n",
    "            f\"Slope: {stats['Slope']:.4f}\\n\"\n",
    "            f\"RÂ²: {stats['R2']:.3f}\\n\"\n",
    "            f\"Status: {stats['Category']}{inverted_marker}\"\n",
    "        )\n",
    "        ax.text(0.05, 0.95, stats_text, transform=ax.transAxes,\n",
    "               verticalalignment='top',\n",
    "               bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8),\n",
    "               fontsize=10, fontweight='bold')\n",
    "    \n",
    "    ax.set_xlabel('Model Scale (Billions of Parameters)', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Baseline Performance (Normalized)', fontsize=12, fontweight='bold')\n",
    "    ax.set_title(f'{benchmark}', fontsize=14, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Remove extra subplots\n",
    "for idx in range(n_benchmarks, len(axes)):\n",
    "    fig.delaxes(axes[idx])\n",
    "\n",
    "plt.suptitle('Baseline Performance vs Model Scale (Normalized: Higher = Better Safety)', \n",
    "             fontsize=18, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Benchmark Saturation Categorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Horizontal bar chart with categorization\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "bench_stats_sorted = bench_stats_df.sort_values('Correlation')\n",
    "colors = ['#2ECC71' if cat == 'Saturated' else '#E74C3C' \n",
    "          for cat in bench_stats_sorted['Category']]\n",
    "\n",
    "y_pos = np.arange(len(bench_stats_sorted))\n",
    "bars = ax.barh(y_pos, bench_stats_sorted['Correlation'], \n",
    "               color=colors, edgecolor='black', linewidth=2, alpha=0.7)\n",
    "\n",
    "# Add threshold line\n",
    "ax.axvline(x=0.5, color='red', linestyle='--', linewidth=3, \n",
    "          alpha=0.7, label='Saturation threshold (0.5)')\n",
    "\n",
    "# Add value labels\n",
    "for i, (bar, row) in enumerate(zip(bars, bench_stats_sorted.itertuples())):\n",
    "    width = bar.get_width()\n",
    "    label_text = f\"{row.Correlation:.3f}\\n(slope: {row.Slope:.4f})\"\n",
    "    ax.text(width + 0.02, bar.get_y() + bar.get_height()/2.,\n",
    "           label_text, ha='left', va='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(bench_stats_sorted['Benchmark'], fontsize=12, fontweight='bold')\n",
    "ax.set_xlabel('Spearman Correlation (Baseline Performance vs Scale)', \n",
    "             fontsize=13, fontweight='bold')\n",
    "ax.set_title('Benchmark Saturation Analysis\\n(Do benchmarks improve with model scale?)',\n",
    "            fontsize=16, fontweight='bold')\n",
    "\n",
    "# Add legend for colors\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [\n",
    "    Patch(facecolor='#2ECC71', label='Saturated (corr > 0.5)'),\n",
    "    Patch(facecolor='#E74C3C', label='Not Saturated (corr â‰¤ 0.5)')\n",
    "]\n",
    "ax.legend(handles=legend_elements, loc='lower right', fontsize=11)\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('\\nðŸ“Š Benchmark Categories:')\n",
    "print('='*60)\n",
    "for category in ['Saturated', 'Not Saturated']:\n",
    "    benchmarks = bench_stats_df[bench_stats_df['Category'] == category]['Benchmark'].tolist()\n",
    "    print(f'{category}: {benchmarks}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Phase 3: Technique Performance Analysis\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Calculate Performance Difference from Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate difference from baseline for each data point\n",
    "# UPDATED: Use technique_id (unique per paper) and Performance_normalized\n",
    "diff_data = []\n",
    "\n",
    "for model in df['Model'].unique():\n",
    "    for scale in df[df['Model'] == model]['Scale'].unique():\n",
    "        for benchmark in df['Benchmark'].unique():\n",
    "            subset = df[\n",
    "                (df['Model'] == model) &\n",
    "                (df['Scale'] == scale) &\n",
    "                (df['Benchmark'] == benchmark)\n",
    "            ]\n",
    "            \n",
    "            # Get baseline performance (normalized)\n",
    "            baseline_perf = subset[subset['Technique'] == 'Baseline']['Performance_normalized']\n",
    "            if len(baseline_perf) == 0:\n",
    "                continue\n",
    "            \n",
    "            baseline_val = baseline_perf.values[0]\n",
    "            scale_numeric = subset['scale_numeric'].values[0]\n",
    "            \n",
    "            # Calculate difference for each UNIQUE technique (technique_id)\n",
    "            for technique_id in subset['technique_id'].unique():\n",
    "                tech_row = subset[subset['technique_id'] == technique_id].iloc[0]\n",
    "                technique_name = tech_row['Technique']\n",
    "                paper_id = tech_row['paper_id']\n",
    "                \n",
    "                if technique_name == 'Baseline':\n",
    "                    continue\n",
    "                \n",
    "                tech_val = tech_row['Performance_normalized']\n",
    "                diff = tech_val - baseline_val\n",
    "                \n",
    "                diff_data.append({\n",
    "                    'Model': model,\n",
    "                    'Scale': scale,\n",
    "                    'scale_numeric': scale_numeric,\n",
    "                    'Benchmark': benchmark,\n",
    "                    'Technique': technique_name,\n",
    "                    'technique_id': technique_id,\n",
    "                    'paper_id': paper_id,\n",
    "                    'Source_paper': tech_row['Source paper'],\n",
    "                    'Baseline_perf': baseline_val,\n",
    "                    'Technique_perf': tech_val,\n",
    "                    'Diff_from_baseline': diff,\n",
    "                    'Relative_change_pct': (diff / baseline_val) * 100 if baseline_val != 0 else 0\n",
    "                })\n",
    "\n",
    "diff_df = pd.DataFrame(diff_data)\n",
    "\n",
    "print(f'\\nâœ“ Calculated performance differences for {len(diff_df)} data points')\n",
    "print(f'\\nUnique techniques (by NAME): {diff_df[\"Technique\"].nunique()}')\n",
    "print(f'Unique techniques (by ID): {diff_df[\"technique_id\"].nunique()}')\n",
    "print(f'\\nðŸ’¡ Same technique from different papers is now treated as different!')\n",
    "print(f'\\nSample data:')\n",
    "print(diff_df[['Technique', 'paper_id', 'technique_id', 'Benchmark', 'Diff_from_baseline']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Performance Gain Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create heatmap of mean performance gain (using technique names for display)\n",
    "top_techniques = diff_df['Technique'].value_counts().head(15).index\n",
    "diff_subset = diff_df[diff_df['Technique'].isin(top_techniques)]\n",
    "\n",
    "heatmap_data = diff_subset.pivot_table(\n",
    "    index='Technique',\n",
    "    columns='Benchmark',\n",
    "    values='Diff_from_baseline',\n",
    "    aggfunc='mean'\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "sns.heatmap(heatmap_data, annot=True, fmt='.2f', cmap='RdYlGn', center=0,\n",
    "           linewidths=1, linecolor='black', cbar_kws={'label': 'Mean Performance Gain'},\n",
    "           ax=ax, vmin=-30, vmax=30)\n",
    "\n",
    "ax.set_title('Technique Effectiveness: Mean Performance Gain vs Baseline\\n(Top 15 Techniques by Name)',\n",
    "            fontsize=16, fontweight='bold')\n",
    "ax.set_xlabel('Benchmark', fontsize=13, fontweight='bold')\n",
    "ax.set_ylabel('Technique', fontsize=13, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('\\nðŸŽ¯ Key Insights:')\n",
    "print('  Green = Technique improves over baseline')\n",
    "print('  Red = Technique worse than baseline')\n",
    "print('  Yellow = Similar to baseline')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Phase 4: Scaling Behavior Analysis\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Performance Gain vs Scale (Critical Analysis)\n",
    "\n",
    "**Key Question**: Do techniques become MORE or LESS effective at larger scales?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate scaling behavior for each technique-benchmark combination\n",
    "# UPDATED: Use technique_id for unique identification\n",
    "technique_scaling = []\n",
    "\n",
    "for technique_id in diff_df['technique_id'].unique():\n",
    "    technique_name = diff_df[diff_df['technique_id'] == technique_id]['Technique'].iloc[0]\n",
    "    \n",
    "    for benchmark in diff_df['Benchmark'].unique():\n",
    "        subset = diff_df[\n",
    "            (diff_df['technique_id'] == technique_id) &\n",
    "            (diff_df['Benchmark'] == benchmark)\n",
    "        ]\n",
    "        \n",
    "        if len(subset) >= 2:\n",
    "            scales = subset['scale_numeric'].values\n",
    "            diffs = subset['Diff_from_baseline'].values\n",
    "            \n",
    "            # Spearman correlation\n",
    "            corr, p_value = spearmanr(scales, diffs)\n",
    "            \n",
    "            # Linear regression\n",
    "            model = LinearRegression()\n",
    "            model.fit(scales.reshape(-1, 1), diffs)\n",
    "            slope = model.coef_[0]\n",
    "            r2 = model.score(scales.reshape(-1, 1), diffs)\n",
    "            \n",
    "            # Categorize\n",
    "            if corr > 0.3:\n",
    "                category = 'Gain Increases with Scale'\n",
    "            elif corr < -0.3:\n",
    "                category = 'Gain Decreases with Scale'\n",
    "            else:\n",
    "                category = 'Gain Stable with Scale'\n",
    "            \n",
    "            technique_scaling.append({\n",
    "                'Technique': technique_name,\n",
    "                'technique_id': technique_id,\n",
    "                'Benchmark': benchmark,\n",
    "                'Correlation': corr,\n",
    "                'P_value': p_value,\n",
    "                'Slope': slope,\n",
    "                'R2': r2,\n",
    "                'N_points': len(subset),\n",
    "                'Category': category,\n",
    "                'Mean_gain': diffs.mean()\n",
    "            })\n",
    "\n",
    "tech_scaling_df = pd.DataFrame(technique_scaling)\n",
    "\n",
    "print(f'\\nâœ“ Analyzed scaling behavior for {len(tech_scaling_df)} technique-benchmark combinations')\n",
    "print(f'  (Using unique technique_id to distinguish same techniques from different papers)')\n",
    "print(f'\\nTop 20 results:')\n",
    "print(tech_scaling_df.sort_values('Slope', ascending=False).head(20)[[\n",
    "    'Technique', 'Benchmark', 'Correlation', 'Slope', 'Mean_gain', 'Category'\n",
    "]].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Slope Analysis: Technique Categorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot: Slope vs Mean Gain\n",
    "# UPDATED: Group by technique_id for unique techniques\n",
    "tech_summary = tech_scaling_df.groupby('technique_id').agg({\n",
    "    'Technique': 'first',\n",
    "    'Slope': 'mean',\n",
    "    'Mean_gain': 'mean',\n",
    "    'N_points': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "tech_summary_filtered = tech_summary[tech_summary['N_points'] >= 5]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 10))\n",
    "\n",
    "scatter = ax.scatter(tech_summary_filtered['Slope'], \n",
    "                    tech_summary_filtered['Mean_gain'],\n",
    "                    s=tech_summary_filtered['N_points'] * 10,\n",
    "                    alpha=0.6, c=range(len(tech_summary_filtered)),\n",
    "                    cmap='viridis', edgecolors='black', linewidth=2)\n",
    "\n",
    "# Add labels\n",
    "for _, row in tech_summary_filtered.iterrows():\n",
    "    label = row['Technique'][:20] if len(row['Technique']) > 20 else row['Technique']\n",
    "    ax.annotate(label, \n",
    "               (row['Slope'], row['Mean_gain']),\n",
    "               fontsize=9, fontweight='bold',\n",
    "               bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.3))\n",
    "\n",
    "# Add quadrant lines\n",
    "ax.axhline(y=0, color='gray', linestyle='-', linewidth=1.5, alpha=0.5)\n",
    "ax.axvline(x=0, color='gray', linestyle='-', linewidth=1.5, alpha=0.5)\n",
    "\n",
    "# Add quadrant labels\n",
    "ax.text(0.7, 0.95, 'HIGH GAIN\\n+ SCALES WELL\\n(BEST)', \n",
    "       transform=ax.transAxes, fontsize=12, fontweight='bold',\n",
    "       bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.7),\n",
    "       ha='center', va='top')\n",
    "ax.text(0.3, 0.95, 'HIGH GAIN\\n+ STABLE/DECREASES', \n",
    "       transform=ax.transAxes, fontsize=12, fontweight='bold',\n",
    "       bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.7),\n",
    "       ha='center', va='top')\n",
    "ax.text(0.7, 0.05, 'LOW GAIN\\n+ SCALES WELL', \n",
    "       transform=ax.transAxes, fontsize=12, fontweight='bold',\n",
    "       bbox=dict(boxstyle='round', facecolor='orange', alpha=0.7),\n",
    "       ha='center', va='bottom')\n",
    "ax.text(0.3, 0.05, 'LOW GAIN\\n+ DECREASES\\n(WORST)', \n",
    "       transform=ax.transAxes, fontsize=12, fontweight='bold',\n",
    "       bbox=dict(boxstyle='round', facecolor='lightcoral', alpha=0.7),\n",
    "       ha='center', va='bottom')\n",
    "\n",
    "ax.set_xlabel('Slope (Change in Gain per Unit Scale)', fontsize=13, fontweight='bold')\n",
    "ax.set_ylabel('Mean Performance Gain', fontsize=13, fontweight='bold')\n",
    "ax.set_title('Technique Categorization: Current Performance vs Scaling Potential\\n(Size = Total Data Points, Unique Techniques by Paper)',\n",
    "            fontsize=16, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'\\nðŸ“Š Total unique techniques analyzed: {len(tech_summary_filtered)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Phase 6: Summary Dashboard\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 One-Page Summary Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 12))\n",
    "gs = fig.add_gridspec(2, 2, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# Panel 1: Benchmark Saturation\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "bench_stats_sorted = bench_stats_df.sort_values('Correlation')\n",
    "colors = ['#2ECC71' if cat == 'Saturated' else '#E74C3C' \n",
    "          for cat in bench_stats_sorted['Category']]\n",
    "y_pos = np.arange(len(bench_stats_sorted))\n",
    "ax1.barh(y_pos, bench_stats_sorted['Correlation'], color=colors, \n",
    "        edgecolor='black', linewidth=2, alpha=0.7)\n",
    "ax1.set_yticks(y_pos)\n",
    "ax1.set_yticklabels(bench_stats_sorted['Benchmark'], fontweight='bold')\n",
    "ax1.axvline(x=0.5, color='red', linestyle='--', linewidth=2, alpha=0.7)\n",
    "ax1.set_xlabel('Correlation', fontweight='bold')\n",
    "ax1.set_title('Benchmark Saturation', fontsize=14, fontweight='bold')\n",
    "ax1.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Panel 2: Technique Effectiveness Heatmap\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "top_5_techniques = diff_df['Technique'].value_counts().head(5).index\n",
    "heatmap_subset = diff_df[diff_df['Technique'].isin(top_5_techniques)].pivot_table(\n",
    "    index='Technique', columns='Benchmark', values='Diff_from_baseline', aggfunc='mean'\n",
    ")\n",
    "sns.heatmap(heatmap_subset, annot=True, fmt='.1f', cmap='RdYlGn', center=0,\n",
    "           linewidths=1, cbar_kws={'label': 'Mean Gain'}, ax=ax2)\n",
    "ax2.set_title('Top 5 Technique Effectiveness', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Benchmark', fontweight='bold')\n",
    "ax2.set_ylabel('Technique', fontweight='bold')\n",
    "\n",
    "# Panel 3: Scaling Trends\n",
    "ax3 = fig.add_subplot(gs[1, 0])\n",
    "tech_summary_top = tech_summary_filtered.head(10)\n",
    "scatter = ax3.scatter(tech_summary_top['Slope'], tech_summary_top['Mean_gain'],\n",
    "                     s=tech_summary_top['N_points'] * 20, alpha=0.6,\n",
    "                     c=range(len(tech_summary_top)), cmap='viridis',\n",
    "                     edgecolors='black', linewidth=2)\n",
    "for _, row in tech_summary_top.iterrows():\n",
    "    ax3.annotate(row['Technique'][:15], (row['Slope'], row['Mean_gain']),\n",
    "                fontsize=8, bbox=dict(boxstyle='round,pad=0.2', \n",
    "                facecolor='yellow', alpha=0.3))\n",
    "ax3.axhline(y=0, color='gray', linestyle='-', alpha=0.5)\n",
    "ax3.axvline(x=0, color='gray', linestyle='-', alpha=0.5)\n",
    "ax3.set_xlabel('Slope (Scaling Behavior)', fontweight='bold')\n",
    "ax3.set_ylabel('Mean Gain', fontweight='bold')\n",
    "ax3.set_title('Scaling Trends (Top 10)', fontsize=14, fontweight='bold')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Panel 4: Best Technique per Benchmark\n",
    "ax4 = fig.add_subplot(gs[1, 1])\n",
    "ax4.axis('off')\n",
    "\n",
    "best_techniques = []\n",
    "for benchmark in diff_df['Benchmark'].unique():\n",
    "    bench_data = diff_df[diff_df['Benchmark'] == benchmark]\n",
    "    best = bench_data.groupby('Technique')['Diff_from_baseline'].mean().idxmax()\n",
    "    best_gain = bench_data.groupby('Technique')['Diff_from_baseline'].mean().max()\n",
    "    best_techniques.append([benchmark, best, f'{best_gain:.2f}'])\n",
    "\n",
    "table_data = [['Benchmark', 'Best Technique', 'Mean Gain']] + best_techniques\n",
    "table = ax4.table(cellText=table_data, cellLoc='left', loc='center',\n",
    "                 colWidths=[0.3, 0.45, 0.25])\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "table.scale(1, 2)\n",
    "\n",
    "for i in range(3):\n",
    "    table[(0, i)].set_facecolor('#3498DB')\n",
    "    table[(0, i)].set_text_props(weight='bold', color='white')\n",
    "\n",
    "ax4.set_title('Best Technique per Benchmark', fontsize=14, fontweight='bold', pad=20)\n",
    "\n",
    "plt.suptitle('Safety Benchmark Scaling Analysis - Summary Dashboard',\n",
    "            fontsize=20, fontweight='bold', y=0.98)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Findings Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('='*80)\n",
    "print('KEY FINDINGS')\n",
    "print('='*80)\n",
    "\n",
    "print('\\n1. BENCHMARK SATURATION:')\n",
    "saturated = bench_stats_df[bench_stats_df['Category'] == 'Saturated']['Benchmark'].tolist()\n",
    "not_saturated = bench_stats_df[bench_stats_df['Category'] == 'Not Saturated']['Benchmark'].tolist()\n",
    "print(f'   Saturated (improve with scale): {saturated}')\n",
    "print(f'   Not Saturated: {not_saturated}')\n",
    "\n",
    "print('\\n2. TECHNIQUE EFFECTIVENESS:')\n",
    "top_3_overall = tech_summary_filtered.nlargest(3, 'Mean_gain')\n",
    "print('   Top 3 techniques by mean gain:')\n",
    "for _, row in top_3_overall.iterrows():\n",
    "    print(f'   - {row[\"Technique\"]}: {row[\"Mean_gain\"]:.2f} gain')\n",
    "\n",
    "print('\\n3. SCALING BEHAVIOR:')\n",
    "scales_well = tech_summary_filtered[tech_summary_filtered['Slope'] > 0.1]\n",
    "print(f'   Techniques that scale well (positive slope > 0.1): {len(scales_well)}')\n",
    "if len(scales_well) > 0:\n",
    "    print('   Top 3:')\n",
    "    for _, row in scales_well.nlargest(3, 'Slope').iterrows():\n",
    "        print(f'   - {row[\"Technique\"]}: slope={row[\"Slope\"]:.4f}')\n",
    "\n",
    "print('\\n4. SWEET SPOT TECHNIQUES (High gain + Positive slope):')\n",
    "sweet_spot = tech_summary_filtered[\n",
    "    (tech_summary_filtered['Mean_gain'] > 0) & \n",
    "    (tech_summary_filtered['Slope'] > 0)\n",
    "]\n",
    "if len(sweet_spot) > 0:\n",
    "    for _, row in sweet_spot.head(5).iterrows():\n",
    "        print(f'   - {row[\"Technique\"]}: gain={row[\"Mean_gain\"]:.2f}, slope={row[\"Slope\"]:.4f}')\n",
    "else:\n",
    "    print('   None found')\n",
    "\n",
    "print('\\n' + '='*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
